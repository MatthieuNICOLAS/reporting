#+STARTUP: inlineimages
#+TODO: TODO IN-PROGRESS CANCELLED DONE

* Journal de bord
** Semaine du <2016-09-12 lun.> au <2016-09-16 ven.>
*** Done
- Faire un état de l'art des mécanismes d'authentification
  - Différences entre OAuth et OpenID
    - OAuth est un protocole d'*autorisation*
    - Il permet à un utilisateur de donner accès à certaines données/actions de son compte à un service tiers
      - Exemple avec Google +
	- Récupérer les informations du profil
	- Récupérer les informations des cercles d'amis de l'utilisateur
	- Envoyer des mails au nom de l'utilisateur
      - Exemple avec GitHub
	- Récupérer les informations du profil
	- Accéder aux repos de l'utilisateur
	- Faire des commits en son nom
    - Il ne s'agit cependant pas d'un protocole d'*authentification*
      - On peut supposer que si Google ou GitHub nous donne accès au compte de l'utilisateur, c'est car ce dernier s'est authentifié correctement
      - Mais rien n'empêche un site ayant obtenu un code (car l'utilisateur a donné l'autorisation à ce dernier) d'utiliser ce code pour usurper l'identité de l'utilisateur sur un autre site
    - OpenID lui est un protocole d'*authentification*
      - Il permet seulement de vérifier l'identité d'un utilisateur
      - Il ne permet pas d'accéder à des données de son compte autre que celles du profil
    - OpenID Connect est un protocole basé sur OAuth 2.0 qui ajoute une couche d'authentification au protocole
    - Sources
      - http://security.stackexchange.com/questions/44611/difference-between-oauth-openid-and-openid-connect-in-very-simple-term
      - https://oauth.net/2/
      - https://en.wikipedia.org/wiki/OpenID
      - http://openid.net/connect/
      - http://www.thread-safe.com/2012/01/problem-with-oauth-for-authentication.html
  - Libraries/Frameworks
    - Pour OAuth2: https://oauth.net/code/
    - Pour OpenID Connect: http://openid.net/developers/libraries/
*** Planned
** Semaine du <2016-09-19 lun.> au <2016-09-23 ven.>
*** Done
- Faire un état de l'art des mécanismes d'authentification
  - Réalisation des slides
    - Disponibles ici : https://github.com/MatthieuNICOLAS/authentication-systems-2016-09-20/
  - Ajout des nouvelles sources :
    - [[https://pdfs.semanticscholar.org/3733/2607f7a7ac8284c514845957fd00583e5614.pdf][Different Ways to Authenticate Users with the Pros and Cons of each Method]]
      - Bonne définition de l'*Authentification*.
      - Parle de l'authentification à facteurs multiples :
        - Passwords
        - Smart cards
        - X509 certificate
        - Biometrics
      - Présente les avantages et inconvénients ainsi que les vulnérabilités de chacune.
    - [[http://stackoverflow.com/questions/663402/what-are-the-differences-between-ldap-and-active-directory][What are the differences between LDAP and Active Directory?]]
      - Donne les définitions de *Directory Service*, *Active Directory* et *Lightweight Directory Access Protocol*
    - [[https://www.neustar.biz/blog/what-is-single-sign-on-deployment-pros-cons][The Pros & Cons of Implementing Single Sign-On]]
      - Explique les différences entre *Full Sign-On*, *Reduced Sign-On* et *Federated Logins*
    - [[https://tools.ietf.org/html/rfc6749][RFC 6749 - The OAuth 2.0 Authorization Framework]]
      - Description du protocole OAuth 2.0
    - [[http://wiki.oauth.net/w/page/27249271/OAuth%202%20for%20Native%20Apps][OAuth 2 for Native Apps]]
    - [[https://tools.ietf.org/html/draft-ietf-oauth-native-apps-03][Authorization Flow for Native Apps Using App-Claimed URI Schemes]]
    - [[http://nat.sakimura.org/2012/01/20/openid-connect-nutshell/][OpenID Connect in a nutshell]]
    - [[http://wiki.openid.net/w/page/12995171/Introduction][OpenID explaination]]
    - [[https://en.wikipedia.org/wiki/OAuth#/media/File:OpenIDvs.Pseudo-AuthenticationusingOAuth.svg][OpenID Authentication vs Pseudo-Authentication using OAuth]]
    - [[https://www.owasp.org/index.php/Authentication_Cheat_Sheet][Authentication Cheat Sheet]]
- Finaliser les slides sur les mécanismes d'authentification et les envoyer à TVPaint
  - Ajout de références vers les articles lus/utilisés et pouvant s'avérer utiles pour leur compréhension
  - Création d'un thread sur le forum de TVPaint pour linker les slides et les inviter à poser leurs questions
- MUTE : Implémenter le bot de traduction
  - Problème avec la librairie *mute-client*
    - *mute-client* embarque *AceEditorAdapter*
    - Ce code JS suppose que *AceEditor* est dans le scope
    - Ce n'est pas le cas dans le bot
- PLM : Activer Blockly
  - Modification du code de *GitActor* pour gérer les langages de programmation visuels
    - Stocker le code généré par l'outil de programmation visuel
    - Mais aussi stocker le /code visuel/
      - Pour pouvoir regénérer l'espace de travail de l'apprenant à sa prochaine connexion
  - Modification du code de *PLMActor* pour gérer les langages de programmation visuels
    - Envoi à *GitActor* le /code visuel/
    - Lors de la récupération de la session de l'utilisateur, on ne récupère pas le code de l'élève
      - Puisqu'il s'agit du code généré par l'outil de programmation visuel
        - Par exemple du code Python dans le cas de Blockly
    - Il faut récupérer plutôt le /code visuel/ pour regénérer l'espace de travail
    - Pour le moment, on se contente de renvoyer un /code visuel/ vide
  - Activation de Blockly
    - Création d'une instance de *LangBlockly*
    - Ajout de celle-ci dans les langages de programmation supportés
  - Modification du code pour gérer les exercices ne supportant pas Blockly
    - Passage automatique au langage de programmation par défaut si le langage courant n'est pas supporté
    - Affichage d'un message d'avertissement lorsque l'utilisateur essaie de passer à un langage non-supporté par l'exo courant
  - Mise à jour des sérialisation JSON des exercices
*** Planned
**** DONE Faire un état de l'art des mécanismes d'authentification
- Étudier les différents mécanismes existants et comment ils interagissent entre eux
  - SAML
  - SOAP
  - OAuth
  - OpenID
  - LDAP

**** DONE Finaliser les slides sur les mécanismes d'authentification et les envoyer à TVPaint
- J'ai noté quelques sources qui pourraient s'avérer utiles pour TVPaint
- Les ajouter aux slides
- Envoyer les slides à TVPaint
**** DONE PLM : Activer Blockly
- [[http://www.pentilanero.com/][Pentila]] m'a recontacté à propos de PLM
- Ils sont en train de travailler sur PLM pour voir comment ajouter de nouveaux exercices fonctionnant avec Blockly
- Sauf que j'ai désactivé Blockly lors du refactoring
- Activer de nouveau Blockly
- Vérifier son bon fonctionnement
** Semaine du <2016-09-26 lun.> au <2016-09-30 ven.>
*** Done
- TVPaint : Réviser les notions autour du *Business Process* et des *Workflows*
  - L'activité /Call/ est-elle utilisable ou à éviter ?
  - Que signifie la notion d'entité lorsqu'on parle de /Choreography/ ?
    - Similaire à la notion de /Pool/ ?
  - Différences entre /Choreography/ et /Collaboration/
    - Pourquoi utiliser l'un et pas l'autre ?
    - Formalisme différent, et la /Choreography/ fonctionne par le biais d'envoi de messages
  - Une /Choreography/ est-elle forcément entre 2 entités ?
    - Il n'y a jamais eu le besoin de définir une choréographie entre 3+ entités ?
    - Non, une /Choreography/ peut avoir lieu entre 3+ entités
- MUTE : Implémenter le bot de traduction
  - Ajout de la classe *TranslatorBot*
    - Instancie un *Bot* pour être pouvoir être ajouté au réseau
    - Instancie un *Coordinator* lors de son ajout au réseau
    - Écoute l'évènement /update/ du *Coordinator* pour déclencher l'opération de traduction
    - Écoute l'évènement /operations/ du *Coordinator* pour transmettre les opérations générées localement aux pairs
    - Lors d'un /update/, parcourt le document pour trouver le texte à traduire
      - Le texte à traduire est délimité par les balises suivantes
        - /tl <langue source> <langue destination>
        - end/
    - Demande à *YandexTranslateService* de traduire le texte
    - Remplace le texte à traduire + balises par le résultat renvoyé par *YandexTranslateService*
  - Malheureusement, ce n'était pas le comportement attendu
    - On souhaite insérer une balise dans le document
    - Le bot doit
      - Récupérer le texte précédent cette balise
      - Le traduire
      - L'insérer après la balise
  - Implémentation de ce comportement
    - Ajout de *RealTimeTranslator*
    - Détecte le tag de l'utilisateur /rt
    - Le remplace par son propre tag
      - Cela lui permet de récupérer l'ID LogootSplit de ce tag
      - Cet ID est ensuite utilisé pour récupérer l'index du tag dans le document
    - Traduit le texte
      - Utilise /diff/ pour comparer l'ancienne traduction et la nouvelle
      - Permet de seulement mettre à jour les parties de la traduction concernées par les modifications du texte initial
- PLM : Corriger l'exécution du code de l'apprenant en Python
  - Le bug rencontré par Cédric n'est pas reproductible dans mon environnement de dev
  - Par contre, dans un container Docker basé sur l'image de webPLM, je rencontre une erreur lié à *JRuby*
    - Je n'inclus pas le jar de *JRuby* dans *PLM-engine*
  - Ajout de *JRuby* dans *PLM-engine*
    - Résous l'erreur lié à *JRuby*
  - Je rencontre dorénavant l'erreur reportée par Cédric
    - Cette erreur semble liée à la version de *Jython* utilisée
  - Mise à jour de *Jython*
  - Le plus troublant est que les juges arrivaient à exécuter du code Python
    - *JRuby* était correctement inclus dans le jar de *PLM* fourni aux juges
    - Mais ils auraient dû déclencher l'erreur liée à la version de *Jython*
*** Planned
**** DONE PLM : Corriger l'exécution du code de l'apprenant en Python
- Cédric de Pentila m'a contacté à propos des problèmes qu'il rencontre pour exécuter du code Python
- Il rencontre l'erreur suivante en essayant d'exécuter son programme Python
#+BEGIN_SRC
java.lang.NullPointerException
  at org.python.core.Py.recursiveIsInstance(Py.java:1861)
  at org.python.core.Py.isInstance(Py.java:1828)
  at org.python.core.__builtin__.isinstance(__builtin__.java:725)
  at org.python.core.Py.displayException(Py.java:1009)
  at org.python.core.PyException.printStackTrace(PyException.java:79)
  at org.python.core.PyException.toString(PyException.java:98)
  at java.lang.Throwable.<init>(Throwable.java:311)
  at java.lang.Exception.<init>(Exception.java:102)
  at javax.script.ScriptException.<init>(ScriptException.java:65)
  at org.python.jsr223.PyScriptEngine.scriptException(PyScriptEngine.java:192)
  at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:43)
  at org.python.jsr223.PyScriptEngine.eval(PyScriptEngine.java:33)
  at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)
  at plm.core.lang.ScriptingLanguage.runEntity(Unknown Source)
  at plm.core.model.lesson.ExerciseRunner$3.run(Unknown Source)
  at java.lang.Thread.run(Thread.java:745)
#+END_SRC
- Trouver l'origine de cette erreur et la résoudre
**** DONE MUTE : Implémenter le bot de traduction
- Dans le cadre de l'évaluation de l'équipe du 13 octobre, une démo de [[https://github.com/coast-team/mute-demo][MUTE]] est prévue
- L'idée est de mettre en avant la fonctionnalité des bots à l'aide d'un bot qui traduirait pour nous le document
- Le code du bot est disponible ici : https://github.com/coast-team/mute-bot-eve
- Analyser le document pour détecter les parties du document à traduire
- Interroger *Yandex* pour traduire le texte
- Remplacer le texte par sa traduction
**** DONE TVPaint : Réviser les notions autour du *Business Process* et des *Workflows*
- Nécessaire pour pouvoir mieux appréhender la tâche consistant à définir un process pour représenter le déroulement de la réalisation d'un film avec TVPaint
- Sources à utiliser:
  - http://webloria.loria.fr/~charoy/uploads/Main/BPM2pp.pdf
  - http://www.workflowpatterns.com/
  - http://fr.bonitasoft.com/ressources
** Semaine du <2016-10-03 lun.> au <2016-10-07 ven.>
*** Done
- TVPaint : Définir le business process correspondant à la réalisation d'un film
  - Quelques questions sont apparus au cours de la réalisation
    - Est-ce qu'il y a une phase de validation
      - Après la réalisation du storyboard
      - Après la réalisation de l'animatique
    - Est-ce que l'animatique et le storyboard se font de façon séquentielle ou parallèle?
    - Est-ce que l'animatique "modifie" le storyboard ou produit une nouvelle donnée?
    - Qui s'occupe de l'élaboration du layout du shot?
    - Est-ce que l'élaboration du layout du shot inclus le typage de chaque clip?
- PLM : Release d'une nouvelle version de PLM
  - Transfert des commits modifiant l'UI dans une branche distincte
  - Activation des leçons sur la récursivité
  - Correction du nom des fichiers de consignes pour l'exercice *Occurrences*
  - Correction de l'entité Java de l'exercice *IsMember*
  - Augmentation de la limite de temps d'exécution de PLM-judge
  - Release de webPLM:2.1.1 et de PLM-judge:2.1.1
- PLM : Corriger l'exécution de code Java
  - L'erreur ne semble se produire qu'en exécutant du code via les juges
  - L'erreur est liée à 2 morceaux de code
    - [[https://github.com/BuggleInc/PLM/blob/master/src/plm/core/lang/ProgrammingLanguage.java#L131-L139][ProgrammingLanguage.getProgrammingLanguage()]]
    - [[https://github.com/BuggleInc/PLM/blob/master/src/plm/core/model/lesson/BlankExercise.java#L26-L27][BlankExercise()]]
  - Lorsqu'on désérialise un exercice, on regénère les instances de *SourceFile* pour chaque langage de programmation
  - Ainsi, on instancie le *SourceFile* et on le stocke dans une *Map* en l'associant au *ProgrammingLanguage* retourné par /getProgrammingLanguage()/
  - Sauf que la fonction /getProgrammingLanguage()/ retourne une valeur par défaut s'il ne trouve pas de *ProgrammingLanguage* correspondant à la chaîne passée en paramètre
  - Du coup, lorsqu'on rencontre un *ProgrammingLanguage* inconnu, on remplace le *SourceFile* associé au langage par défaut par celui nouvellement créé
  - Ici, c'est *Blockly* qui n'est pas supporté par *PLM-judge* et qui pose problème
  - Ajout du support de *Blockly*
  - Ouverture d'une issue pour garder une trace du problème : https://github.com/BuggleInc/PLM/issues/477
- PLM : Corriger l'exercice Polygon360
  - L'erreur ne provient pas de la sérialisation JSON stockée
    - Supprimer la sérialisation de l'exercice et l'instancier depuis les sources ne corrige pas le problème
  - Le plus troublant est l'affichage du monde objectif
  - Une partie du polygone semble manquer dans le monde objectif
    #+CAPTION: Partie manquante du polygone
    #+NAME:   fig:polygon360-missingline.png
    [[file:img/polygon360-missingline.png]]
  - Représentation des lignes indiquées dans le message d'erreur
    #+CAPTION: Lignes posant problème
    #+NAME:   fig:polygon360-showlines.png
    [[file:img/polygon360-showlines.png]]
    - La ligne bleue correspond à la ligne manquante dans le monde objectif
    - La ligne rouge correspond à la ligne manquante dans le monde courant
  - On arrive à la même conclusion en listant les *Shapes* présentes dans chacun des mondes
    #+BEGIN_SRC
    // Lines of the current world with x1 or x2 > 262
    CurrentWorld: Line (x262,263 y127,117 / x263,445 y135,028 / black)
    CurrentWorld: Line (x262,923 y164,938 / x263,968 y154,996 / black)
    CurrentWorld: Line (x263,445 y135,028 / x264,177 y149,000 / black)

    // Lines of the answer world with x1 or x2 > 262
    AnswerWorld: Line (x262,263 y127,117 / x263,445 y135,028 / black)
    AnswerWorld: Line (x262,923 y164,938 / x263,968 y154,996 / black)
    AnswerWorld: Line (x263,968 y154,996 / x264,177 y149,000 / black)
    #+END_SRC
*** Planned
**** DONE PLM : Release une nouvelle version de PLM
- Pour le module de TOP à TN, les étudiants vont travailler sur PLM
- Les enseignements de ce module porte notamment sur la récursivité
- Les leçons correspondantes ne sont pas activées dans la version actuelle de la PLM
- Les activer et déployer la nouvelle version
**** DONE PLM : Corriger l'exécution de code Java
- Depuis la mise à jour, l'exécution du code Java génère une erreur
  #+BEGIN_SRC
  Compilation error: Environment.java:0:class, interface, or enum expected
  #+END_SRC
- Trouver l'origine de l'erreur et la corriger
** Semaine du <2016-10-10 lun.> au <2016-10-14 ven.>
*** Done
- TVPaint : Définir le business process correspondant à la réalisation d'un film
  - Ajout des phases de vérification pour
    - Le storyboard
    - L'animation
  - Ajout des données des formulaires
    - Création du film
    - Phases de vérification
  - Difficultés rencontrées
    - Bloque sur l'instanciation d'un *Storyboard* et d'une *Animation* à partir des fichiers soumis par les utilisateurs
    - Aussi des difficultés sur la multi-instanciation du process pour gérer un shot
      - On souhaite générer une instance du process pour chaque shot
      - La liste des shots se trouvent dans l'instance de *Film*
      - Mais je n'arrive pas accéder à la variable de process /movie/
      - Pour l'instant, je passe par une autre variable de process /shots/
    - Je n'arrive plus non plus à créer un type /enum/ avec Bonita Soft
  - Quelques questions sur le workflow
    - Que se passe-t-il/que fait-on si un shot ne passe pas la dernière étape de validation?
      - L'étape de validation concernée se trouve après l'activité /compositing/
      - Est-ce qu'elle ne sert qu'à valider cette activité ou elle sert à valider l'ensemble des clips?
  - Quelques questions sur les modèles de données
    - À quoi correspond le modèle de données *Scene* ?
    - Que contient le modèle de données *Animation* ?
- MUTE : Préparer la démo pour l'évaluation INRIA
  - Philippe a installé les différents composants de la démo sur les Raspberries et configuré les interfaces réseaux
  - Scénario :
    - Introduction
      - Au départ, seul MUTE est déployé
      - Le bot est démarré, mais il n'est pas encore relié au réseau
      - Gérald (qui projetera son écran) se connectera à un document pré-existant
      - Collaboration rapide pour montrer l'aspect temps réel
    - Ajout du bot
      - On le connecte au réseau
      - On l'invite dans l'édition collaborative
      - On l'active
      - On observe qu'il traduit le texte déjà existant et met ensuite à jour en temps réel (mais avec délai) la traduction
    - Sans serveur
      - On débranche la Raspberry correspondant au serveur
      - On observe que la collaboration fonctionne toujours
  - Texte à taper :
    #+BEGIN_QUOTE
    Pourquoi je veux vous réunir : pour vous ou pour moi?
    Evidemment pour moi, cela résoudrait tout pour moi ; j'en ai décidé ainsi depuis longtemps...
    On m'a raconté que votre soeur Adélaïde avait dit de mon portrait qu'avec une beauté pareille on pouvait bouleverser le monde.
    Mais j'ai renoncé au monde ; cela vous paraît drôle venant de moi, alors que vous me rencontrez couverte de bijoux et de dentelles en compagnie d'ivrognes et de scélérats?
    Ne faites pas attention à cela, je n'existe presque plus, et je le sais ; Dieu sait ce qui habite en moi à ma place.
    -- Dostoïevski dans son roman "Idiot"
    #+END_QUOTE
  - Répétition de la démo
- PLM : Release une image de NGINX pour le mode développement
  - Ajout d'un répertoire pour indiquer que la config actuelle est celle pour TELECOM Nancy
  - Modification de la configuration pour ajouter le reverse-proxy pour la queue de message
  - Ajout d'une autre configuration pour le mode développement
  - Génération des images Docker correspondantes
*** Planned
**** DONE PLM : Release une image de NGINX pour le mode développement
- Depuis le rework du protocole de communication avec les juges, NGINX est utilisé comme reverse-proxy pour accéder à la queue de message
  - Le port 15674 (utilisé par STOMP) n'est pas ouvert sur la machine de TN
  - NGINX permet d'accéder à ce port via un reverse-proxy
- Ceci empêche le docker-compose pour le mode développement de fonctionner correctement
  - Il ne met pas en place de NGINX
- Faire une image Docker de NGINX pour le développement
- Mettre à jour le docker-compose correspondant
**** DONE MUTE : Préparer la démo pour l'évaluation INRIA
- Tester l'environnement de la démo
- Établir le scénario de la démo
- Trouver un texte à taper et se le répartir
** Semaine du <2016-10-17 lun.> au <2016-10-21 ven.>
*** Done
- TVPaint : Définir le business process correspondant à la réalisation d'un film
  - De nouvelles questions sur le workflow
    - Est-ce que l'activité "Assignation outils/images" peut être entièrement automatisée ou pas?
- MUTE : Faire la roadmap du projet
  - La liste des tâches est disponible sur [[https://app.asana.com/][Asana]]
  - Définition des différentes étapes
    - v1: MUTE--
    - v2: Notepad equivalent
    - v3: Fourre-tout
  - Choix des technos
    - Angular 2
    - Angular Matérial 2
    - Ava
    - TSLint
*** Planned
**** DONE MUTE : Faire la roadmap du projet
- Pour répondre au besoin d'INRIA
- Lister les tâches à effectuer
- Les attribuer aux différentes étapes/versions du projet
- Établir le coût de chacune des tâches
**** DONE MUTE : Se former aux technologies choisies
- Lire [[https://books.ninja-squad.com/angular2][Become A Ninja With Angular 2]] et [[https://www.gitbook.com/book/basarat/typescript/details][TypeScript Deep Dive]]
** Semaines du <2016-11-14 lun.> au <2016-11-25 ven.>
*** Done
- MUTE : Développement de la nouvelle version de MUTE
  - Ajout de l'éditeur de texte *CodeMirror*
  - Création du modèle de données *LogootSRopes* du document
  - Détection des modifications de l'utilisateur et génération des *TextOperations* correspondantes
    - *TextInsert* et *TextDelete*
  - Mise à jour du modèle à partir de ces opérations textes
  - Envoi des *LogootSOperations* correspondantes aux modifications aux collaborateurs
  - Réception et ré-instantiation des *LogootSOperations*
  - Mise à jour du modèle à partir des opérations *LogootSplit*
  - Mise à jour de la vue du document
  - Passage à la nouvelle version de /Mute-structs/
  - Correction de bugs liées à la nouvelle version de /Mute-structs/
    - Suppression d'une boucle infinie dans la recherche d'un noeud
    - Correction des assertions effectuées lors des rotations
    - Correction des expressions permettant d'indiquer si l'on peut /append/ ou /prepend/ du texte à un bloc
** Semaine du <2016-11-28 lun.> au <2016-12-02 ven.>
*** Done
- TVPaint : Étudier comment collaborer dans un projet réalisé avec BonitaSoft
  - D'après [[http://community.bonitasoft.com/questions-and-answers/does-bonita-support-git-repository][ce topic]] et [[http://documentation.bonitasoft.com/6.x-7.2/workspaces-and-repositories-1][cette doc]], on ne peut pas partager un projet dans *Bonita* avec la version /Community/
  - La fonctionnalité est ajoutée dans la version /Subscription/
  - La doc ne semble disponible que pour SVN par contre
  - Et il manque les tarifs sur le site de *Bonita*, il faut prendre contact avec eux pour les obtenir
- MUTE : Développement du bot de stockage
  - Projet disponible [[https://github.com/coast-team/mute-bot-storage][ici]]
*** Planned
**** DONE TVPaint : Envoyer le travail effectué
**** DONE TVPaint : Étudier comment collaborer dans un projet réalisé avec BonitaSoft
- Voir si on peut partager avec Git
**** DONE MUTE : Développement du bot de stockage
** Semaine du <2016-12-05 lun.> au <2016-12-09 ven.>
*** Done
- MUTE : Lire l'article sur les barrières causales
  - Happened before relation
    - a -> b if a and b are events in the same process and a occured before b
    - a -> b if a is the event of sending a message M in a process and b is the event of delivery of the same message to another process
    - if a -> b and b -> c then a -> c
    - if a -/> b and b -/> a then a and b are said to be concurrent and represented as a || b
  - Reception != Delivery
    - Réception du message par le protocole de causal ordering
    - Livraison du message par le protocole de causal ordering à l'application
  - Destination set
    - Si on reçoit un message M2 ayant pour dépendance M1.
    - Si on a pas reçu M1 car on ne fait pas parti de son destination set, alors on peut le retirer de la liste des dépendances et délivrer M2.
  - Matrice Delivered
    - En local stocke et tient à jour matrice NxN Delivered tel que
      - Soit i le process courant, j et k 2 autres processes
      - Delivered(i)[j, k] = x indique que tous les messages de Pj à destination de Pk ayant une clock <= x ont été délivrés
	- Delivered(i) indique la matrice Delivered du process i
  - Vecteur CB
    - En local, maintien un vecteur CB de taille N de telle manière que si (k,x) appartiennent à CB(i)[j], cela implique que le prochain message de Pi à Pj devra être délivré uniquement après avoir reçu le Xième de Pk.
    - CB(i) = { j: { (process_id, counter), ... }, ... }
*** Planned
**** DONE MUTE : Lire l'article sur les barrières causales
- Maintenant que le bot de stockage est implémenté, on rencontre un problème lors de la synchronisation
- Si un utilisateur possède une version plus récente du document que le bot de stockage, celle-ci va se faire écraser lors de la synchronisation
- Alors qu'avec *LogootSplit*, on pourrait juste déterminer les opérations manquantes et synchroniser proprement les documents
- Dans un 1er temps on va implémenter un vecteur d'état pour déterminer les opérations manquantes
- Mais à terme, on pourra le remplacer par une barrière causale (ou mieux) pour optimiser le processus
- Se renseigner sur les barrières causales
** Semaine du <2016-12-12 lun.> au <2016-12-16 ven.>
*** Done
- MUTE : Refactorer *DocService*
  - Gestion des messages spécifiques à ce service dans *DocService*
    - Réception des messages
    - Sérialisation/Désérialisation des objets de /mute-structs/
  - Remplacement des instances de *Subject* par des couples *Observable*/*Observer*
    - Le but de *Subject* a l'air d'émettre une valeur, et non pas une suite d'évènements
      - *AsyncSubject* peut posséder plusieurs valeurs au cours du temps, mais celle-ci n'est émise seulement qu'à la complétion du /stream/
      - *BehaviorSubject* permet d'émettre plusieurs valeurs au cours du temps, mais il a besoin d'une valeur initiale et garde en mémoire la dernière valeur émise
      - *ReplaySubject* permet d'émettre plusieurs valeurs au cours du temps, mais il rejoue l'historique des valeurs à chaque abonné
    - C'est le couple *Observable*/*Observer* qui semble le plus indiqué dans ce cas
      - Il permet d'émettre plusieurs valeurs au cours du temps
      - Aucune valeur n'est nécessaire à sa création
      - Si on s'y abonne "en retard", on ne recevra que les prochaines valeurs émises
- MUTE : Ajouter *EditorService*
  - Ajout de *EditorService*
  - Modification de *DocService* pour lire les opérations locales à partir du stream
  - Modification de *EditorComponent* pour émettre les opérations via *EditorService*
- MUTE : Nettoyer les streams
  - À la destruction de *EditorComponent*, le stream /operationStream/ n'est pas automatiquement détruit.
  - Désabonnement aux streams à la destruction de *EditorComponent*
  - Ajout d'une méthode /clean()/ à *DocService* pour déclencher le désabonnement aux streams
  - Appel de /DocService.clean()/ à la destruction de *DocComponent*
- MUTE : Trouver l'origine du bug empêchant d'envoyer les opérations locales
  - Le problème provient plus exactement de la sérialisation de l'opération *LogootSAdd* ou *LogootSDel*
  - Maintenant qu'on utilise la version corrigée de /mute-structs/, j'ai mis à jour la définition du message *Identifier* dans /protobuf/
  - Au lieu d'envoyer une liste de /double/, j'envoie désormais une liste de /int32/
  - Cependant, le /replicaNumber/ fourni par /sigver/ est potentiellement un /double/
    - Voir https://github.com/coast-team/sigver/blob/master/src/sigver.js#L4 et https://github.com/coast-team/sigver/blob/master/src/sigver.js#L130
  - Notification du problème @Philippe
*** Planned
**** DONE MUTE : Refactorer *DocService*
- Actuellement, une partie de la logique de *DocService* se trouve dans *NetworkService*
- La déplacer dans *DocService*
- En profiter pour nettoyer le code
**** DONE MUTE : Ajouter *EditorService*
- Actuellement, *EditorComponent* interagit avec *DocService* pour lui fournir les opérations locales
  - Il expose son stream à *DocService* qui s'y abonne
- Cependant, lorsque *EditorComponent* est détruit (lorsque l'utilisateur retourne sur la liste des documents), *DocService* écoute toujours le stream des opérations locales
- Déplacer le stream des opérations locales dans un service *EditorService*
- Modifier *EditorComponent* pour qu'il émette les opérations par le biais de *EditorService*
**** DONE MUTE : Nettoyer les streams
- Actuellement, les différents *Services* et *Components* communiquent par le biais de /streams/
- Cependant, ceux-ci ne sont pas nettoyés notamment à la destruction des *Components*
  - Ceci semble provoquer des erreurs lors de la détection des changements puisqu'on essaie de mettre à jour vues qui ont été détruites
  - Et il s'agit bien évidemment d'une fuite mémoire
- Se désabonner des streams à la destruction des *Components*
- Se désabonner des streams à la destructions des *Services*
**** DONE MUTE : Trouver l'origine du bug empêchant d'envoyer les opérations locales
- De temps en temps, l'éditeur plante après la génération de la 1ère opération locale
- Cependant aucune erreur n'est générée
- Ce bug semble lié à l'envoi de l'opération aux autres pairs
  - Mais ce bug se déclenche même lorsqu'on édite le document seul
- Trouver l'origine de ce bug
** Semaine du <2017-01-03 mar.> au <2017-01-06 ven.>
*** Done
- MUTE : Ajouter le stockage au sein du navigateur du document
  - Plusieurs librairies sont disponibles pour utiliser une base de données au sein du navigateur
    - [[https://pouchdb.com/][PouchDB]]
      - Semble porté par sa communauté
      - Celle-ci est importante (<2017-01-03 mar.>: 8022 stars, 236 contributeurs)
    - [[https://localforage.github.io/localForage/][localForage]]
      - Porté par *Mozilla*
      - Dispose d'une communauté importante (<2017-01-03 mar.>: 7103 stars)
    - [[http://jio.readthedocs.io/en/latest/][jIO]]
      - Porté par *Nexedi*
      - Dispose de connecteurs pour stocker les données sur un service distant
  - Ces librairies reposent principalement sur /IndexedDB/, mais possèdent un mécanisme de fallback sur /WebSQL/ ou même /LocalStorage/
  - Elles proposent toutes les 3 une syntaxe reposant sur les /Promises/ ou les /callbacks/
  - Pour le moment, utilise *jIO*
  - Ajout du service *StorageService*
    - Instancie une base de données
    - Dispose d'une méthode /put()/ pour enregistrer un objet
    - Dispose d'une méthode /get()/ pour récupérer un objet
  - Modification de *DocService*
    - Lorsque le document est mis à jour, utilise *StorageService* pour stocker cette version du document
    - Lorsqu'on crée une session de collaboration, initialise le document à partir de sa version stockée si possible
- TVPaint : Étudier la gestion des ressources dans BonitaSoft
  - Pour gérer les organisations, groupes, rôles et utilisateurs, il faut passer par *BPM Studio*
    - Organization > Manage
    - Un utilisateur appartient à un groupe ou plusieurs groupes d'une organisation et possède un rôle dans chacun de ces groupes
  - Attribuer les tâches (/actor mapping/)
    - Dans l'onglet *Actors* du workflow, on peut créer et supprimer les différents filtres utilisés pour faire du /actor mapping/
    - C'est ensuite dans *Configuration* que l'on peut ajouter un comportement aux filtres
      - Possibilité de filtrer par groupe, rôle ou appartenance
      - Possibilité de définir un ensemble d'utilisateurs spécifiques
- TVPaint : Étudier comment interagir avec BonitaSoft par API
  - S'authentifier
    - POST http://localhost:8728/bonita/loginservice
    - Paramètres:
      - /username/
      - /password/
      - /redirect/: un booléen indiquant si on souhaite être dirigé après l'exécution de la requête
	- /redirectURL/ si /redirect/ est égal à /true/
    - Exemple:
      - POST http://localhost:8728/bonita/loginservice?username=walter.bates&password=1234&redirect=true
    - Réponse:
      - Le cookie obtenu dans la réponse est l'élément important qui permettra d'authentifier les requêtes suivantes
  - Récupérer l'identifiant de l'utilisateur
    - GET http://localhost:8728/bonita/API/identity/user
    - Paramètres
      - /f/: le champ avec lequel on souhaite filtrer
    - Exemple:
      - GET http://localhost:8728/bonita/API/identity/user?f=userName=walter.bates
    - Réponse:
      #+BEGIN_SRC
[
  {
    "firstname": "Walter",
    "icon": "icons/default/icon_user.png",
    "creation_date": "2016-10-03 10:58:50.106",
    "userName": "walter.bates",
    "title": "Mr",
    "created_by_user_id": "-1",
    "enabled": "true",
    "lastname": "Bates",
    "last_connection": "2017-01-05 11:42:22.132",
    "password": "",
    "manager_id": "3",
    "id": "4",
    "job_title": "Human resources benefits",
    "last_update_date": "2016-10-03 10:58:50.106"
  }
]
      #+END_SRC
  - Récupérer les tâches disponibles ou assignées à l'utilisateur
    - GET http://localhost:8728/bonita/API/bpm/humanTask
    - Paramètres:
      - /f/: le champ avec lequel on souhaite filtrer
    - Exemple:
      - GET http://localhost:8728/bonita/API/bpm/humanTask?f=assigned_id=4
    - Réponse:
      #+BEGIN_SRC
[
  {
    "displayDescription": "",
    "executedBy": "0",
    "rootContainerId": "3002",
    "assigned_date": "2017-01-05 13:17:45.454",
    "displayName": "Create movie project",
    "executedBySubstitute": "0",
    "dueDate": "2017-01-05 12:25:15.246",
    "description": "",
    "type": "USER_TASK",
    "priority": "normal",
    "actorId": "315",
    "processId": "5578352443955206281",
    "caseId": "3002",
    "name": "Create movie project",
    "reached_state_date": "2017-01-05 11:25:15.251",
    "rootCaseId": "3002",
    "id": "60004",
    "state": "ready",
    "parentCaseId": "3002",
    "last_update_date": "2017-01-05 11:25:15.251",
    "assigned_id": "4"
  }
]
      #+END_SRC
    - Autre exemple:
      - GET http://localhost:8728/bonita/API/bpm/humanTask?state=waiting
    - Réponse:
      #+BEGIN_SRC
[
  {
    "displayDescription": "",
    "executedBy": "0",
    "rootContainerId": "3013",
    "assigned_date": "",
    "displayName": "Submit storyboard",
    "executedBySubstitute": "0",
    "dueDate": "2017-01-05 18:06:12.223",
    "description": "",
    "type": "USER_TASK",
    "priority": "normal",
    "actorId": "318",
    "processId": "5720704175050565481",
    "caseId": "3013",
    "name": "Submit storyboard",
    "reached_state_date": "2017-01-05 17:06:12.228",
    "rootCaseId": "3013",
    "id": "60038",
    "state": "ready",
    "parentCaseId": "3013",
    "last_update_date": "2017-01-05 17:06:12.228",
    "assigned_id": ""
  }
]
      #+END_SRC
  - Assigner une tâche
    - PUT http://localhost:8728/bonita/API/bpm/humanTask/:humanTaskID
    - Exemple:
      - PUT http://localhost:8728/bonita/API/bpm/humanTask/60038
        #+BEGIN_SRC
{
  "assigned_id":"4"
}
        #+END_SRC
    - Réponse:
      - Réponse vide, juste le status *200* témoigne du bon fonctionnement de la requête
  - Effectuer une tâche
    - POST http://localhost:8728/bonita/API/bpm/userTask/:userTaskID/execution
    - Paramètres:
      - Dans l'URL
	- /userTaskID/: l'identifiant de la tâche que l'on exécute
      - Dans le corps de la requête
	- Les éléments du formulaire si nécessaire
    - Exemple:
      - POST http://localhost:8728/bonita/API/bpm/userTask/60037/execution
        #+BEGIN_SRC
{
  "name":"Scott Pilgrim 2",
  "width":1920,
  "height":1080,
  "framerate":24
}
	#+END_SRC
    - Réponse:
      - Réponse vide, juste le status *204* témoigne du bon fonctionnement de la requête
- TVPaint : Définir le business process correspondant à la réalisation d'un film
  - Présentation du business process lors de la réunion du <2017-01-06 ven.>
  - Reste à développer la tâche /compositing/
- MUTE : Ajouter la gestion des documents stockées en local
  - Ajout d'un service *StorageManagerService*
    - Garde une liste des services de stockage
    - Expose une référence vers le service de stockage courant
  - Ajout d'une classe abstraite *AbstractStorageService*
    - S'enregistre auprès de *StorageManagerService*
    - Expose deux fonctions:
      - /isReachable(): Promise<boolean>/
      - /getDocuments(): Promise<any>/
  - Modification de *BotStorageService* pour hériter de *AbstractStorageService*
  - Ajout du component *StorageComponent*
    - Possède en paramètre un *AbstractStorageService*
    - Vérifie si le service de stockage est disponible
    - Permet d'accéder à la liste de ses documents si c'est le cas
    - Se désactive sinon
  - Modification de *NavComponent*
    - Récupère dorénavant la liste des services de stockage à l'aide de *StorageManagerService*
    - Instancie un *StorageComponent* pour chacun d'entre eux
  - Modification de *DocsComponent*
    - Récupère dorénavant la liste des documents à l'aide de *StorageManagerService*
    - Reste à voir comment uniformiser le comportement des différents services de stockage
      - Récupération du document
      - Sauvegarde
*** Planned
**** DONE MUTE : Ajouter le stockage au sein du navigateur du document
**** DONE TVPaint : Étudier la gestion des ressources dans BonitaSoft
- Voir pour générer des rôles
- Voir pour générer des acteurs
- Voir comment attribuer les tâches
**** DONE TVPaint : Étudier comment interagir avec BonitaSoft par API
- Voir comment spécifier le début et la fin d'une tâche par API
**** DONE TVPaint : Définir le business process correspondant à la réalisation d'un film
- Le process a été décrit sommairement dans l'image suivante
  #+CAPTION: Réalisation d'un film
  #+NAME:   fig:WorkflowExample2.png
  [[file:img/WorkflowExample2.png]]
- Le réaliser sous *Bonita Studio*
  - Réfléchir aux données nécessaires
  - Voir comment représenter la réalisation des shots et des clips
** Semaine du <2017-01-09 lun.> au <2017-01-13 ven.>
*** Done
- RH : Durée maximale de CDDs à Inria
  - J'ai vu avec le service RH concernant la durée maximale que je peux passer en CDD à Inria
  - Elle est de 5 ans et 11 mois
  - Par contre, les thèses sont sous un autre régime
  - Je peux donc effectuer une thèse auprès Inria après mon contrat
- MUTE : Ajouter la gestion des documents stockées en local
  - Renommage de *StorageService* en *LocalStorageService*
  - Modification de *StorageManagerService*
    - Injecte *LocalStorageService* et *BotStorageService* dans son constructeur
    - Les enregistre lui-même dans sa liste des services de stockage
    - Sinon, tant que ces services ne sont pas injectés par ailleurs, ils ne sont pas instanciés et enregistrés
  - Modification de *LocalStorageService*
    - Ajout de /getDocuments()/ pour renvoyer la liste des documents stockés en local
  - Modification de *NetworkService*
    - Ajout de la méthode /cleanWebChannel()/
      - Auparavant effectué au cours du /join()/
      - Séparation de ces 2 méthodes
  - Modification de *DocComponent*
    - On avait un problème lorsqu'on passait d'un document à l'autre
    - Les *Services* sont instanciés à leur 1ère utilisation et sont conservés jusqu'à la fin de la session
    - Cependant les *Components* eux sont détruits dès qu'ils ne sont plus affichés, et de nouveau instanciés lorsqu'on y accède de nouveau
    - Si on change d'URL mais qu'il s'agit toujours du même *Component*, on conserve l'instance courante
    - Ainsi, lorsqu'on passait d'un document à l'autre, il arrivait que les services n'arrivent plus à communiquer entre eux
      - Certains streams étaient coupés à la destruction de *DocComponent*
    - Ou qu'au contraire les services continuent d'envoyer des données correspondant à un autre document
      - Notamment lorsqu'on revenait à la page de gestion des documents
    - Destruction du *WebChannel* lors de la destruction de *DocComponent*
    - Destruction du *WebChannel* précédent et génération d'un nouveau lors du passage d'un document à un autre
*** Planned
**** DONE MUTE : Ajouter la gestion des documents stockées en local
- Ajouter une page listant les documents stockées en local
- Elle devrait permettre d'accéder à ces documents et de les supprimer
** Semaine du <2017-01-16 lun.> au <2017-01-20 ven.>
*** Done
- MUTE : Refactoring du couplage entre les services
  - Mapping des entrées/sorties des services dans le *Module* qui les fournit
- MUTE : Rendre générique l'utilisation des *StorageServices* par *DocService*
  - Puisque les services fonctionnent dorénavant par le biais d'entrées/sorties, cette tâche n'est plus pertinente
- MUTE : Ajouter le mécanisme de join propre
  - Ajout de *SyncService*
    - Ajoute à chaque opération émise un couple /id/ et /clock/
    - Maintient en parallèle un vecteur d'état indiquant pour chaque pair la /clock/ correspondant à sa plus récente opération reçue
    - Lorsqu'on rejoint un document, émet ce vecteur d'état à un pair
    - Celui-ci compare notre vecteur d'état avec le sien pour déterminer quelles sont les opérations que nous avons manquées
  - Ajout de *SyncMessageService*
    - Il s'agit du composant chargé de l'envoi et de la réception des messages de synchronisation
    - Observe les sorties de *SyncService*, les encode avec /Protobuf/ et utilise *NetworkService* pour les transmettre aux autres pairs
    - Observe *NetworkService* pour récupérer les messages lui étant destinés et instancie à partir de ces données les objets attendus par *SyncService*
  - Ajout de *SyncStorageService*
    - Il s'agit du composant chargé d'enregistrer l'état de *SyncService* pour le réutiliser à la prochaine session
    - Observe les sorties de *SyncService* pour récupérer son état et le stocker
    - Observe *NetworkService* pour détecter le document courant, récupérer l'état correspondant et le transmettre à *SyncService*
  - Ajout du mapping de ces services dans *DocModule*
*** Planned
**** DONE MUTE : Refactoring du couplage entre les services
- Actuellement, les services sont couplés
  - On injecte dans le constructeur d'un service des instances des autres services dont il dépend
- Ainsi, ajouter un nouveau composant telle que *SyncService* implique de modifier les services qui vont interagir avec lui
- Refactorer les services pour qu'ils exposent leurs entrées et sorties sous forme de streams
- Mapper les entrées/sorties des services dans un composant d'/orchestration/
**** DONE MUTE : Ajouter le mécanisme de join propre
**** CANCELLED MUTE : Rendre générique l'utilisation des *StorageServices* par *DocService*
- Actuellement, l'utilisation de *BotStorageService* et *LocalStorageService* est implémentée en dur dans le code de MUTE
- De même, leur comportement est différent
  - Le bot de stockage se comporte comme un utilisateur
  - Alors que *LocalStorageService* fonctionne par le biais d'appels à une API
- Uniformiser leur utilisation par *DocService*
** Semaine du <2017-01-23 lun.> au <2017-01-27 ven.>
*** Done
- MUTE : Ajouter la seconde partie du mécanisme de synchronisation
  - Ajout de *Interval* qui représente les opérations qu'il nous manque d'un pair
    - /id/ identifie le pair
    - /begin/ et /end/ délimitent l'interval des /clocks/ qu'il nous manque
  - Modification du message *REPLYSYNC* pour qu'il comporte aussi un tableau d'*Intervals*
  - Génération de ce tableau lors du traitement du message *QUERYSYNC* par *SyncService*
- MUTE : Refactorer la liste des documents
  - Besoin de mettre en place un service intermédiaire *DocsStorage*
  - Ce service devrait
    - Récupérer les listes de documents provenant des différents services de stockage
    - Fusionner les listes de documents pour ne conserver qu'une seule occurrence par document
*** Planned
**** DONE MUTE : Ajouter la seconde partie du mécanisme de synchronisation
- Actuellement, lorsqu'on se connecte, on envoie son vecteur d'état à un pair
- Celui-ci détecte les opérations qu'il possède mais qu'il nous manque à partir de ce vecteur et du sien
- Il nous envoie ces opérations
- Il peut aussi détecter les opérations que l'on possède mais qu'il lui manque à partir de ces mêmes informations
- Ajouter cette seconde passe
** Semaines du <2017-01-30 lun.> au <2017-02-17 ven.>
*** Done
- MUTE : Extraire la logique interne de MUTE dans une librairie /mute-core/
  - Librairie disponible ici: https://github.com/coast-team/mute-core
  - Comporte *CollaboratorsService*, *DocService*, *SyncService* et *SyncMessageService*
  - Ajout de l'interface *MessageEmitter* permettant de spécifier le protocole et d'exposer
    - Les messages à broadcaster
    - Les messages à envoyer à un pair particulier
    - Les messages à envoyer à un seul pair, mais choisi aléatoirement
- MUTE : Corriger le déclenchement du message /QuerySync/
  - Ajout d'un setter /setJoinAndStateSources(joinSource, stateSource?)/
    - S'agit de la fusion des setters /set joinSource/ et /set storedStateSource/
  - Le traitement des 2 sources étant lié, il est nécessaire de lier leur affectation
  - /stateSource/ est un paramètre optionnel
  - C'est sa présence ou non qui permet de définir le comportement à adopter
- MUTE : Refactorer *EditorComponent* pour gérer correctement les tableaux de *TextOperations*
  - La méthode /CodeMirror.Editor.operation(fn: () => void)/ est employée ici
  - Consiste à exécuter un appel de la méthode passée en paramètre
  - Elle permet de modifier le contenu de l'éditeur *CodeMirror* sans que celui-ci ne déclenche de rafraîchissements
  - Ceci permet donc d'effectuer un grand nombre d'opérations avant d'afficher uniquement l'état final
  - Amélioration des performances de l'application
    - Initialisation plus rapide (de quelques secondes à 1 seule) de l'éditeur lors de la synchronisation via un pair ou le système de stockage avec 100+ opérations
- MUTE : Ajouter la suppression d'un ou plusieurs documents depuis la liste des docs
  - Ajout de /delete (name: string): Promise<void>/ et /deleteAll (): Promise<void>/ à *AbstractStorageService*
  - Modification de *DocsComponent* pour utiliser ces fonctions
  - Utilisation de *MdSnackBar* pour afficher le message d'erreur si une se produit
    - Besoin de respecter un délai (~500ms) entre chaque message sous peine de faire planter l'UI
*** Planned
**** DONE MUTE : Extraire la logique interne de MUTE dans une librairie /mute-core/
- Actuellement, la logique de l'application se trouve directement dans *MUTE*
- Ceci pose des problèmes de duplication du code lorsqu'il s'agit d'écrire
  - un bot
  - une version /React/ de *MUTE*
- Extraire la logique interne et la déplacer dans une librairie /mute-core/ permettrait une meilleure réusabilité du code
**** DONE MUTE : Adapter /mute/ pour utiliser /mute-core/
- Une fois /mute-core/ implémentée, il est nécessaire de refactorer /mute/ pour
  - Supprimer les services obsolètes/dupliqués dans /mute-core/
  - Instancier *MuteCore* et effectuer le mapping de ses entrées/services avec les composants de /mute/
  - Gérer la destruction de l'instance de *MuteCore*
**** DONE MUTE : Refactorer *CursorsService* en *CursorsDirective*
- Un *Service* Angular ne devrait pas affecter l'interface graphique selon moi
- Cependant, *CursorsService* le fait
- L'utilisation d'un *Component* ou d'une *Directive* est plus indiquée dans ce cas de figure
- Ici, il s'agirait plutôt d'une *Directive*
  - On souhaite ajouter un comportement à un *Component* existant, l'éditeur
- Ceci permettra en plus
  - De passer des paramètres à la directive via les /@Input()/
  - De profiter des lifehooks proposés par Angular
**** DONE MUTE : Remplacer les couples *Observable/Observers* par des *Subjects*
- Jugeant que *AsyncSubject*, *BehaviorSubject* et *ReplaySubject* ne correspondaient pas à ce que je voulais faire, j'ai utilisé à la place des couples *Observable/Observers*
  - Génère et expose un *Observable*
  - Maintient la liste des *Observers* qui ont /subscribe()/ à l'*Observable*
- Ceci complexifie inutilement le code
  - Lors de la génération de l'*Observable*, on retrouve généralement le code suivant
    #+BEGIN_SRC
    this.observable = Observable.create((observer) => {
      // Ajout du nouvel observer à la liste des observers lors d'un subscribe()
      this.observers.push(observer)
    })
    #+END_SRC
  - Puis, lors de l'émission d'une nouvelle valeur
    #+BEGIN_SRC
    this.observers.forEach((observer) => {
      observer.next(valeur)
    })
    #+END_SRC
- Philippe vient de m'apprendre qu'il existe le type *Subject* qui implémente déjà ce comportement
- Utiliser *Subject* à la place de ces couples
**** DONE MUTE : Remplacer les /Observable.map()/ par des *Subscriptions* + *Subjects*
- Actuellement, certains *Observables* exposés par des composants de l'application sont générés en effectuant un mapping sur un flux en entrée
- Cependant ceci pose plusieurs problèmes
  - Le composant ne possède pas le moindre contrôle sur l'*Observable* généré par le mapping
    - Il se contente de transformer les valeurs qu'il reçoit et les retransmettre
    - Il ne peut donc pas émettre d'évènement /complete/ si besoin, notamment lors de sa destruction
  - Le flux de sortie n'est pas généré tant que le flux d'entrée n'a pas été passé au composant
    - Ainsi, essayer de s'abonner au flux de sortie avant d'avoir fourni le flux d'entrée génèrera une erreur
- Ce comportement me paraît donc /error-prone/
- Il serait préférable d'instancier en interne des *Subjects* et de les exposer sous forme d'*Observables*
- Le composant doit donc s'abonner aux flux d'entrées, effectuer les opérations de transformation et émettre les nouvelles valeurs via les *Subjects*
**** DONE MUTE : Corriger le déclenchement du message /QuerySync/
- La condition de déclenchement du message /QuerySync/ est un peu particulière
  #+BEGIN_SRC
  if storageService
    wait(JoinEvent, IsReadyEvent) // où IsReadyEvent indique quand on a fini d'initialiser le document à partir de la version stockée dans le système de stockage
  else
     wait(JoinEvent)
  #+END_SRC
- Actuellement, le code gérant ce déclenchement est brouillon
- Améliorer ça
**** DONE MUTE : Coder en dur le nom des services lorsqu'ils émettent des messages
- Un message est composé de contenu mais aussi d'un champ /service/
- Ce champ permet aux services, lorsqu'ils reçoivent un message, de filtrer uniquement les messages les concernant
- Actuellement, lorsqu'un service émet un message, il renseigne ce champ en utilisant /this.constructor.name/ afin que ce nom soit unique
- Cependant, lorsqu'on build l'application pour le mode production, une minification du code est effectuée
- Et il semblerait que suite à cette minification, plusieurs services se retrouvent avec un constructeur de même nom
- Les services essaient donc de parser et de traiter des messages qui ne leurs sont pas destinés
- Remplacer les /this.constructor.name/ par des constantes
**** DONE MUTE : Refactorer /mute-core/ pour pouvoir traiter des tableaux de *LogootSOperations*
- Comme on émet les *LogootSOperations* une par une, /mute-core/ est designé pour les recevoir une par une
- Ainsi, lorsqu'une *RichLogootSOperation* est reçue
  - Elle est transformée en *LogootSoperation* par *SyncService*
  - Elle est appliquée au modèle par *DocService*
  - Puis *DocService* émet la *TextOperation* correspondante
- Cependant, nous pouvons recevoir plusieurs *RichLogootSOperations* d'un coup maintenant
  - Lors de l'initialisation en utilisant le système de stockage
  - Lors de la synchronisation avec un autre pair
- Nous envoyons donc une multitude de messages inutilement
- Ces messages peuvent eux-mêmes déclencher des rafraichissements de l'interface
**** DONE MUTE : Refactorer *EditorComponent* pour gérer correctement les tableaux de *TextOperations*
- Maintenant que *DocService* émet plusieurs *TextOperations* en un seul message, il est nécessaire d'adapter *EditorComponent*
- Améliorer les performances de l'application en désactivant le rafraichissement de *CodeMirror* dès que son contenu est modifié
  - Rafraichir *CodeMirror* qu'une fois qu'on a fini d'appliquer l'ensemble des *TextOperations*
**** DONE MUTE : Ajouter la suppression d'un ou plusieurs documents depuis la liste des docs
- Maintenant que nous proposons une liste des documents, il est nécessaire de pouvoir la gérer
- Il faut donc pouvoir supprimer un ou plusieurs documents
** Semaine du <2017-02-20 lun.> au <2017-02-24 ven.>
*** Done
- CRDT : Lire *CRDT notes*
  - Pessimistic Replication
    - Système de lock
    - Lorsqu'un client essaie d'accéder ou de modifier la donnée repliquée, un système de synchronisation empêche les réplicas de diverger
    - Semble lourd (quid de la disponibilité ?) mais semble utilisable dans certains cas
      #+BEGIN_QUOTE
      But, in local area networks, it can be acceptable because the latency to contact another replica is low.
      #+END_QUOTE
  - Active Replication
    - Lorsqu'un client essaie de modifier la donnée répliquée, l'ensemble des replicas est modifié au cours de la transaction
      - Ne peut pas avoir une divergence des replicas
*** Planned
** Semaine du <2017-02-27 lun.> au <2017-03-03 ven.>
*** Done
- TVPaint
  - Récupérer la liste des process
    - GET http://localhost:8728/bonita/API/bpm/process?s
    - Paramètres
      - /s/: le champ avec lequel on souhaite filtrer
    - Exemple:
      - GET http://localhost:8728/bonita/API/bpm/process?s=Demo Movie Management
    - Réponse:
      #+BEGIN_SRC
[
  {
    "displayDescription": "",
    "deploymentDate": "2017-02-27 16:55:39.076",
    "displayName": "Demo Movie Management",
    "name": "Demo Movie Management",
    "description": "",
    "deployedBy": "4",
    "id": "5736144163858952046",
    "activationState": "ENABLED",
    "version": "1.0",
    "configurationState": "RESOLVED",
    "last_update_date": "2017-02-27 16:55:39.208",
    "actorinitiatorid": "502"
  },
  {
    "displayDescription": "",
    "deploymentDate": "2017-02-20 15:37:44.819",
    "displayName": "Movie Management",
    "name": "Movie Management",
    "description": "",
    "deployedBy": "4",
    "id": "9107737410621326053",
    "activationState": "ENABLED",
    "version": "1.0",
    "configurationState": "RESOLVED",
    "last_update_date": "2017-02-20 15:37:45.103",
    "actorinitiatorid": "401"
  }
]
      #+END_SRC
  - Récupérer le contrat d'instantiation d'un process
    - GET http://localhost:8728/bonita/API/bpm/process/:processId/contract
    - Exemple:
      - GET http://localhost:8728/bonita/API/bpm/process/5736144163858952046/contract
    - Réponse:
      #+BEGIN_SRC
{
  "inputs": [],
  "constraints": []
}
      #+END_SRC
  - Instancier un process
    - POST http://localhost:8728/bonita/API/bpm/process/:processId/instantiation
    - Exemple:
      - POST http://localhost:8728/bonita/API/bpm/process/5736144163858952046/instantiation
    - Réponse:
      #+BEGIN_SRC
{
  "caseId": 5004
}
      #+END_SRC
- MUTE : Modifier *MuteCore* pour qu'il n'attende plus un *JoinEvent* pour démarrer
  - Ajout de /initSubject/ dans *MuteCore* et de la méthode /init(key: string)/ pour signaler que le modèle est initialisé
  - Modification de *DocService* pour dépendre de *InitEvent* et non plus *JoinEvent*
  - Modification de *SyncService* pour récupérer l'id de l'utilisateur lors de la construction de l'objet
  - Modification de *NetworkService* pour déclencher le join après l'initialisation
  - Modification de *StorageService* pour récupérer l'état après l'initialisation
  - Modification de *DocComponent*
    - Génération de l'id de l'utilisateur
      - On ne dispose à ce moment d'aucune information sur les autres pairs
      - Il faut donc générer un identifiant (*int32*) et espérer qu'il soit unique
      - /Math.random()/ permet de le faire
      - Mais utiliser un générateur de nombres aléatoires à sécurité cryptographique permet de s'assurer un comportement plus aléatoire
      - [[https://developer.mozilla.org/en-US/docs/Web/API/RandomSource/getRandomValues][getRandomValues()]] permet de faire ça dans le browser
*** Planned
**** DONE MUTE : Modifier *MuteCore* pour qu'il n'attende plus un *JoinEvent* pour démarrer
- Actuellement, *JoinEvent* est l'évènement initial qui permet au modèle
  - De s'instancier
  - De récupérer l'état stocké en local
  - De se synchroniser avec un pair
- Cet évènement est seulement déclenché lorsqu'on rejoint le réseau
- Si un problème empêche l'utilisateur de rejoindre le réseau, alors l'éditeur est inutilisable
- Alors que rien n'empêche de travailler en mode déconnecté pour le moment et de tenter de se reconnecter plus tard
- Modifier la logique interne de *MuteCore* pour
  - Ajouter un *InitEvent* indiquant au modèle
    - De s'instancier
    - De récupérer l'état stocké en local
  - *JoinEvent* ne permettra que de déclencher une synchronisation
** Semaine du <2017-03-06 lun.> au <2017-03-10 ven.>
*** Done
- MUTE : Refactorer la liste des documents
  - Philippe s'en est chargé
- MUTE : Ajouter la gestion du titre du document
  - Un simple *LWW-Register* suffit
  - La seule difficulté concerne la génération du timestamp utilisé pour ordonner les titres
  - Une horloge de Lamport ne semble pas suffire
    - Permet d'assurer que pour des opérations /a/ et /b/ et une fonction /C(x)/ retournant le timestamp d'une opération /x/, alors /a -> b => C(a) < C(b)/
    - Mais ne permet pas de s'assurer que /C(a) < C(b) => a -> b/
    - En effet /C(a) < C(b)/ peut aussi indiquer que les opérations sont en concurrence
  - Besoin d'ajouter un élément pour créer un ordre total
    - Dans le cas d'une égalité entre les horloges, l'identifiant de l'utilisateur peut être utilisé pour ordonner les timestamps
    - Exemple pour deux timestamps /<ts, id>/ et /<ts', id'>/ :
      #+BEGIN_SRC
      if (ts > ts') return <ts, id>
      if (ts' > ts) return <ts', id'>
      if (id > id') return <ts, id>
      return <ts', id'>
      #+END_SRC
- Thèse : Discussion avec Claudia
  - Discussion autour du sujet de thèse
  - Continuer les travaux autour du modèle de confiance
  - Amélioration de l'expérience basée sur le trust game
    - Ajout d'un mécanisme de réputation
      - La confiance est
      - La réputation, elle, est établie sur un groupe d'utilisateurs
    - Comparaison du comportement des utilisateurs par rapport à la confiance et/ou à la réputation
  - Application du modèle de confiance à l'édition collaborative
    - Evaluation de la qualité des contributions d'un utilisateur
    - Déterminer les compromis d'une collaboration
    - Prédiction des actions d'un utilisateur en fonction
    - Gestion des droits d'accès par rapport au score de confiance
- Thèse : Discussion avec Gérald
  - Problème de renaming pas adapté à l'équipe
    - Pas exactement le domaine de compétence et le type de problème habituellement traité
  - Idée d'interactions entre un système externe et un CRDT
    - Un document et son modèle CRDT correspondant pourraient être stockés dans git
    - Un utilisateur, avec une application externe, pourrait vouloir modifier le document
    - Comment détecter les changements et mettre à jour son modèle CRDT
    - Comment transmettre ces modifications à l'éditeur
  - Faire le lien avec les cahiers scientifiques
    - De plus en plus de cahiers scientifiques sont rédigés pour des raisons de reproductibilité
      - Plusieurs formats: IPython, ORG-mode
    - Pourraient être rédigés de façon collaborative
    - Embarque des morceaux de code à exécuter
    - Où exécuter les tâches ?
      - Peut vouloir déléguer l'exécution des tâches si elles sont lourdes
      - Retrouve la notion de bot ?
    - Besoin de répliquer les données de l'expérience si délégation de l'exécution
- MUTE : Débugger la liste des documents
  - [ ] Réinitialiser *NavComponent* lors de son affichage
    - Dans ce cas, sélectionner un dossier déclencherait une mise à jour de /activeFile/
  - [ ] Distinguer /activeDoc/ et /activeFolder/
    - *DocsComponent* a besoin de connaitre /activeFolder/ à son initialisation pour générer la liste des documents
    - *NavComponent* permet de mettre à jour /activeFolder/ lorsqu'un changement du dossier sélectionné est détecté
    - *DocComponent* peut mettre à jour /activeDoc/ à son initialisation et à sa destruction
    - *ToolbarComponent* a besoin de connaitre /activeFolder/ et /activeDoc/
      - Si /activeDoc/ est défini, alors affiche ses infos
      - Sinon, affiche les infos de /activeFolder/
      - Peut facilement être implémenté avec /combineLatest/ de *RxJS*
        #+BEGIN_SRC
        activeFolderSource
         .combineLatest(activeDocSource)
         .map(([folder: Folder, doc: Doc]) => {
           if (doc) {
             return doc
           }
           return folder
         })
         .subscribe((file: File) => {
           // Do something
         })
        #+END_SRC
- PIDR : Réflexion avec Quentin et Gérald
  - Lien entre l'automate qu'on souhaite obtenir et le fonctionnement interne d'une chaîne de Markov par exemple
  - La question était "Pourquoi ne pas effectuer les actions du bot directement plutôt que de générer un fichier ?"
    - Reproductibilité de l'expérience
    - Permet de découpler le modèle d'apprentissage et l'implémentation du comportement
    - Comme le fichier suit le formalisme d'une grammaire, peut facilement utiliser un parser pour implémenter le comportement
- PIDR : Réflexion avec Vinh et Le
  - Le résultat de l'analyseur de traces doit être une machine à états ou sa représentation
    - Sinon, de quoi pourrait-il s'agir ?
      - Une séquence d'actions ?
	- Dans ce cas, on aurait un ou plusieurs comportements fixés pour un groupe d'utilisateurs donné
	- Est-ce représentatif ?
	- Mais au moins, ça serait reproductible
  - Une chaîne de Markov a pour but de prédire la prochaine action en fonction de l'état actuel
    - L'état actuel doit contenir toutes les informations nécessaires pour faire le choix
  - Une chaîne de Markov ne correspondrait donc pas si ce que l'on cherche à obtenir est un automate
  - Conseillent de regarder plutôt du côté du /process mining/
    - Vinh recommande [[http://www.processmining.org/prom/start][Prom]]
    - Permettrait d'extraire une machine à états pondérée à partir de séquences d'actions
- MUTE : Débugger l'ouverture d'un nouveau document
  - Bug dû à l'enchainement des évènements
  - Actuellement, lorsque *EditorComponent* détecte un changement de l'instance *DocService*, il s'abonne à /onDocValue/ pour récupérer la nouvelle valeur
  - Lorsqu'une nouvelle valeur du document lui est fourni, il met à jour le contenu du document
  - Mais depuis la refonte pour utiliser /onInit/, /onDocValue/ est déclenché avant que *EditorComponent* se soit abonné
  - Attendre que *EditorComponent* soit initialisé pour appeler /muteCore.init()/ dans *DocComponent*
*** Planned
**** CANCELLED MUTE : Refactorer la liste des documents
- Actuellement, on affiche la liste de tous les objets contenus dans la BDD pour former la liste des documents
- Sauf que certains objets ne devraient pas en faire partie
  - Les données conservées pour chaque document pour *SyncService* par exemple
- De plus, on différencie les documents en fonction des systèmes de stockage où ils sont conservées
- Cela peut s'avérer perturbant pour les utilisateurs
- Unifier la liste des documents
**** DONE MUTE : Débugger la gestion des documents
- Depuis la mise à jour de la liste des documents, la sauvegarde en locale des documents n'est plus disponible
- On enregistre les meta-données du document
  - Titre
  - Clé
  - Stockages utilisés
- Mais plus son contenu
- Débugger ça
** Semaine du <2017-03-13 lun.> au <2017-03-17 ven.>
*** Done
- MUTE : Débugger l'ouverture d'un nouveau document
  - Ajout de l'évènement /isReady/ dans *EditorComponent*
    - Déclenché initialement lors de /ngOnInit()/
    - Puis ensuite à chaque /ngOnChanges()/
  - Modification de *DocComponent* pour qu'il attende cet évènement pour déclencher /muteCore.init()/
- MUTE : Affichage du digest du document
  - Modification de *DocService*
    - Ajout du stream /onDocDigest/
    - Permet d'émettre la valeur du digest
    - Le digest est généré une fois dans l'état /idle/ depuis 1s
      - Permet d'éviter de le calculer à chaque modification locale ou distante
  - Modification de *DocComponent*
    - S'abonne au stream /onDocDigest/
    - Transmet les différentes valeurs à *UiService*
  - Modification de *DevLabelComponent*
    - Récupère le digest par le biais de *UiService*
    - Affiche le digest
- TVPaint : Implémenter le prototype orchestrant TVPaint server et Bonita
  - Erreur lors du lancement du programme
    - =error while loading shared libraries: libPocoFoundation=
  - POCO s'avère être un ensemble de librairies pour développer des applications web en C++
    - https://github.com/pocoproject/poco
  - Installation de POCO
    #+BEGIN_SRC
    ./configure
    make -s
    sudo make install
    #+END_SRC
  - Mise à jour de /ldconfig/
    - =sudo ldconfig=
  - Le serveur démarre dorénavant
    - Si exécuté en tant qu'utilisateur: /Instruction non permise/
    - Si exécuté en tant que root: rend la main mais ne semble pas démarré
  - Jordan a réussi à débugger le serveur
    - Le problème semblait provenir de librairies qui étaient incluses dans le build mais non nécessaires
  - Initialisation du serveur
    - Ne pas cliquer sur le 1er bouton !
    - Besoin de générer la BDD au 1er lancement
      - Create New Database -> Warp Factor 2.5
      - Sélectionner un dossier vide qui va contenir la BDD
      - Crash de l'appli normal
    - 2nd lancement
      - Sélectionner la base de données précédemment créée dans la liste de droite
  - Serveur dorénavant disponible à [[http://localhost:9980]]
  - API de TVPaint Server
    - Colorspaces
      - Récupérer la liste des colorspaces
        #+BEGIN_SRC
$ curl http://localhost:9980/colorspace
<html>
<head>
<title>Colorspace list</title>
</head>
<body>
<ul>
    <li><a href="/colorspace/8199c8bd-35d8-44d8-8992-1c5a6d091817">8199c8bd-35d8-44d8-8992-1c5a6d091817</a></li>
    <li><a href="/colorspace/7920bf82-9054-47a9-b248-f1f4de67e619">7920bf82-9054-47a9-b248-f1f4de67e619</a></li>
    <li><a href="/colorspace/cf95b451-abae-4194-8cb2-30fee386d4d9">cf95b451-abae-4194-8cb2-30fee386d4d9</a></li>
    <li><a href="/colorspace/70201f7f-c4aa-40df-99f7-0da45ddd6bbd">70201f7f-c4aa-40df-99f7-0da45ddd6bbd</a></li>
    <li><a href="/colorspace/35b6b6a6-533f-4b0f-92da-cd142d057ae4">35b6b6a6-533f-4b0f-92da-cd142d057ae4</a></li>
    <li><a href="/colorspace/7a8357db-595c-4379-80c7-ae4f0d17c474">7a8357db-595c-4379-80c7-ae4f0d17c474</a></li>
    <li><a href="/colorspace/56546542-3c5c-482b-9e2d-0bd7d674ccb2">56546542-3c5c-482b-9e2d-0bd7d674ccb2</a></li>
    <li><a href="/colorspace/7a832038-9c47-481d-8fc5-0cba01875471">7a832038-9c47-481d-8fc5-0cba01875471</a></li>
    <li><a href="/colorspace/7335efb6-a311-458a-8e6e-adbe6bc35dc2">7335efb6-a311-458a-8e6e-adbe6bc35dc2</a></li>
    <li><a href="/colorspace/426a891a-ed0b-4c06-8614-0aa893ca8d33">426a891a-ed0b-4c06-8614-0aa893ca8d33</a></li>
    <li><a href="/colorspace/87089522-ff78-4e99-81b1-24240595592a">87089522-ff78-4e99-81b1-24240595592a</a></li>
    <li><a href="/colorspace/3790cd83-9e7b-4e8b-9356-6b6e78bb1ca7">3790cd83-9e7b-4e8b-9356-6b6e78bb1ca7</a></li>
    <li><a href="/colorspace/652e0344-33f3-4f20-ac43-dcfb2493e623">652e0344-33f3-4f20-ac43-dcfb2493e623</a></li>
    <li><a href="/colorspace/51658695-398b-44cf-9591-b35ab304d1b7">51658695-398b-44cf-9591-b35ab304d1b7</a></li>
    <li><a href="/colorspace/32c28afa-f8e7-441b-81ec-2599cf5091ce">32c28afa-f8e7-441b-81ec-2599cf5091ce</a></li>
    <li><a href="/colorspace/809f1634-1a8f-45bd-87bf-8aec9c67a621">809f1634-1a8f-45bd-87bf-8aec9c67a621</a></li>
    <li><a href="/colorspace/040b7ff1-1975-4e03-93c9-0a15671cfe41">040b7ff1-1975-4e03-93c9-0a15671cfe41</a></li>
    <li><a href="/colorspace/4d1ac0d3-5943-457d-901d-95322a8d39c7">4d1ac0d3-5943-457d-901d-95322a8d39c7</a></li>
    <li><a href="/colorspace/a2b11f5c-0144-4691-a92a-529a594e929f">a2b11f5c-0144-4691-a92a-529a594e929f</a></li>
</ul>
</body>
</html>
        #+END_SRC
    - Clips
      - Récupérer la liste des clips
        #+BEGIN_SRC
$ curl http://localhost:9980/clip
<html>
<head>
<title>Clip list</title>
</head>
<body>
<ul>
    <li><a href="/clip/c8635487-54a0-4cc3-ba6a-774a4c3f2628">c8635487-54a0-4cc3-ba6a-774a4c3f2628</a></li>
</ul>
</body>
</html>
        #+END_SRC
      - Récupérer un clip
	#+BEGIN_SRC
$ curl http://localhost:9980/clip/c8635487-54a0-4cc3-ba6a-774a4c3f2628
<object class="::nClip::cClip" instanceauid="c8635487-54a0-4cc3-ba6a-774a4c3f2628">
    <state>
        <data2state>
            <object class="::nData2::nStructure::cStructPersistentState">
                <state mName="data2state" mValue="void" />
                <children>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentname">
                            <mValue format="string-utf8">clip1</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcopyright">
                            <mValue format="string-utf8">@copyleft</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcomment">
                            <mValue format="string-utf8">@So cute :)</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cInteger">
                        <state mName="width" mValue="100" mStep="1" />
                    </object>
                    <object class="::nData2::nValue::cInteger">
                        <state mName="height" mValue="200" mStep="1" />
                    </object>
                    <object class="::nData2::nValue::cDouble">
                        <state mName="framerate" mValue="24" mStep="1" />
                    </object>
                    <object class="::nData2::nValue::cInteger">
                        <state mName="currentframe" mValue="0" mStep="1" />
                    </object>
                    <object class="::nDocument::nData2::nValue::cFavorites">
                        <state mName="favorites">
                            <mValue>
                                <mRoot>
                                    <mName format="string-utf8">Root</mName>
                                    <mContent />
                                </mRoot>
                            </mValue>
                        </state>
                    </object>
                </children>
            </object>
        </data2state>
        <userdata>
            <object class="::nData2::{Data2.UserData.cpp}::cUserDataStructPersistentState">
                <state mName="userdata" mValue="void" />
                <children>
                    <object class="::nData2::nStructure::cStruct">
                        <state mName="timeline" mValue="void" />
                        <children>
                            <object class="::nData2::nValue::cDouble">
                                <state mName="framestart" mValue="0" mStep="1" />
                            </object>
                            <object class="::nData2::nValue::cDouble">
                                <state mName="framesize" mValue="20" mStep="1" />
                            </object>
                            <object class="::nData2::nValue::cInteger">
                                <state mName="newlayerindex" mValue="0" mStep="1" />
                            </object>
                        </children>
                    </object>
                </children>
            </object>
        </userdata>
    </state>
    <relations>
        <UsedInShots kind="unordered">
            <target object="ea7a9721-4291-4043-b589-37e0932a806b" />
        </UsedInShots>
        <RootLayer kind="1">
            <target object="dd7abe7c-5a02-4ed0-aca2-ea41fd779445" />
        </RootLayer>
        <Colorspace kind="1">
            <target object="8199c8bd-35d8-44d8-8992-1c5a6d091817" />
        </Colorspace>
        <StoredIn kind="unordered">
            <target object="aac82be6-d624-46c0-98a8-ec3209f87c4d" />
        </StoredIn>
        <Content kind="unordered" />
    </relations>
</object>
        #+END_SRC
      - Créer un clip
        #+BEGIN_SRC
$ curl -X POST -d name=clip1 -d width=100 -d height=200 -d framerate=24 -d colorspace=8199c8bd-35d8-44d8-8992-1c5a6d091817 http://localhost:9980/clip
<newobject class="::nClip::cClip" instanceauid="1b00f271-1754-4d71-8f2f-24e49a568fb0" />
        #+END_SRC
      - Associer un clip à un shot
        #+BEGIN_SRC
$ curl -X PUT http://localhost:9980/shot/ea7a9721-4291-4043-b589-37e0932a806b/append/c8635487-54a0-4cc3-ba6a-774a4c3f2628
<html>
<head>
<title>OK</title>
</head>
<body>
LINK ESTABLISHED</body>
</html>
        #+END_SRC
    - Shots
      - Récupérer la liste des shots
        #+BEGIN_SRC
$ curl http://localhost:9980/shot
<html>
<head>
<title>Shot list</title>
</head>
<body>
<ul>
    <li><a href="/shot/ea7a9721-4291-4043-b589-37e0932a806b">ea7a9721-4291-4043-b589-37e0932a806b</a></li>
    <li><a href="/shot/bde3202e-5dcf-4e5d-9a2c-b014abfc76b8">bde3202e-5dcf-4e5d-9a2c-b014abfc76b8</a></li>
    <li><a href="/shot/3c773691-f34c-4950-94a1-bc8b4e4a66c3">3c773691-f34c-4950-94a1-bc8b4e4a66c3</a></li>
</ul>
</body>
</html>
        #+END_SRC
      - Récupérer un shot
        #+BEGIN_SRC
$ curl http://localhost:9980/shot/ea7a9721-4291-4043-b589-37e0932a806b
<object class="::nFilm::cShot" instanceauid="ea7a9721-4291-4043-b589-37e0932a806b">
    <state>
        <data2state>
            <object class="::nData2::nStructure::cStructPersistentState">
                <state mName="data2state" mValue="void" />
                <children>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentname">
                            <mValue format="string-utf8">shot1</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcopyright">
                            <mValue format="string-utf8">@copyleft</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcomment">
                            <mValue format="string-utf8">@So cute :)</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cDoubleBounded">
                        <state mName="opacity" mValue="1" mStep="1" mMinValue="0" mMaxValue="1" />
                    </object>
                    <object class="::nData2::nValue::cEnum">
                        <state mName="backgroundmode" mValue="::nClip::eBackground::kNone" />
                    </object>
                    <object class="::nData2::nValue::cColor">
                        <state mName="backgroundcolor" mValue="[0.964203,1.000000,0.824890,1.000000]" />
                    </object>
                    <object class="::nData2::nValue::cColor">
                        <state mName="backgroundcolor1" mValue="[0.436035,0.222473,0.013916,1.000000]" />
                    </object>
                    <object class="::nData2::nValue::cColor">
                        <state mName="backgroundcolor2" mValue="[0.385101,0.716919,0.097076,1.000000]" />
                    </object>
                    <object class="::nDocument::nData2::nValue::cFavorites">
                        <state mName="favorites">
                            <mValue>
                                <mRoot>
                                    <mName format="string-utf8">Root</mName>
                                    <mContent />
                                </mRoot>
                            </mValue>
                        </state>
                    </object>
                </children>
            </object>
        </data2state>
        <userdata>
            <object class="::nData2::{Data2.UserData.cpp}::cUserDataStructPersistentState">
                <state mName="userdata" mValue="void" />
                <children>
                    <object class="::nData2::nStructure::cStruct">
                        <state mName="timeline" mValue="void" />
                        <children>
                            <object class="::nData2::nValue::cDouble">
                                <state mName="framestart" mValue="0" mStep="1" />
                            </object>
                            <object class="::nData2::nValue::cDouble">
                                <state mName="framesize" mValue="20" mStep="1" />
                            </object>
                            <object class="::nData2::nValue::cInteger">
                                <state mName="currentframe" mValue="0" mStep="1" />
                            </object>
                        </children>
                    </object>
                </children>
            </object>
        </userdata>
    </state>
    <relations>
        <Clips kind="ordered">
            <target object="c8635487-54a0-4cc3-ba6a-774a4c3f2628">
                <properties>
                    <property name="plane" object="5f57bd5e-b57a-4f87-96f9-de2e48edb0f9" />
                </properties>
            </target>
        </Clips>
        <Parents kind="unordered">
            <target object="3fac5b0e-fcd6-4432-837d-b1252dd8285b" />
            <target object="3fac5b0e-fcd6-4432-837d-b1252dd8285b" />
        </Parents>
        <StoredIn kind="unordered">
            <target object="b6ff362e-7001-4328-8079-4e82fa698501" />
        </StoredIn>
        <Content kind="unordered" />
    </relations>
</object>
	#+END_SRC
      - Créer un shot
        #+BEGIN_SRC
$ curl -X POST -d name=shot1 http://localhost:9980/shot
<newobject class="::nFilm::cShot" instanceauid="0d52f072-f6bb-4271-9cd0-669e4ad7ec36" />
        #+END_SRC
      - Associer un shot à un film
        #+BEGIN_SRC
$ curl -X PUT http://localhost:9980/film/9491a3d8-1e2e-4b54-a6d7-806de9c2d767/append/ea7a9721-4291-4043-b589-37e0932a806b
<html>
<head>
<title>OK</title>
</head>
<body>
LINK ESTABLISHED</body>
</html>
        #+END_SRC
    - Films
      - Récupérer la liste des films
	#+BEGIN_SRC
$ curl http://localhost:9980/film
<html>
<head>
<title>Film list</title>
</head>
<body>
<ul>
    <li><a href="/film/9491a3d8-1e2e-4b54-a6d7-806de9c2d767">9491a3d8-1e2e-4b54-a6d7-806de9c2d767</a></li>
    <li><a href="/film/997a7a0e-b6de-44a7-9c5d-ebe45903bab1">997a7a0e-b6de-44a7-9c5d-ebe45903bab1</a></li>
</ul>
</body>
</html>
        #+END_SRC
      - Récupérer un film
        #+BEGIN_SRC
$ curl http://localhost:9980/film/9491a3d8-1e2e-4b54-a6d7-806de9c2d767
<object class="::nFilm::cFilm" instanceauid="9491a3d8-1e2e-4b54-a6d7-806de9c2d767">
    <state>
        <data2state>
            <object class="::nData2::nStructure::cStructPersistentState">
                <state mName="data2state" mValue="void" />
                <children>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentname">
                            <mValue format="string-utf8">tagada</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcopyright">
                            <mValue format="string-utf8">@copyleft</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcomment">
                            <mValue format="string-utf8">@So cute :)</mValue>
                        </state>
                    </object>
                </children>
            </object>
        </data2state>
        <userdata>
            <object class="::nData2::{Data2.UserData.cpp}::cUserDataStructPersistentState">
                <state mName="userdata" mValue="void" />
                <children>
                    <object class="::nData2::nStructure::cStruct">
                        <state mName="timeline" mValue="void" />
                        <children>
                            <object class="::nData2::nValue::cDouble">
                                <state mName="zoom" mValue="1" mStep="1" />
                            </object>
                            <object class="::nData2::nValue::cInteger">
                                <state mName="newshotindex" mValue="0" mStep="1" />
                            </object>
                        </children>
                    </object>
                </children>
            </object>
        </userdata>
    </state>
    <relations>
        <RootScene kind="1">
            <target object="3fac5b0e-fcd6-4432-837d-b1252dd8285b" />
        </RootScene>
        <CameraSettings kind="1">
            <target object="1ee1f8e0-0dca-4d2e-9208-8c579e59150a" />
        </CameraSettings>
        <StoredIn kind="unordered">
            <target object="0f76db8d-1ca3-4e05-8ede-a84d7e6cc74b" />
        </StoredIn>
        <Content kind="unordered" />
    </relations>
</object>
        #+END_SRC
      - Créer un film
        #+BEGIN_SRC
$ curl -X POST -d name=tagada http://localhost:9980/film
<newobject class="::nFilm::cFilm" instanceauid="997a7a0e-b6de-44a7-9c5d-ebe45903bab1" />
        #+END_SRC
  - Problème pour récupérer le lien entre le film et ses shots
    - Actuellement, un film comporte une *RootScene*
    - C'est en récupérant les informations de la *RootScene* que nous sommes capables de retrouver les shots du film
      #+BEGIN_SRC
$ curl http://localhost:9980/object/3fac5b0e-fcd6-4432-837d-b1252dd8285b
<object class="::nFilm::cRootScene" instanceauid="3fac5b0e-fcd6-4432-837d-b1252dd8285b">
    <state>
        <data2state>
            <object class="::nData2::nStructure::cStructPersistentState">
                <state mName="data2state" mValue="void" />
                <children>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentname">
                            <mValue format="string-utf8">@Untitled-07</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcopyright">
                            <mValue format="string-utf8">@copyleft</mValue>
                        </state>
                    </object>
                    <object class="::nData2::nValue::cWString">
                        <state mName="documentcomment">
                            <mValue format="string-utf8">@So cute :)</mValue>
                        </state>
                    </object>
                </children>
            </object>
        </data2state>
    </state>
    <relations>
        <Film kind="1">
            <target object="9491a3d8-1e2e-4b54-a6d7-806de9c2d767" />
        </Film>
        <Children kind="ordered">
            <target object="ea7a9721-4291-4043-b589-37e0932a806b" />
            <target object="ea7a9721-4291-4043-b589-37e0932a806b" />
            <target object="ea7a9721-4291-4043-b589-37e0932a806b" />
        </Children>
        <Parents kind="unordered" />
        <StoredIn kind="unordered" />
        <Content kind="unordered" />
    </relations>
</object>
      #+END_SRC
    - Mais cette URL est censée être temporaire
    - Comment récupérer la liste des shots d'un film à terme ?
      - Semblerait que les shots conservent une référence vers la *RootScene*
      - Utiliser ce lien ?
  - En fait, on peut s'en sortir sans avoir besoin d'accéder directement à la *RootScene*
    - Il suffit de vérifier que la *RootScene* référencée dans *Film* est bien aussi référencée par les *Shots*
  - Parser du XML en JS
    - Une librairie semble se démarquer: [[https://github.com/Leonidas-from-XIV/node-xml2js]]
    - Exemple d'utilisation
      #+BEGIN_SRC
this.cookieRequest.get("http://localhost:9980/film/9491a3d8-1e2e-4b54-a6d7-806de9c2d767",
  (error, response, body) => {
     xml2js.parseString(body, function (err, result) {
     console.dir(result);
  });
      #+END_SRC
    - Résultat
      #+BEGIN_SRC
{ object:
   { '$':
      { class: '::nFilm::cFilm',
        instanceauid: '9491a3d8-1e2e-4b54-a6d7-806de9c2d767' },
     state: [ [Object] ],
     relations: [ [Object] ] } }
      #+END_SRC
  - L'authentification avec Bonita
    - Besoin d'ajouter /redirect=false/ à la query string
    - C'est la seule manière de récupérer un code de status de 500 si l'authentification échoue
    - Sans ce paramètre, peu importe le résultat de l'authentification, le code de status est de 200
      - Il faut dans ce cas comparer le contenu de la réponse pour déterminer si l'authentification a réussi ou échoué
- CRDT : Lire *A Conflict-Free Replicated JSON Datatype*
  - Objectifs:
    - Tous les replicas doivent converger vers le même état
    - Aucun input ne doit être perdu
      - Exclu les *LWW-Registers*
    - Fonctionne avec un réseau P2P
    - Les communications peuvent être chiffrées
  - Le format JSON est une structure à plusieurs niveaux
    - Implique qu'il faut pouvoir gérer des opérations concurrentes à différents niveaux de l'arbre
  - Comportement
    - Si modifie un registre en concurrence, conserve l'ensemble des valeurs (*Multi-Value Register*)
    - Si modifie une liste en concurrence, merge les listes
    - Si modifie un registre composé de plusieurs registres, merge les modifications
      - Du coup, on peut se retrouver avec un scénario /bizarre/
      - Si un utilisateur supprime l'objet alors qu'un utilisateur met à jour une de ses propriétés en concurrence
      - Le résultat de la fusion sera alors que la propriété mise à jour
      - L'objet n'est plus cohérent
- Emacs : Configuration de l'éditeur
  - Activation du gestionnaire de packages *MELPA*
    #+BEGIN_SRC
    (require 'package) ;; You might already have this line
    (add-to-list 'package-archives '("melpa" . "https://melpa.org/packages/"))
    (when (< emacs-major-version 24)
      ;; For important compatibility libraries like cl-lib
      (add-to-list 'package-archives '("gnu" . "http://elpa.gnu.org/packages/")))
    (package-initialize) ;; You might already have this line
    #+END_SRC
  - Installation de packages
    - Affichage de la liste des paquets: =M-x list-packages=
    - Installer un paquet:
      - Touche =i= pour marquer le paquet comme à installer
      - Puis =x= pour quitter le menu et passer à l'installation
    - Activer un thème: =M-x load-theme=
  - Liens intéressants
    - Sur les commandes de Emacs: https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf
    - Sur les fonctionnalités de base de *ORG-MODE*: http://orgmode.org/worg/org-tutorials/org4beginners.html
- PLM : Ajouter [[plumbr.eu][Plumbr]] à PLM
  - Ajout de nouvelles images Docker
    - Ajout d'un *VOLUME* contenant le jar de *Plumbr*
    - Modifie *ENTRYPOINT* ou *CMD* pour référencer ce jar
  - Il s'avère que des fichiers sont manquants
    #+BEGIN_SRC
    plm_1           | *************************************************************************************
    plm_1           | * Plumbr is missing the following required properties: serverUrl,logConf,accountId. *
    plm_1           | * Either make sure the plumbr.properties file is present next to plumbr.jar         *
    plm_1           | * or specify individual properties via -D parameters in your startup script.        *
    plm_1           | *                                                                                   *
    plm_1           | * Check out https://plumbr.eu/support/agent-configuration                           *
    plm_1           | * for more information or contact support@plumbr.eu                                 *
    plm_1           | *************************************************************************************
    #+END_SRC
  - Ajout des fichiers manquants
  - En fait, il n'y avait pas besoin de générer des images custom pour utiliser les agents
    - Dans /docker-compose.yml/, on peut ajouter des volumes au conteneur et modifier la commande utilisée
    - Il est donc possible de définir l'utilisation d'agents que dans le /docker-compose.yml/
    - Ceci permet d'avoir une image de base et de personnaliser son comportement en fonction de la config
  - Mise à jour des /docker-compose.yml/
*** Planned
**** DONE MUTE : Débugger l'ouverture d'un nouveau document
- Si on est sur *DocComponent* et qu'on ouvre un nouveau document, le contenu du document n'est pas remis à 0
- Résoudre ce bug
**** DONE MUTE : Affichage du digest du document
- Pour s'assurer que toutes les copies du document convergent, une fonction /digest()/ existe
- Celle-ci retourne un entier représentant le contenu du document
- Afficher le digest du document pour faciliter la comparaison des copies
**** DONE PLM : Ajouter [[plumbr.eu][Plumbr]] à PLM
- S'agit d'une solution de monitoring pour JVM
- Voir comment elle s'intègre avec Docker et si conflit avec New Relic
** Semaine du <2017-03-20 lun.> au <2017-03-24 ven.>
*** Done
- MUTE : Écrire la partie /Re-synchronisation hors-ligne et collaboration ad-hoc/
  - Explication du couple /<id, clock>/ ajouté à chacune des opérations
  - Explication du vecteur d'états
  - Explication du comportement de l'algo à la réception d'une opération
  - Explication du comportement de l'algo à la réception d'un vecteur d'état
- Discussion avec Weihai
  - Problème de collision d'identifiants
    - Dans la théorie, on admet que l'identifiant de site est unique et sert de désambiguiteur
    - En pratique, il s'avère que générer cet identifant unique n'est pas trivial
      - Comme nous supportons les partitions, nous n'avons pas accès à l'ensemble des identifiants des autres pairs lors de la connexion
      - Nous pouvons donc pas nous assurer que l'identifiant que nous générons n'est pas partagé avec un autre utilisateur
    - Pour réduire les chances d'obtenir une collision d'identifiant, nous pouvons utiliser un identifiant long
    - Mais comme l'identifiant de site est utilisé pour générer les identifiants de blocs, ceci impacte les performances du modèle
      - Existe-t-il des études sur l'impact de la taille de l'identifiant sur les performances?
    - Mais même en utilisant des identifiants longs il sera toujours possible d'obtenir des collisions
    - Il faudrait donc un mécanisme de /recovery/ pour gérer ce cas
  - La probabilité d'obtenir une collision d'identifiants en fonction de la taille de l'identifiant est décrite ici
    - /Birthday attack/: https://en.wikipedia.org/wiki/Birthday_attack
  - Mais obtenir une collision sur l'identifiant de site n'est pas une fatalité
  - Le problème survient si plusieurs utilisateurs génère des blocs différents avec le même identifiant
    - La probabilité de cet évènement est plus difficile à déterminer
      - Probabilité d'obtenir une collision d'identifiants
      - Probabilité que les nombres aléatoires générés soit égaux
	- Dépend de l'emplacement de l'insertion
	- Puisque l'interval de génération du nombre aléatoire dépend du bloc précédent et du suivant
      - Probabilité que les horloges logiques soient égales
  - L'idée de Weihai est de
    - Utiliser des identifiants de site par session plutôt qu'à vie pour limiter la casse
    - Posséder pour chaque session
      - Un identifiant de site long, qui servira de désambiguiteur
      - Un identifiant de site court, qui servira pour générer les opérations
    - Lorsqu'un utilisateur démarre une session, son identifiant long est ajouté au log d'opérations
    - Lorsqu'un utilisateur quitte une session, une opération est ajoutée au log
    - Lorsqu'un utilisateur rejoint un réseau, il doit vérifier s'il n'y a pas eu de collisions d'identifiants de bloc
      - Doit reparcourir le log pour faire ça ?
      - Comment identifier la paternité de l'opération dans le cas suivant ?
	- /open longId1 shortId/
	- /open longId2 shortId/
	- /insert shortId/
    - S'il y a eu une collision, utiliser le /longId/ comme désambiguiteur
      - Renaming des opérations ?
      - Suppression des opérations ?
- TVPaint : Implémenter le prototype orchestrant TVPaint server et Bonita
  - Librairies de clients REST
    - [X] request: [[https://github.com/request/request]]
      - Très populaire
      - "Bas" niveau
    - [ ] node-rest-client: [[https://github.com/aacerox/node-rest-client]]
      - Plus élaboré et "haut" niveau que request
      - Largement moins populaire
      - Manque la gestion des cookies
  - Outils de génération de doc
    - [ ] TSDoc: [[https://github.com/xperiments/TSDoc]]
      - Pas maintenu
    - [X] typedoc: [[https://github.com/TypeStrong/typedoc]]
      - Plus populaire
      - Maintenu
      - Mais la documentation de l'outil est peu fournie
      - Commande pour générer la doc : =typedoc --out docs/ --mode file --module commonjs --target ES6 src=
  - Implémentation de *BonitaClient*
    - Fournit des méthodes de base pour interagir avec Bonita
      - /retrieveProcessId()/
      - /instantiateProcess()/
      - /retrieveTaskIds()/
*** Planned
**** DONE MUTE : Écrire la partie /Re-synchronisation hors-ligne et collaboration ad-hoc/
- Dans un des livrables pour *Open-Paas::NG*, une partie porte sur le mécanisme de synchronisation implémenté dans /mute-core/
  - https://github.com/coast-team/mute-core/blob/master/lib-src/sync/SyncService.ts
- Rédiger cette partie
** Semaine du <2017-03-27 lun.> au <2017-03-31 ven.>
*** Done
- MUTE : Ne plus accepter d'opérations tant qu'on est pas synchronisé
  - Ajout d'un attribut /isSync/
    - /false/ par défaut
    - Une fois qu'on a reçu et traité le message /replySync/, /isSync/ passe à /true/
  - Mise en buffer des opérations tant que /isSync/ est /false/
  - Quand /isSync/ passe à /true/, traite les opérations contenues dans le buffer
  - Puis ensuite traite les opérations reçues dès qu'elles arrivent
    - Plus de mise en buffer
- MUTE : Ne plus répondre à des messages /querySync/ tant qu'on est pas synchronisé
  - Réutilise /isSync/
  - Met en buffer les requêtes tant que /isSync/ est /false/
  - Traite les requêtes dès que /isSync/ passe à /true/
  - Puis ensuite traite les requêtes reçues dès qu'elles arrivent
- MUTE : Ne plus accepter d'opérations sans posséder les précedentes
  - Ajout de /isAlreadyApplied()/ pour vérifier si une opération a déjà été délivrée ou non
  - Ajout de /isAppliable()/ pour vérifier si l'on peut délivrer une opération ou non
  - Lorsqu'on reçoit une opération, vérifie si /isAppliable()/
    - Si l'opération est applicable, la délivre
    - Sinon
      - Met l'opération en buffer
      - Set le flag /isSync/ à /false/
	- Dorénavant, les opérations reçues et les demandes de synchronisation seront mise en attente
      - Envoi d'un message /querySync/
*** Planned
**** DONE MUTE : Ne plus accepter d'opérations tant qu'on est pas synchronisé
- Actuellement, on accepte les opérations distantes avant même que l'on se soit synchronisé avec un pair
- Ceci a conduit à un bug où une opération a été délivrée avant que la synchronisation se soit effectuée
  - Ceci est aussi dû au fait que l'on accepte les opérations dès que /clock > v.get(id)/
- Corriger cela
**** DONE MUTE : Ne plus répondre à des messages /querySync/ tant qu'on est pas synchronisé
- De la même façon, on répond aux messages /querySync/ avant même que l'on soit soi-même synchronisé
- Du coup, un utilisateur peut se croire synchroniser, et se retrouver avec un document vide, alors qu'en fait des opérations existent
- Corriger cela
**** DONE MUTE : Ne plus accepter d'opérations sans posséder les précedentes
- Actuellement, une opération est acceptée tant que sa /clock/ est supérieure à celle que l'on connaît pour cet /id/
- Cependant, des opérations intermédiaires peuvent être manquantes
- Accepter cette opération masque donc ce fait
- Résoudre ce problème
** Semaine du <2017-04-03 lun.> au <2017-04-07 ven.>
*** Done
- MUTE : Supprimer le système de flag /isSync/
  - Suppression des systèmes de bufferisation liés à ce flag
  - Simplification de la gestion des opérations distantes reçues
    - Auparavant, si une opération reçue n'était pas applicable, on rejetait la liste d'opérations, marquait /isSync/ comme /false/ et déclenchait le mécanisme d'anti-entropie
    - Les opérations ainsi que les messages /querySync/ reçus pendant ce temps étaient mis en attente
    - Maintenant, si une opération reçue n'est pas applicable, on se contente de mettre l'opération concernée en attente
    - Récupérer l'opération manquante sera la tâche du mécanisme d'anti-entropie
- MUTE : Déclencher le mécanisme de synchronisation périodiquement
  - Tous les 10±5 secondes, envoi un message /querySync/
- MUTE : Ajouter un bouton pour pouvoir facilement exporter le log d'opérations
  - Ajout d'un bouton /Log/ dans *DevLabelComponent*
  - Ajout de la méthode /exportLog()/ dans *DevLabelComponent*
    - Récupére le /docID/ du document courant
    - Extrait le log d'opérations depuis la base de données
    - Génère un *ObjectURL* à partir de ce log pour pouvoir le télécharger
    - Génère un nom de fichier de la forme /log-<docID>-<digest>.json/
    - Met à jour une balise /<a>/ pour pointer vers ce fichier
    - Simule un click de l'utilisateur sur ce lien
- MUTE : Ajouter un bouton pour pouvoir facilement exporter le modèle
*** Planned
**** DONE MUTE : Supprimer le système de flag /isSync/
- On a constaté plusieurs problèmes liés à ce flag
- Le 1er est qu'il existe un scénario dans lequel aucun pair n'est marqué comme synchronisé
  - Ceci se produit si le 1er utilisateur quitte sans avoir répondu au message /querySync/ du 2nd utilisateur
  - Le 2nd utilisateur n'ayant jamais reçu de /replySync/, son flag /isSync/ est bloqué à /false/
  - Ainsi, chaque opération reçue ou /querySync/ par le 2nd utilisateur sera bufferisée
  - Les utilisateurs qui se connecteront par la suite n'arriveront jamais à se synchroniser dans ce cas
- Le 2nd problème est que le mécanisme de bufferisation s'avère perturbant pour les utilisateurs
  - Soudainement, car une opération qui a été reçue n'a pu être délivrée, l'ensemble des opérations distantes sont mises en attente
  - L'utilisateur ne perçoit donc plus les modifications distantes tant que la synchronisation ne s'est pas de nouveau correctement effectuée
  - De plus, on peut de nouveau tomber dans le cas du 1er problème si le pair avec lequel nous essayons de nous synchroniser se déconnecte sans avoir émis de réponse
- Puisque ce mécanisme semble apporter plus de problèmes qu'il n'en résout, le supprimer
**** DONE MUTE : Déclencher le mécanisme de synchronisation périodiquement
- Puisqu'on a simplifié le mécanisme de synchronisation en retirant le système de flag, on peut dorénavant le déclencher périodiquement
- Ceci permettrait de récupérer les opérations qui n'ayant pas été reçues
**** DONE MUTE : Ajouter un bouton pour pouvoir facilement exporter le log d'opérations
- Lorsqu'une divergence est constatée, nous extrayons les logs d'opérations des différentes copies pour essayer d'identifier l'origine du problème
- Actuellement, j'ai un snippet de code à exécuter directement dans la console du navigateur pour exporter le log
- Ajouter un bouton pour automatiser l'exécution de ce code et faciliter la tâche pour les utilisateurs s'avéra sûrement très utile
**** DONE MUTE : Ajouter un bouton pour pouvoir facilement exporter le modèle
- Nous avons eu un cas particulier où les digests des différentes copies du document divergés, mais où les contenus des copies étaient identiques
- En extrayant les logs d'opérations, nous n'avons pas réussi à reproduire la divergence des digests
- Comparer les modèles nous permettrait d'identifier l'origine de cet étrange résultat
- Logger les différentes versions du modèle me paraît inutilement lourd
- Un bouton pour exporter la sérialisation du modèle me semble plus adapté
** Semaine du <2017-04-24 lun.> au <2017-04-28 ven.>
*** Done
*** Planned
** Semaine du <2017-05-02 mar.> au <2017-05-05 ven.>
*** Done
- <<bug-ct44>> MUTE : Checker le log du document /ct44/ fourni par Victorien avec l'insertion foireuse
  - On trouve dans ce log un problème de causalité
    - L'opération *LogootSDel* supprimant le bloc /{ base: [ 2146186868, -1138834828, 9, 28, 836704880, 669113997, 0 ], begin: 0, end: 0 }/ est exécutée avant l'opération insérant ce bloc
    - L'opération *LogootSDel* est exécutée en 485 tandis que l'opération *LogootSAdd* concernée est exécutée en 506
  - En examinant le déroulement de l'algorithme, nous avons remarqué que l'opération de suppression ci-dessus n'est pas ignorée lorsqu'on ne trouve pas le noeud correspondant
  - À la place, on effectue une suppression dans un autre noeud
  - Ceci entraîne un état inconsistent de cet autre noeud et donc du modèle
  - Des modifications futures de ce bloc échouent donc
- MUTE : Modifier la méthode /delBlock()/ pour gérer le cas où certains identifiants ont déjà été retirés du bloc
- MUTE : Modifier la méthode /delBlock()/ pour ne pas effectuer de suppression dans le cas où le bloc n'existe pas
  - Plus précisement, ce bug était dû au fait que la suppression concernait le bloc résultant d'un split et provenait du comportement de /searchPos()/
  - Dans /delBlock()/, on utilise /searchPos()/ pour trouver le bloc possédant l'identifiant /id/ du début de l'interval à supprimer
  - Dans /searchPos()/
    - On prend la racine comme 1er noeud de comparaison
    - On compare l'identifiant /id/ recherché par rapport aux identifiants de début /id1/ et de fin /id2/ du noeud courant
      - Si /id/ < /id1/ alors on continue la recherche à partir du fils gauche du noeud courant
      - Si /id2/ < /id/ alors on continue la recherche à partir du fils droit du noeud courant
      - Sinon, on considère qu'on a trouvé le noeud comportant l'identifiant recherché et on arrête la recherche
  - Dans notre cas, on comparait un identifiant /id/ avec un noeud délimité par /id1/ et /id2/ tels que
    - /id1/ est de la forme /{ base: [ random1, site1, clock1], last: 0 }/
    - /id2/ est de la forme /{ base: [ random1, site1, clock1], last: 10 }/
    - /id/ est de la forme /{ base: [ random1, site1, clock1, 5, random2, site2, clock2 }, last: 0/
  - Ainsi, on a /id1/ < /id/ < /id2/, /searchPos()/ considère alors qu'on a trouvé le bon noeud
  - Alors que les bases des 2 identifiants sont différentes
  - Modification de /searchPos()/ pour comparer les bases lorsque /id1/ < /id/ < /id2/ pour s'assurer d'avoir trouvé le bon noeud
*** Planned
**** DONE MUTE : Checker le log du document /ct44/ fourni par Victorien avec l'insertion foireuse
- Une opération *LogootSAdd* présente dans ce log d'opération semble avoir mal été générée
- On aurait /begin/ > /end/
- Ce qui viole une assertion lors de son application au document
- Vérifier que l'assertion levée correspond bien au cas où /begin/ > /end/
- Si c'est le cas, trouver l'origine de cette opération malformée
**** DONE MUTE : Modifier la méthode /delBlock()/ pour gérer le cas où certains identifiants ont déjà été retirés du bloc
- Il est possible que plusieurs opérations de suppression concurrentes essaient de supprimer la même partie d'un bloc
- À l'heure actuelle, cela entraîne un /split()/
**** DONE MUTE : Modifier la méthode /delBlock()/ pour ne pas effectuer de suppression dans le cas où le bloc n'existe pas
- Comme remarqué dans [[bug-ct44]], si un bloc est introuvable lorsqu'on essaie de le supprimer, il s'avère que /delBlock()/ effectue une suppression dans un autre bloc
- Ceci rend inconsistent le modèle
- Il est donc nécessaire de modifier /delBlock()/ pour ne plus effectuer de suppression si la recherche du bloc n'aboutit pas
** Semaine du <2017-05-09 mar.> au <2017-05-12 ven.>
*** Done
- TVPaint : Implémenter le prototype orchestrant TVPaint server et Bonita
  - <<tvpaint-frontend-transaction>>Comme me l'a fait remarqué Quentin, la robustesse du front-end est primordiale
    - C'est lui qui pilote les /transactions/
      - Exemple avec la création d'un film:
	- Requête *TVPaint Server* pour créer la ressource
	- Récupère l'identifiant de film contenu dans la réponse du serveur
	- Requête *Bonita* pour instancier le processus avec en paramètre l'identifiant du film
    - Actuellement si une erreur survient au cours de l'échange, la transaction n'est pas annulée
    - Des incohérences peuvent donc apparaître en cas de panne
      - Présence dans la BDD de films associés à aucun processus par exemple
  - Plusieurs philosophies possibles pour le front-end
    - [ ] S'efface au maximum, minimise son rôle
      - Agit uniquement comme un proxy quand possible
	- Par exemple lors d'une requête pour récupérer une ressource
      - Ne comprend pas les données qui transitent
	- Pas de notion de *Movie* et autres
      - Mais parser les données s'avère nécessaire dans certains cas
	- Besoin de lire la réponse de *TVPaintServer* pour extraire l'identifiant lors de la création d'une nouvelle ressource
      - Finalement son comportement est bâtard
    - [X] Connaît en détail la logique de l'application
      - Connaît le type des données qui transitent, ou du moins une version allégée
	- Notion de *Movie* et autres
	- Capable d'instancier ces objets à partir des réponses de TVPaint Server
	- Capable de réécrire ces objets sous un autre format
      - Mais dans ce cas, il est plus fortement couplé à TVPaint Server
        - Modifier le type *Movie* implique une mise à jour du front-end
  - Pour le prototype, j'ai choisi la 2nde option
    - Me paraît plus cohérent comme fonctionnement
  - Ajout d'un client REST générique *RestClient*
    - Construit à partir du /hostname/ et du /port/ du service interrogé et du /name/ de la ressource
    - Pour une ressource d'un type *T* donné, permet de
      - récupérer la liste des identifiants des objets de type *T* avec /getResources(): string[]/
      - récupérer le détail d'une ressource avec /getResource(id: string/): T/
      - créer une nouvelle ressource et récupérer son identifiant avec /post(body: any): string/
  - Ajout d'une extension de ce client REST générique pour interagir spécifiquement avec le serveur de TVPaint, *TVPaintClient*
    - Fournit une implémentation par défaut des méthodes de *RestClient*
    - Requiert une /factory/ pour instancier un objet *T* à partir de la réponse XML, provenant du le serveur de TVPaint, dans /getResource(id)/
  - Mais certaines ressources disposent d'actions supplémentaires
    - On peut associer des *Shots* à un *Movie* et des *Clips* à un *Shot*
  - Il est nécessaire de fournir une nouvelle extension du client REST
  - De nouveau, un choix de conception se pose
    - [ ] Faut-il contraindre avec /append()/ le type de la donnée ajoutée
      - /append(obj: T1, appendee: T2)/
      - Dans ce cas, il faut réussir à exprimer le lien entre *T1* et *T2*
	- Que *T2* est /appendable/ à *T1*
      - Et il faut s'assurer que les objets fournis au front-end lors de cette requêtes soient bien du bon type
	- S'assurer que l'objet /obj/ correspond bien à un *T1* au niveau du serveur TVPaint
    - [X] ou juste se contenter de manipuler des identifiants
      - /append(id1: string, id2: string)/
      - On délègue au serveur TVPaint de vérifier que les objets manipulés sont du bon type
	- Puisque dans tous les cas, il faudra effectuer des vérifications sur les données manipulées au niveau du serveur TVPaint
  - Moyen de récupérer l'ensemble des tâches pour un utilisateur
    #+BEGIN_SRC
$ curl http://localhost:8728/bonita/API/bpm/humanTask?p=0&c=10&f=user_id=4
[
  {
    "displayDescription": "",
    "executedBy": "0",
    "rootContainerId": "14001",
    "assigned_date": "",
    "displayName": "Add shots",
    "executedBySubstitute": "0",
    "dueDate": "2017-05-12 11:40:35.199",
    "description": "",
    "type": "USER_TASK",
    "priority": "normal",
    "actorId": "1601",
    "processId": "8562487203032783503",
    "caseId": "14001",
    "name": "addShots",
    "reached_state_date": "2017-05-12 10:40:35.216",
    "rootCaseId": "14001",
    "id": "280002",
    "state": "ready",
    "parentCaseId": "14001",
    "last_update_date": "2017-05-12 10:40:35.216",
    "assigned_id": ""
  }
]
    #+END_SRC
  - Moyen de récupérer une variable d'un /case/
    #+BEGIN_SRC
$ curl http://localhost:8728/bonita/API/bpm/caseVariable/14001/movieId
{
  "case_id": "14001",
  "name": "movieId",
  "description": "",
  "type": "java.lang.String",
  "value": "1a60e1ad-d14e-4b64-a088-7cc3c63c4bdd"
}
    #+END_SRC
- MUTE : Étudier les logs du document /ct25/ pour trouver l'origine de la divergence
  - En rejouant le log d'opérations /chamber-bazooka-mercy/, pas de problème de causalité
  - Par contre, avec les logs /patron-user-arcade/ et /trapeze-arnold-sheriff/, on en rencontre
    - L'opération 3523 essaie de supprimer un bloc n'existant pas encore dans le cas de /patron-user-arcade/
    - L'opération 1849 essaie de supprimer un bloc n'existant pas encore dans le cas de /trapeze-arnold-sheriff/
  - Il s'agit donc à 1ère vue d'un problème de causalité
*** Planned
**** DONE MUTE : Étudier les logs du document /ct25/ pour trouver l'origine de la divergence
- Une divergence a été constatée à la fin de la réunion du /ct25/
- Comparer les logs pour déterminer son origine
** Semaine du <2017-05-15 lun.> au <2017-05-19 ven.>
*** Done
- PIDR : Étudier et commenter la v2 du rapport
  - Formulation trop familière, vocabulaire trop vague
  - I. Intro
    - A. Contexte
      - Parle de /tests à effectuer sur les éditeurs collaboratifs/, lesquels ?
    - D. Motivations
      - Parle de /travail interdisciplinaire/, pourquoi insister sur le /interdisciplinaire/ ?
      - Revoir le paragraphe sur les différences entre les éditeurs collaboratifs
        - Semble incomplet (parle d'OT mais pas de CRDT)
        - OT ne scale pas, même en pair-à-pair
        - Bien faire la distinction entre algorithmes de réplication et architecture des applications
      - La conclusion de cette partie ne me paraît pas clair
        - Manque une transition avec le reste de la partie
        - Insister sur l'hétérogénéité des applications
        - La réponse ne doit pas "s'adapter", elle doit être générique
      - Finalement l'objectif n'apparaît pas encore clairement
  - II. Etat de l'art
    - A. Différents types d'écriture collaborative
      - Est-ce que les comportements observés dépendent du type d'outil (synchrone ou asynchrone) ?
      - Justifier la citation
      - Intérêt de parler des système d'éditions collaboratifs asynchrones ?
      - Manque la citation vers Docuviz ou autre document illustrant ces comportements
    - C. Représentation du comportement d'un utilisateur
      - 1. Nécessité de cette représentation
	- Pourquoi cette partie existe toujours ?
	- Fusionner avec *Motivations*
      - 2. Réprésentation à l'aide de grammaires
	- Si la figure a été récupérée d'un document, indiquer sa source
    - Manque finalement de recontextualisation
      - Présente des choses (comportements, actions...) mais sans dire comme cela va vous être utile
  - III. Contribution
    - B. Modélisation de la session de travail collaborative
      - Mauvaise position dans l'article
	- Interrompt la réflexion sur les grammaires proposées
      - Pas assez claire
      - Insister sur la notion d'/orchestration/ des bots dans la 1ère approche
        - Le comportement des bots est déterminé par le chef d'orchestre
	- Les bots se contentent de suivre un comportement prédéfini
      - Insister sur la notion de /décentralisation/ des bots dans la 2nde approche
	- Les bots sont capables de déterminer les comportements à employer pour effectuer la tâche
	- Ils sont capables de se répartir le travail
    - C. Ecriture d’une grammaire qui modélise un automate à états finis
      - Des limites à cette nouvelle grammaire ?
  - IV. Validation
    - A. Implémentation de la solution via Python
      - Justification des critères choisis pour la comparaison des AST ?
      - D'après ces critères, recommandez-vous un outil plutôt que l'autre ?
    - Pourquoi une seule sous-partie ?
  - V. Perspectives
    - Est-ce que l'application /permet de voir comment l'éditeur gère les conflits/ ?
    - Pourquoi ces actions /lire/ et /communiquer/ ? Qu'est-ce qu'elles apportent ?
  - VI. Conclusion
    - Pas besoin d'être exhaustif sur les implémentations
    - Revenir sur les contributions en mettant en valeur la principale (la grammaire v2)
    - Le reste a déjà été énoncé dans problématique
- TVPaint: Implémenter le prototype orchestrant TVPaint server et Bonita
  - Modification de GET /film
    - Auparavant, récupérer l'ensemble des films de la BDD
    - Maintenant, liste seulement les films pour lesquels l'utilisateur a des tâches en cours/disponibles
    - Format de la réponse:
      #+BEGIN_SRC
      [ { caseId: '16001',  movieId: 'c803d583-f4b3-47ea-be3d-555ba2dc0da8' } ]
      #+END_SRC
  - Ajout d'une route GET /film/:movieId/task
    - Nécessite de passer en paramètre le /caseId/ qui correspond au /movieId/
    - Permet de récupérer la liste des tâches concernant ce film/cette instance de process pour l'utilisateur courant
    - Format de la réponse:
      #+BEGIN_SRC
      [ { displayDescription: '',
          executedBy: '0',
          rootContainerId: '16001',
          assigned_date: '',
          displayName: 'Add shots',
          executedBySubstitute: '0',
          dueDate: '2017-05-17 12:34:29.730',
          description: '',
          type: 'USER_TASK',
          priority: 'normal',
          actorId: '1901',
          processId: '7285702430994004787',
          caseId: '16001',
          name: 'addShots',
          reached_state_date: '2017-05-17 11:34:29.747',
          rootCaseId: '16001',
          id: '320002',
          state: 'ready',
          parentCaseId: '16001',
          last_update_date: '2017-05-17 11:34:29.747',
          assigned_id: '' } ]
      #+END_SRC
  - Ajout d'un script simulant un client
    - Crée un film
    - Récupère la liste des films
    - Récupère les tâches concernant le film précédemment créé
    - S'assigne la tâche /addShots/
    - Récupère les shots
    - Complète la tâche /addShots/
  - Comportements à discuter
    - GET /film
      - Retourne l'ensemble des films ?
      - Retourne les films en cours de l'utilisateur ?
	- Interroge *Bonita* pour récupérer les tâches associés à l'utilisateur pour savoir quels films le concernent
    - GET /shot et GET /clip
      - Retourne l'ensemble des ressources ?
      - Retourne un sous-ensemble jugé "accessible" par l'utilisateur ?
	- Quels critères utiliser ?
    - Mécanisme de transaction
      - voir [[tvpaint-frontend-transaction]]
*** Planned
**** DONE PIDR : Étudier et commenter la v2 du rapport
** Semaine du <2017-05-22 lun.> au <2017-05-24 mer.>
*** Done
- PIDR : Noter le rapport des étudiants de PIDR
  - Des éléments de fonds intéressants
  - Éléments regrettables
    - La notion de grande échelle de l'expérience est manquante dans l'introduction
      - Il faut attendre la fin de la motivation pour voir apparaître cet aspect
      - Du coup, on ne comprend moins la nécessité de simuler les utilisateurs
    - La liste des actions élémentaires est aussi absente
      - Besoin d'attendre la 1ère grammaire pour connaître ces actions
  - Entachés par la forme
    - Quelques paragraphes mal organisé
      - 1er paragraphe de Contribution
    - Discours trop familier, trop oral
  - Quelques confusions sur le fond semblent aussi persistées
    - Exemples
      - CRDT et P2P
      - L'expérience permettra de voir comment l'éditeur gère les conflits
      - Différents types d'écriture collaborative et Différents types de comportements
    - Est-ce à cause d'une mauvaise maitrise de ces éléments ?
    - Ou juste est-ce qu'ils se sont mal exprimés ?
  - On ressent un manque de rigueur
    - Au niveau du langage utilisé
    - Mais aussi sur le fond présenté
      - Donne des chiffres sur le nombres d'actions par état sans raison apparente
- PIDR : Noter le travail des étudiants de PIDR
  - Points positifs
    - Idée de la grammaire pour représenter l'automate
  - Points négatifs
    - Ressenti un manque d'implication dans le projet
      - Plusieurs semaines, le travail fourni semblait léger par rapport à nos attentes
      - Quand interrogés sur la quantité de travail fourni, les étudiants répondaient qu'ils avaient dû travailler d'autres projets
      - Lors des réunions, du moins au début, la prise de notes était limitée
	- Besoin de nous-même prendre en photo et de leur envoyer l'automate réaliser au tableau au cours d'une réunion
      - Lorsque demandés de chercher des parsers-lexers pour Python, les résultats fournis étaient les 1ers retournés par Google
	- Un d'entre eux ne correspondait absolument pas au projet (API de Python pour modifier le parser du langage)
*** Planned
**** DONE PIDR : Noter le rapport des étudiants de PIDR
**** DONE PIDR : Noter le travail des étudiants de PIDR
**** DONE TVPaint : Implémenter le prototype orchestrant TVPaint server et Bonita
- Devrait orchestrer les 2 serveurs pour jouer les tâches suivantes :
  - Créer un film
  - Associer des shots au film
    - Besoin potentiellement de créer les shots
  - Associer des clips au shot
    - Besoin potentiellement de créer les clips
**** DONE TVPaint : Mener une refléxion sur l'implémentation ou non d'un mécanisme de transaction
- Voir [[tvpaint-frontend-transaction]]
- Est-ce qu'un mécanisme de transaction doit être implémenté ?
- Ou est-ce que l'on peut s'en passer ?
  - En designant le système pour être tolérant aux pannes par exemple
**** DONE TVPaint : Définir un workflow correspondant au scénario de test
**** DONE Thèse : Préparer le dossier de candidature pour la thèse
- Doit fournir les pièces suivantes
  - [X] CV
    - Retravailler la section sur *MUTE* dans *Projets* pour parler de *OpenPaas::NG* à la place
    - Ajouter une section *Logiciels* indiquant la participation aux logiciels
      - Se baser sur [[file:~/T%C3%A9l%C3%A9chargements/softwarecriteria-ce_2011-08-01.pdf][ce document]] pour spécifier l'état de maturité des logiciels
  - [X] Lettre de motivation
  - [X] Lettres de recommandation
  - [X] Sujet de thèse
  - [X] Mémoire d'ingénieur
  - [X] Diplôme TN
  - [X] Notes TN
  - [X] Diplôme DUT
  - [X] Notes DUT
  - [X] Diplôme BAC
  - [X] Notes BAC
** Semaine du <2017-05-29 lun.> au <2017-06-02 ven.>
*** Done
- TVPaint : Rédiger README pour Bonita
  - [X] Décrire la version à installer
    - 7.3
  - [ ] Décrire le rôle des différents diagrammes
  - [X] Décrire comment changer
    - [X] Le port utilisé par Bonita
    - [X] L'utilisateur par défaut
- TVPaint : Rédiger README pour front-end
  - [X] Décrire le processus d'installation
  - [X] Documenter le scénario joué par le script simulant client
  - [ ] Récapituler les questions existantes
    - [ ] Gestion des transactions
    - [ ] Gestion des droits d'accès aux ressources
      - Plus précisement, aux shots et clips
    - [ ] Technologies à utiliser
      - Inciter à regarder du côté de Go
** Semaine du <2017-06-19 lun.> au <2017-06-23 ven.>
*** Done
*** Planned
**** DONE TVPaint : Rédiger README pour Bonita
**** DONE TVPaint : Rédiger README pour front-end
**** CANCELLED MUTE : Débugger la liste des documents
- Lorsqu'on souhaite re-afficher la liste des documents alors que nous étions sur la page d'un document, la liste ne se génère pas dans tous les cas
- La liste de *DocsComponent* est générée à partir de la valeur /activeFile/
  - S'il s'agit d'un *Folder*, récupère et affiche la liste de ses documents
  - Sinon ne fait rien
- La valeur de /activeFile/ est mise à jour à plusieurs endroits
  - Lorsque l'utilisateur sélectionne un nouveau dossier dans *NavComponent*
    - Dans ce cas, /activeFile/ prend pour valeur ce dossier
  - Lorsqu'on accède à un document
    - Alors /activeFile/ prend pour valeur ce document
- Cependant, comme *NavComponent* n'est jamais détruit, il se "souvient" du précédent dossier selectionné
- Ainsi, si on sélectionne le même, aucun changement n'est détecté
  - La valeur de /activeFile/ n'est donc pas mise à jour
- Ainsi, comme /activeFile/ correspond toujours au dernier document ouvert, *DocsComponent* ne génère pas de liste
- Corriger ce comportement
**** CANCELLED TVPaint : Développer la tâche compositing
- La tâche /compositing/ est une activité complexe
- La décomposer en plusieurs sous-tâches
  - Revoir les posts du forum qui y sont dédiés
** Semaine du <2017-07-03 lun.> au <2017-07-07 ven.>
*** Done
- Suivi la formation NVIDIA sur Deep Learning
  - Présentation de l'écosystème NVIDIA
  - Présentation de NVIDIA Digits
    - Environnement d'apprentissage en ligne
  - Formation s'est déroulée sur NVIDIA Digits
    - Bases du Machine Learning
      - Génération d'un dataset
      - Génération d'un réseau de neurones à partir d'un modèle pré-existant
      - Sélection des hyperparamètres
      - Entraînement du modèle
      - Validation
    - Intéressant de revoir les bases mais je pensais qu'on allait aborder des sujets plus bas niveaux
- Suivi la formation NVIDIA sur OpenACC
  - Présentation de l'écosystème NVIDIA
  - Présentation de OpenACC
    - Pour langages C/C++ et Fortran principalement
    - Permet d'ajouter des directives pour que le compilateur rende parallèle des portions du code et les exécutent sur le GPU
      - Pas de refactoring du code
    - Ressemble de fait à OpenMP
  - Suivi d'un tutoriel pour prendre en main OpenACC
    - Mise en évidence du problème de transfert des données de la mémoire du système vers le GPU
      - Transférer inutilement des données peut ralentir le programme au point qu'il soit moins performant que sa version séquentielle tournant sur le CPU
    - Étapes du tuto
      - Détection d'une zone parallélisable
      - Profiling du résultat de la parallèlisation
      - Correction des problèmes de parallélisation initiaux
	- Par exemple, transfert de données inutiles entre le CPU et le GPU
      - Optimisation du réglage des paramètres de parallélisation en fonction de l'environnement cible
	- Par exemple, réglages spécifiques pour un GPU NVIDIA
- MUTE : Ajouter des tests d'intégration se basant sur les logs d'opérations
  - On a finalement peu de logs sur lesquels se baser pour ce genre de tests
  - Généralement, je récupère un log par digest pour observer et comprendre les différences
  - Je n'ai donc peu de logs de différents utilisateurs qui convergent
  - Écriture du test d'intégration
    - Création d'une macro de test /everyLogsConvergeMacro/
      - À partir d'un set de fichiers de log, extrait les opérations et les rejoue pour obtenir le modèle de chaque utilisateur
      - Compare ensuite les digests et contenus des modèles entre eux
      - Permet de vérifier si les documents convergent ou non, en fonction du résultat attendu
    - Génération de 3 tests à partir de cette macro
      - L'ensemble de ces tests reposent sur des logs issus du document /ct3/
      - 1er set : les logs de Claudia et Long convergent
      - 2nd set : les logs de Le et Philippe convergent
      - 3eme set : les logs de Claudia et Philippe ne convergent pas
- MUTE : Corriger /hasPlaceAfter/Before()/ dans le cas où /last/ approche d'une borne
  - Modification de /hasPlaceAfter/Before()/ pour prendre en compte la valeur de /last/
  - Ajout de tests unitaires pour ces cas
*** Planned
**** DONE MUTE : Ajouter des tests d'intégration se basant sur les logs d'opérations
- Puisqu'on dispose de traces d'utilisation sur MUTE, nous pouvons récupérer plusieurs logs d'opérations pour ajouter des tests supplémentaires
- Ces tests consisteraient à rejouer des logs d'opérations différents mais convergents pour s'assurer qu'ils convergent toujours après nos modifications
  - Comment choisir ces logs ?
- Des logs supplémentaires divergents pourraient être ajoutés pour s'assurer que ces scénarios divergent toujours
**** DONE MUTE : Corriger /hasPlaceAfter/Before()/ dans le cas où /last/ approche d'une borne
- /hasPlaceAfter/Before()/ sont utilisées avant un append/prepend pour vérifier qu'on peut effectuer cette opération
- Cependant, ces fonctions ne vérifient pas si append/prepend à ce bloc ne déclencherait pas un over/underflow
** Semaine du <2017-07-10 lun.> au <2017-07-13 jeu.>
*** Done
- MUTE : Refactorer la génération des identifiants par *IdFactory*
  - Modification de la méthode /createBetweenPosition()/
  - Pour le moment, se contente de recopier les 4 éléments du tuple si on ne peut pas générer un nombre aléatoire entre les 2 1ers éléments des tuples
*** Planned
**** DONE MUTE : Refactorer la génération des identifiants par *IdFactory*
- À l'heure actuelle, *IdFactory* implémente la version /optimisée/ de génération des identifiants
- Concrètement, dès qu'un intervalle suffisant est détecté pour générer un nombre aléatoire, *IdFactory* insère le tuple <rnd, replicaNumber, clock, offset>
- Par exemple, avec id1 = [rnd1, replicaNumber1, clock1, offset1] et id3 = [rnd3, replicaNumber3, clock3, offset3], *IdFactory* génère :
  - id2 = [rnd2, replicaNumber2, clock2, last2] si rnd3 - rnd1 > 2
  - id2 = [rnd, rnd2, replicaNumber2, clock2, last2]  si replicaNumber3 - replicaNumber1 > 2
  - id2 = [rnd, replicaNumber1, rnd2, replicaNumber2, clock2, last2] si clock3 - clock1 > 2
  - id2 = [rnd, replicaNumber1, clock1, rnd2, replicaNumber2, clock2, last2] si last3 - last1 > 2
  - id2 = [rnd, replicaNumber1, clock1, last1, rnd2, replicaNumber2, clock2 last2] sinon
- Par la suite, nous allons donc potentiellement comparer des éléments de types différents (rnd avec replicaNumber par exemple)
- Ceci fonctionne car nous utilisons le même type de données pour chacun de ces éléments
- Ceci peut être amené à évoluer
- Il ne faudait qu'effectuer la comparaison sur rnd, et le cas échéant, recopier l'ensemble du préfixe
** Semaine du <2017-07-17 lun.> au <2017-07-21 ven.>
*** Done
- MUTE : Setup Travis CI pour /mute-structs/
  - Pour release facilement la nouvelle version, besoin d'ajouter la clé d'API
  - Pour la chiffrer et l'ajouter directement à la configuration de Travis
    - travis encrypt api_key="myAPIKey" --add deploy.api_key
*** Planned
** Semaine du <2017-07-24 lun.> au <2017-07-28 ven.>
*** Done
- Mettre à jour la version d'un projet via npm
  - npm version major | minor | patch -m "chore: Update version number to %s"
- Mute : Étudier la fusion de /last/ avec /base/
  - Création de *IdentifierTuple*, objet composé de /random/, /replicaNumber/, /clock/ et /offset/
  - Modification de *Identifier* pour remplacer /base/ et /last/ par /tuples: IdentifierTuple[]/
- MUTE : Ajout de *SafeAny*
  - On a remarqué que le type *any* contourne totalement le compilateur de TypeScript, notamment pour la gestion des /null/
  - Exemple :
      #+BEGIN_SRC
      let id: Identifier
      let toto: any = null
      id = toto // Le compilateur devrait indiquer que toto est p-e null
      id.compareTo(...) // Déclenche une NullPointerException
      #+END_SRC
  - Comme *any* ne se comporte pas comme on le souhaite, on a ajouté *SafeAny*
  - Ce type nous permet d'identifier une variable comme *any* mais permet de conserver les indicateurs du compilateur lorsqu'on la manipule
- MUTE : Nettoyage de *RopesNode*
  - Renommage de /offset/ en /actualBegin/ pour rendre plus clair le code
  - Refonte de /maxOffset()/ en /actualEnd/ pour rendre plus clair le code
- MUTE : Nettoyage de *IteratorHelperIdentifier*
  - Renommage de /id1/ et /id2/ en /idInterval1/ et /idInterval2/ pour éviter une confusion sur le type de ces propriétés
- MUTE : Nettoyage de *LogootSBlock*
  - Renommage de /id/ en /idInterval/ pour éviter une confusion sur le type de cette propriété
  - Suppression des paramètres /begin/ et /end/ de /delBlock()/ qui étaient inutilisés
- MUTE : Amélioration des tests effectués sur les logs d'opérations existants
  - Jusqu'à maintenant, on ne vérifiait pas si le log contenait une opération sérialisée incorrectement
  - Ainsi, le test pouvait planter à l'exécution à cause d'une *NullPointerException*
  - Modification du test pour /fail/ proprement si on détecte une opération sérialisée incorrectement
  - Modification du test pour /fail/ proprement si on détecte un log vide
*** Planned
** Semaine du <2017-07-31 Mon> au <2017-08-04 Fri>
*** Done
- MUTE : Ajouter des tests pour *IteratorHelperIdentifier*
  - Ajout d'une méthode /compareBaseMacro()/ permettant d'appeler /compareBase()/ sur 2 *IdentifierInterval* et de comparer le résultat à la valeur attendue
  - Implémentation de tests se basant sur cette macro pour vérifier le comportement de /compareBase()/ pour les différents cas possibles
*** Planned
**** DONE MUTE : Ajouter des tests pour *IteratorHelperIdentifier*
- Cette partie du code est critique puisque c'est elle qui détermine où on insère un bloc par rapport à un autre
- Cependant, aucun test n'est écrit pour /compareBase()/
- Les ajouter
** Semaine du <2017-08-07 Mon> au <2017-08-11 Fri>
*** Done
- MUTE : Ajouter l'intégration continue dans /mute-core/
  - Modification de l'architecture du repo pour coller à celle de /mute-structs/ et /mute/
  - Difficulté avec les fichiers de /Protobuf/
    - À l'origine, les dossiers /src/, /test/ et /proto/ sont au même niveau
    - Mais une fois le code transpilé, on obtient les répertoires /proto/, /dist/src/ et /dist/test/
    - L'import des fichiers de /Protobuf/ échoue donc, le chemin relatif ne correspondant plus
    - Pour le moment, se contente de /cp -r proto dist/proto/
  - Ajout de tests pour *SyncMessageService*
    - [X] Test la sérialisation, envoi puis désérialisation d'une liste de *RichLogootSOperation*
    - [X] Test de la sérialisation, envoi puis désérialisation d'une *QuerySync*
    - [X] Test que le *ReplySyncEvent* en réponse à une *QuerySync* est bien destiné au bon utilisateur
    - [X] Test la sérialisation, envoi puis désérialisation d'un *ReplySyncEvent*
- Soutenance blanche - Quentin TARDIVON
  - "De janvier à août 2017"
    - On ne sait pas que tu as le fait le PIDR, et t'es censé parlé du stage uniquement non ?
  - Slide 6: On veut surtout illustrer nos travaux
  - Slide 7 et 9: Écrire les objectifs ?
  - Slide 12: Mettre la(es) référence(s) ?
  - Slide 15:
    - Reformuler "Mise en place de cette API"
  - Slide 16:
    - Animer le schéma ?
    - Grossir la taille des flèches ?
  - Slide 19:
    - À mon sens, pas clair dans ton discours la différence entre /signaling/ et /P2P/
*** Planned
**** DONE MUTE : Ajouter l'intégration continue dans /mute-core/
- Il serait temps d'ajouter des tests dans /mute-core/
- Ajouter la configuration pour Travis de façon à les exécuter fréquemment
** Semaine du <2017-08-22 Tue> au <2017-08-25 Fri>
*** Done
- MUTE : Ajout de *LogootSOperation* et de *TextOperation*
  - Ajout de ces nouveaux types
  - Pas pu les déclarer dans des fichiers /.d.ts/
  - Ils n'étaient alors pas exportés
  - Il a donc été nécessaire de les déclarer dans des fichiers /.ts/
- MUTE : Amélioration de la génération du /digest/ et du /tree/
  - Création d'un flux /updateSubject/
    - Ce flux indique quand un nouveau /digest/ et un nouveau /tree/ doivent être émis
  - /subscribe/ à ce flux, couplé à l'opérateur /debounceTime/
  - Suppression des /subscribes/ en double liés aux fluxs de *TextOperations* ou de *LogootSOperations*
  - Modification des /subscribes/ aux fluxs de *TextOperations* ou de *LogootSOperations* pour aussi émettre une nouvelle valeur via /updateSubject/
- MUTE : Suppression de /docValueSubject/
- MUTE : Ajouter des tests pour *DocService*
  - Ajout du test /textOperation-correct-send-and-delivery/
    - Connecte 2 instances de *DocService*, /docServiceIn/ et /docServiceOut/
    - Fournit des *TextOperations* locales à l'instance /docServiceIn/
    - Vérifie que /docServiceOut/ émet bien les *TextOperations* distantes correspondantes
*** Planned
**** DONE MUTE : Ajout de *LogootSOperation* et de *TextOperation*
- Actuellement, plusieurs variables sont du type *LogootSAdd | LogootSDel* ou *TextDelete | TextInsert*
- Ces unions de types alourdissent le code
- Ajouter de nouveaux types correspondant à ces unions
**** DONE MUTE : Amélioration de la génération du /digest/ et du /tree/
- Actuellement, le /digest/ et le /tree/ correspondant sont générés suite à la réception d'une *TextOperation* locale ou d'une *LogootSOperation* distante
- Pour éviter de générer des valeurs inutiles, on utilise l'opérateur /debounceTime/ pour attendre que le document se stabilise
- On a donc plusieurs /subscribes/ pour un même flux
  - Un premier appliquant chaque opération reçue au modèle et émettant la ou les opérations résultantes
  - Un second, agrémenté de l'opération /debounceTime/ pour générer le /digest/
- Le fait de /subscribe/ plusieurs fois au même flux à pour effet de dupliquer ce flux, et donc les traitements qui sont appliqués aux données
  - On a pu observer ce comportement étrange via des logs qui se sont retrouvés dupliqués
- Aussi, on observe que les /debounceTime/ n'étant pas liés entre eux, on génère tout de même des /digests/ intermédiaires inutiles
- Améliorer ce comportement
**** DONE MUTE : Suppression de /docValueSubject/
- Cet attribut est utilisé pour émettre le texte correspondant au modèle
- Cette valeur est émise à l'initialisation du document
- Ceci nous permet de réinitialiser le contenu de l'éditeur lorsque l'utilisateur change de document
- Mais le modèle étant vide à son initialisation, la chaîne de caractère l'est aussi
- On peut donc simplifier le comportement de l'éditeur pour réinitialiser son contenu
- Ce flux n'a donc plus d'utilité
- Le supprimer
**** DONE MUTE : Ajouter des tests pour *DocService*
** Semaine du <2017-08-28 Mon> au <2017-09-01 Fri>
*** Done
- Commandes pour la démo
  - Effectuer le build de la démo
    - ~npm run build -- -e demo~
  - Uploader l'appli
    - ~scp -r /path/to/source pi@192.168.0.101:/home/pi/html~
- MUTE : Identifier l'origine des digests différents pour un contenu identique
  - On rejouant les logs, labellés avec des digests différents, on obtient bien un seul et même digest
  - Le contenu des documents s'avère aussi identique
  - En listant les digests intermédiaires des logs, on ne retrouve pas les digests obtenus au cours de la session collaborative
  - En jouant un rapide scénario, j'ai pu obtenir des digests différents pour un même document
    - Utilisateur 1 insère "a\n" en 0
    - Utilisateur 2 insère "c" en 2
    - Une partition se produit
    - Utilisateur 1 insère "d" en 3
    - Utilisateur 2 insère "b" en 1
    - Résolution de la partition
  - Les arbres sont en fait équilibrés différement
  - Mais en rejouant le log (en rechargeant la page), cette fois-ci les digests convergent
    - Ce qui est troublant, pourquoi les digests ne continuent-ils pas de diverger ?
- MUTE : Génération du build correspondant à la nouvelle version de MUTE
  - Installation de https://github.com/google/zopfli
  - Modification de la valeur de DIR dans /scripts/brotli_build_compression.js/
  - Installation de https://github.com/google/brotli
*** Planned
** Semaine du <2017-09-11 Mon> au <2017-09-15 Fri>
*** Done
- MUTE : Identifier l'origine des digests différents pour un contenu identique
  - Je n'ai pas réussi à reproduire ce problème à l'aide du scénario précédent
  - Les digests obtenus sont égaux, même si les arbres sont équilibrés différemment
    - Ce nouveau résultat s'avère tout aussi troublant
  - Ajout d'un test /convergent-trees/
    - Permet de vérifier qu'on obtient un arbre balancé de la même manière en exécutant des opérations dans un ordre différent
    - Échoue à l'heure actuelle
    - Permet de vérifier si https://github.com/coast-team/mute-structs/issues/11 est résolue
  - Ajout d'un test /non-convergent-balanced-trees-different-digests/
    - Permet de vérifier qu'on obtient un digest différent pour des arbres équilibrés de manière différente
    - Échoue à l'heure actuelle
    - Permet de vérifier si https://github.com/coast-team/mute-structs/issues/12 est résolue
- MUTE : Séparer *StateVector* de *SyncService*
  - Déplacement de l'implémentation du vecteur d'état dans *StateVector*
  - Ajout de tests pour *StateVector*
    - Constructeur
    - set
  - Mise à jour de *SyncService* pour utiliser *StateVector*
*** Planned
**** DONE PLM : Implémenter un script monitorant les juges
- /WebPLM/ est utilisée de nouveau à TELECOM Nancy
- Cependant, nous avons toujours un problème avec les juges
  - Ils peuvent devenir des /zombies/ après avoir exécuté le code d'un étudiant
  - Dans cet état, ils se comportent d'une des manières suivantes
**** DONE MUTE : Séparer *StateVector* de *SyncService*
- À l'heure actuelle, l'implémentation du vecteur d'état est faite au sein du composant *SyncService*
- Afin de tester l'implémentation du vecteur d'état (son initialisation, sa mise à jour, le calcul des opérations manquantes), il est nécessaire de les séparer
- Implémenter *StateVector*
** Semaine du <2017-09-20 Wed> au <2017-09-22 Fri>
*** Done
- TEACHING : Notes sur le cours 2
  - Slide 7 : confusion possible avec la notation des fonctions
- MUTE : Utiliser *StateVector* dans *SyncMessageService*
  - Mise à jour des *Subject* concernés dans *SyncMessageService* et *SyncService*
  - Mise à jour des méthodes /generateQuerySyncMsg()/ et /handleQuerySyncMsg()/
- MUTE : Compléter les tests pour *StateVector*
  - Ajout des tests pour :
    - /isAlreadyDelivered()/
    - /isDeliverable()/
    - /computeMissingIntervals()/
- MongoDB : Pense-bête
  - Lister les databases: /db.adminCommand( { listDatabases: 1 } )/
  - Lister les collections: /show collections/
  - Lister les documents d'une collection: /db.<collection>.find({})/
  - Faire une requête dans une collection: /db.<collection>.find({ <fieldName>: "<value>"}/)
  - Faire une requête dans une collection en utilisant une regexp: /db.<collection>.find({ <fieldName>: /^regexp/ })/
- MUTE : Identifier l'origine des digests différents pour un contenu identique
  - J'ai rejoué le scénario de la démo pour essayer de produire ce bug
  - Je n'ai pas réussi à le reproduire à l'aide du scénario de test retranscrit dans /convergent-trees/
  - Ce scénario semble donc ne pas correspondre
  - En modifiant le scénario, j'ai finalement *réussi à reproduire le bug*
    - Le scénario reprend les différentes étapes du précédent : /création d'un groupe, collaboration, partition, travail séparé, reconnexion, resynchronisation/
    - La modification concerne la partie /reconnexion/
    - Au cours de la /reconnexion/, un pair émet de nouvelles opérations
  - Le document *testABCDEFGHIJKLMN* est une reproduction de ce nouveau scénario
    - Les opérations générées au cours de la phase de /reconnexion/ sont les opérations /29/ à /38/ de l'utilisateur /-600040289/
    - Elles correspondant au texte "Ayé la reconnexion va se produire..."
  - On constate un *problème lié au mécanisme de synchronisation*
    - Dès que la reconnexion est effectuée, les opérations nouvellement générées sont transmises aux autres pairs
    - Mais la synchronisation n'a pas encore eu lieu
    - Ces opérations ne sont donc pas encore délivrables, elles sont bufferisées
    - Elles sont ensuite délivrées (lors de la resynchronisation probablement)
    - Mais elles sont de nouveau bufferisées par la suite (toujours lors de la synchronisation ?)
  - On constate un *problème lié au modèle*
    - On observe un saut dans le déplacement du curseur lorsqu'on arrive à la position "Ayé la reconnex|ion va se produire..."
    - Le saut observé est différent selon l'utilisateur
    - Ceci indiquerait un problème aux niveaux des identifiants au sein de la structure de données
  - Après un rechargement de la page par le Chromebook, les digests convergent
  - Il est nécessaire de creuser plusieurs points :
  - Pourquoi certaines opérations sont bufferisées plusieurs fois ?
    - Probablement, au cours de la synchronisation, l'opération /28/ est délivrée
    - On délivre alors en cascade les opérations en attente
    - Puis on continue de traiter le message de synchronisation
    - Voir pourquoi ces opérations sont rebufferisées
  - Pourquoi ce saut de curseur ?
    - Pourquoi les modèles divergent ?
  - Pourquoi en rechargeant la page, les digests convergent ?
    - Comparer les logs du Chromebook avant/après reload pour voir s'ils diffèrent
    - Comparer les arbres du Chromebook avant/après reload pour voir s'ils diffèrent
- Meeting:
  - Spend most of the summer working on /mute-structs/
    - Start to refactor the identifiers
    - Since we rely heavily on it, had to refactor many parts of the library
    - Clean the code and add tests and documentation
    - Still not merge
  - Also work on /mute-core/
    - A intern told us it would be neat to have tests in /mute-core/
    - Not a bad idea
    - Specially helpful for the update to the next version of /mute-structs/
    - So setup continuous integration and add tests
  - Finally investigating a bug with the digests
    - Not the same digests for same content
    - Was able to reproduce the bug
    - Retrieve as much data as possible (logs, trees, console.log...)
*** Planned
**** DONE MUTE : Utiliser *StateVector* dans *SyncMessageService*
- Auparavant, on transmettait des *Map<number, number>* lors de /querySync/ et /replySync/
- Transmettre des *StateVector* permettrait de plus contraindre les données envoyées
**** DONE MUTE : Compléter les tests pour *StateVector*
- Ajouter des tests pour :
  - /isAlreadyDelivered()/
  - /isDeliverable()/
  - /computeMissingIntervals()/
** Semaine du <2017-09-25 Mon> au <2017-09-29 Fri>
*** Done
- TEACHING : Notes sur le cours 3
  - Slide 3
    - "Mauvais pratique" d'associer 1 évènement JS à un élément HTML
      - Pourquoi ?
      - Comment faire alors ?
  - Slide 9
    - Mettre un lien vers une doc pour la syntaxe des queries ?
  - Slide 20
    - "Il faut être sûr que le dom est entièrement chargé pour enregistrer un handler"
      - Pour être sûr que le noeud existe ?
  - Slide 24
    - Faute de frappe "onlick"
- MUTE : Identifier l'origine des digests différents pour un contenu identique
  - Pourquoi ce saut de curseur ?
    - On comparant les arbres récupérés, on s'aperçoit que les arbres divergent
      #+BEGIN_SRC
                                #
                        Id[2139784055,-600040289,1, 0 .. 0]
                                #
                Id[1534008470,931389836,0, 0 .. 0]
                                #
                        Id[-233297242,-600040289,3, 11 .. 32]
                                        #
                                Id[-233297242,-600040289,3, 0 .. 10]
                                        #
        Id[-1537848274,-600040289,0, 1 .. 5]
                                #
                        Id[-1537848274,-600040289,0,0,-460848852,931389836,1, 0 .. 0]
                                #
                Id[-1537848274,-600040289,0, 0 .. 0]
                        #
----------------------------------------------
                                #
                        Id[2139784055,-600040289,1, 0 .. 0]
                                #
                Id[1534008470,931389836,0, 0 .. 0]
                                #
                        Id[-233297242,-600040289,3, 0 .. 32]
                                #
        Id[-1537848274,-600040289,0, 1 .. 5]
                                #
                        Id[-1537848274,-600040289,0,0,-460848852,931389836,1, 0 .. 0]
                                #
                Id[-1537848274,-600040289,0, 0 .. 0]
                        #
      #+END_SRC
    - Le bloc /<-233297242,-600040289, 3, [0..32]>/ est coupé en deux parties chez le collaborateur qui a reçu ces contributions
    - Les contenus sont donc identiques, mais les structures différentes
    - Les logs ne présentent aucune raison justifiant ce résultat
      - Les opérations sont correctements ordonnées, ne sont pas dupliquées
    - L'ordre des opérations dans le log ne correspond donc pas à l'ordre des opérations réellement joué
      - Ceci explique pourquoi recharger la page permet de faire converger les logs
    - Écriture d'un test pour vérifier l'ordre dans lequel les opérations sont effectivement transmises au *DocService* par le *SyncService*
      - Test disponible ici: https://github.com/coast-team/mute-core/blob/69a7ac541289e99c7d489989a15365a79e3c1fb6/test/SyncService.test.ts#L38-L66
      - Il s'avère que les opérations bufferisées sont finalement délivrées dans l'ordre inverse
      - Modification de *SyncService* pour corriger ce bug
  - Pourquoi certaines opérations sont bufferisées plusieurs fois ?
    - Il s'avère qu'on bufferise une opération si /isDeliverable()/ renvoie /false/
    - Cependant une opération n'est pas /isDeliverable()/ si elle a déjà été délivrée
    - Modification de *SyncService* pour ne bufferiser uniquement les opérations n'étant ni délivrables ni délivrées
- MUTE : Améliorer la gestion de la position des curseurs
  - /mute-core/
    - Déclaration d'une interface *Position* représentant la position dans un bloc d'un curseur
    - Renommage de /idFromIndex()/ en /positionFromIndex()/
    - Déclaration du package /sync/ dans /sync.proto/
      - Nécessaire de déclarer un package pour pouvoir importer les messages définis
  - /mute/
    - Suppression et remplacement de *MuteCorePosition* par *Position*
    - Modification de *PositionMsg* pour utiliser *IdentifierMsg*
- MUTE : Étudier la fusion de /last/ avec /base/
  - /mute-structs/
    - Ajout du getter /base()/ dans *Identifier* et *IdentifierInterval* pour préserver /mapBaseToBlock/ dans *LogootSRopes*
    - Mise à jour du code de *LogootSRopes* pour correspondre à la nouvelle implémentation de la librairie
    - Export de *IdentifierTuple* pour rendre ce type accessible aux librairies/applications externes
    - Modification du build pour seulement exclure les fichiers de tests portant sur les /logs/ et les /trees/
    - Il est finalement temps de déployer cette nouvelle version
    - Il faut donc mettre à jour /mute-core/ et /mute/
  - /mute-core/
    - Mise à jour de /generateRichLogootSOps()/ de *Helpers*
    - Mise à jour de /positionFromIndex()/ de *DocService*
    - Mise à jour de /sync.proto/
  - /mute/
    - Mise à jour des fichiers générés à partir /cursor.proto/
      - Le message *PositionMsg* dépend de *sync.IdentifierMsg*
      - Les champs de *IdentifierMsg* ayant été modifiés, il est nécessaire de regénérer les fichiers /cursor_pb.js/ et /cursor_pb.d.ts/ pour qu'ils correspondent à la nouvelle implémentation
  - Réalisation de tests
    - Les utilisateurs collaborent au travers de différents scénarios
      - On cherche à vérifier que les copies convergent
	- Observation du digest
      - On cherche à vérifier que les curseurs distants fonctionnent correctement
      - Récupération de logs si convergence, les trees en plus si divergence
    - Scénario 1
      - Tout le monde connecté tout au long de la session de collaboration
      - Document https://www.coedit.re:8080/doc/hKib6coBIh
      - A convergé
      - Digest obtenu : /clark-total-disco/
    - Scénario 2
      - Des membres du groupe se déconnecte/reconnecte au cours de la session de collaboration
      - Document https://www.coedit.re:8080/doc/3ShRFwGD0f
      - A convergé
      - Digest obtenu : /regular-chant-samba/
    - Scénario 3
      - Collaboration perturbée par des partitions
      - Document https://www.coedit.re:8080/doc/HtCcFpETan
      - A convergé
      - Digest obtenu : /classic-penguin-magic/
    - Bug observé au niveau des curseurs lors du 1er test
      - Les curseurs des autres collaborateurs n'étaient pas affichés
      - Aucune erreur n'était répertoriée dans les logs
      - Pas réussi à le reproduire
- Présentation Quentin
  - Slide 3
    - Besoin des sous-points ?
    - Tu te contentes de les lire
  - Slide 4
    - Churn -> "Random connections/disconnections"
    - Pas plutôt "Frequent connections/disconnections" ?
  - Slide 5
    - Manque la légende non ?
  - Slide 7
    - Nécessaire ?
    - J'ai l'impression qu'elle casse le rythme
  - Slide 8
    - Le numéro de la slide déconne
    - "Newcomer" tient en un seul mot
  - Slide 14
    - Pas compris ce que tu voulais mettre en avant avec les données confidentielles
  - Slide 16
    - J'ai l'impression que le schéma porte à confusion
    - Entre données accessibles et données répliquées
  - Slide 18
    - Donne des noms aux pairs pour faciliter les explications
*** Planned
**** DONE MUTE : Identifier l'origine des digests différents pour un contenu identique
- On a obtenu plusieurs fois des digests différents alors que le contenu du document semblait identique
- Recharger la page permettait de regénérer le digest attendu
- Vérifier les logs
**** DONE MUTE : Améliorer la gestion de la position des curseurs
- Pour représenter la position des curseurs distants dans /mute/, nous utilisons la position du curseur de l'utilisateur dans un bloc
- Pour obtenir cette position au sein du bloc, /mute/ interagit avec /mute-core/
- Cependant, les structures de données utilisées ne sont pas explicitement liées
  - L'interface *MuteCorePosition*, déclarée dans /cursors.directive/, n'est pas explicitement sensée correspondre au type de retour de /idFromIndex()/ de *DocService*
  - Les messages definis dans /cursor.proto/ déclarent des structures de données équivalentes à celles définies dans /sync.proto/
- Ces structures de données étant modifiées dans la nouvelle version de /mute-structs/, les lier permettrait d'effectuer la mise à jour plus aisément
**** DONE MUTE : Étudier la fusion de /last/ avec /base/
- Actuellement, dans un *Identifier*, nous conservons séparé la /base/ de /last/
- Cependant, dès que nous avons besoin de comparer des identifiants, nous concaténons /last/ à /base/
- Voir si fusionner ces deux paramètres pose un problème particulier
** Semaine du <2017-10-02 Mon> au <2017-10-06 Fri>
*** Done
- TEACHING : Notes sur le cours 4
  - Pas de mentions du système de module intégré en ES6 ?
- TEACHING : Notes sur le cours 5
  - Slide 5
    - Faire un lien entre les fonctions utilitaires de jQuery et /filter()/ et /map()/ ?
  - Slide 20
    - Retirer la zone de titre ?
- TEACHING : Notes sur le TD 1
  - Exercice 3.2
    - Leur faire utiliser /reduce()/ plutôt ?
  - Exercice 4.7
    - Pourquoi /dateNaiss.getMonth() === month-1/ dans la correction ?
- ECSCW 2017 : Payer la facture
  - Appeler le +44 114 225 5668 pour régler la facture
  - Fait
  - Pas reçu de confirmation de paiement de leur part
  - Mais le virement a du moins bien été effectué
  - Reste à apporter la preuve de paiement à Sylvie
- <<perspective-cap-theorem>> DISTRIBUTED SYSTEMS : Lire *Perspectives on the CAP Theorem*
  - La consistence est une propriété classique de /safety/
  - La disponibilité est une propriété classique de /liveness/
  - À partir du moment où les communications entre pairs sont asynchrones, il n'est pas possible de garantir la /safety/ et la /liveness/ du système
  - Exemple du registre répliqué
    - Un ensemble de serveurs /{p1, p2, ..., pn}/ permettent de manipuler un registre répliqué (lire sa valeur / modifier sa valeur)
    - Si une partition se produit, on est obligé de sacrifié la /safety/ ou la /liveness/ du système
    - Pour garantir la /liveness/ lors d'une requête de lecture, un serveur doit fournir une réponse
      - Mais comme le serveur /pi/ traitant la requête de lecture peut être coupé des autres, il ne peut être sûr de la consistence de la valeur du registre
      - Il doit alors retourner une valeur qui peut ne pas être consistente
    - Pour garantir la /safety/ lors d'une requête de lecture, un serveur doit attendre que la partition soit résolue
      - Mais la partition peut ne jamais se résoudre
      - Il ne répondrait donc jamais à la requête
  - Si les communications sont asynchrones, un pair ne peut pas faire la distinction entre le retard d'un message ou l'apparition d'une partition
  - Passe au résultat du /FLP theorem/ : le consensus est impossible dans un système pouvant avoir des pannes avec des communications asynchrones
  - Exemple du consensus
    - Le consensus et la validité sont des propriétés de /safety/
    - La terminaison est une propriété de /liveness/
    - Plus complexe que le problème du registre répliqué
    - Donc pas possible d'assurer la /safety/ et la /liveness/ en cas de partition
    - Le /FLP theorem/ montre aussi que les algorithmes de consensus garantissant la /safety/ boucle indéfiniement dans certains scénarios ou si un pair ne répond plus : perte de la /liveness/
  - 2 pistes de réflexions
    - À quel point le réseau doit-il être fiable pour pouvoir assurer la /safety/ et la /liveness/ ?
    - Quel niveau de consistence peut-on assurer si le réseau n'est pas fiable ?
  - Fiabilité du réseau
    - Utilisation d'un réseau synchrone
      - À creuser, je ne comprends pas exactement ce qu'est un réseau synchrone et comment cela fonctionne
      - Cependant, un système de détection d'erreurs offrirait les mêmes garanties qu'un réseau synchrone
      - Un système d'élection de leader équivaudrait à un système de détection d'erreurs
    - /eventual synchrony/ : un réseau peut alterner entre des phases asynchrones et des phases synchrones
    - /f + 2/ rounds (où /f/ est le nombre de noeud pouvant crasher) sont nécessaires avec un réseau synchrone pour atteindre un consensus
    - Dans un système où le réseau est /eventual synchronous/, on peut résoudre le consensus si moins de /n ÷ 2/ noeuds crashent (où /n/ est le nombre de noeuds)
  - Niveau de consistence
    - Introduction du problème du /set agreement/
      - Plutôt que d'arriver à un consensus, on renvoie ici un /k-set/ de valeurs, /k/ étant un nombre compris entre 1 et /n/ (où /n/ est le nombre de noeuds)
    - Il a été démontré que, pour résoudre le problème du /k-set/, seulement /k - 1/ erreurs ne peuvent se produire
    - Il a aussi été démontré que pour résoudre le problème du /k-set/, /t ÷ k + 1/ rounds sont nécessaires (où /t/ est le nombre d'erreurs)
  - Insiste sur la possibilité d'effectuer le compromis entre consistence et disponibilité au cas par cas dans une application selon plusieurs dimensions
    - Type de données
      - Certaines données peuvent être critiques tandis que d'autres négligeables
      - On peut donc nécessiter une forte consistence pour les données critiques et préférer une grande disponibilités pour les autres données
    - Type d'opérations
      - Même raisonnement que pour les données
      - Les opérations de lectures peuvent nécessiter une forte disponibilité tandis que les opérations de modifications requièrent une forte consistence
    - Décomposition fonctionnelle
      - On peut décomposer l'applications en sous-services, chacun effectuant son propre compromis entre consistence et disponiblité
    - Décomposition géographique / par utilisateurs
      - Idée de faire des data-centers proches des utilisateurs
      - Les noeuds d'un data-center étant proches, on peut faire des suppositions sur la fiabilité du réseau et mettre l'accent sur la consistence offerte
- <<dottedb>> DISTRIBUTED SYSTEMS : Lire *DottedDB: Anti-Entropy without Merkle Trees, Deletes without Tombstones*
  - *DottedDB* est un  /Dynamo-like key-value store/
  - Les données sont répliquées parmi un certain nombre de noeud en fonction du facteur de réplication
  - Les clients ne possèdent pas une réplique des données, ils y accèdent et les modifient par le biais des noeuds
  - Répond à 3 problèmes des bases de données répliquées
    - La croissance du /version vector/ liée au /node churn/
      - À chaque fois qu'un noeud rencontre une erreur, il est remplacé
      - Mais on conserve dans le /version vector/ l'entrée du noeud défaillant et on ajoute une nouvelle entrée pour le noeud
    - La nécessité de conserver des /tombstones/
      - Pour empêcher un objet de "ressurgir" après sa suppression, à cause de la relivraison d'une opération précédente, les systèmes actuels conservent une /tombstone/
      - Ainsi, l'espace de stockage utilisé ne peut que croître puisque supprimer un objet laisse quand même des méta-données
    - L'utilisation "lourde" de Merkle Trees pour le système d'anti-entropie
      - /Bloom Filter/ et /Merkle Trees/ sont présentées comme étant les structures de données les plus communément utilisées dans un mécanisme d'anti-entropie
      - /Bloom Filter/ est dit comme ne scalant pas en fonction du nombre de données
      - /Merkle Trees/ permet d'effectuer un compromis entre la précision du mécanisme d'anti-entropie et la quantité de méta-données échangées
	- Le mécanisme d'anti-entropie va comparer les racines des arbres des différents noeuds pour détecter une différence
	- Si une différence est détectée, on va comparer en profondeur les /Merkle Trees/ pour en trouver l'origine
	- Mais la hauteur de l'arbre dépend du nombre d'objets stockés par feuille
	- Si on stocke un objet par feuille, on aura une précision de 100% mais il faudra beaucoup de rounds pour déterminer l'origine de la différence
	- Si on stocke plusieurs objets par feuille, le nombre de rounds nécessaires est réduit mais on va potentiellement renvoyer plusieurs objets inutilement
  - Propose le framework /Node-wide Dot-based Clocks (NDC)/
    - L'idée principale est de factoriser/compacter les méta-données normalement ajoutées à chaque objet stocké en les déplaçant sur le noeud
    - Ajoute à chaque version d'un objet
      - Un /dot/ : un couple /(nodeId, nodeClock)/ identifiant la version de façon unique
      - Un /causal context/ : les dots des versions précédentes de l'objet
	- Le /causal context/ d'un objet peut être supprimé si le noeud possède toutes les versions précédentes de l'objet
    - Ajoute au niveau du noeud 4 structures de données
      - /Node Clock (NC)/
	- Liste des /dots/ (et donc versions des objets) connus par le noeud
      - /Dot-Key Map (DKM)/
	- Map associant un /dot/ à la clé d'un objet
	- Permet de retrouver l'objet divergent à partir d'un de ses /dots/ au cours du mécanisme d'anti-entropie
	- Une fois le /dot/ connu par l'ensemble des pairs, il peut être retiré de cette map
      - /Watermark (WM)/
	- Matrice des /NC/ de chaque noeud
	- Au cours du déroulément du mécanisme d'anti-entropie, un noeud envoie son /NC/ à un autre noeud
	- Celui-ci le stocke alors son /NC/ avant de pouvoir déterminer quand retirer un /dot/ de /DKM/
      - /Non-Stripped Keys (NSK)/
	- Liste des objets dont le /causal context/ n'est pas vide
	  - Possible si on a loupé des versions de l'objet
      - /Storage (ST)/
	- Map associant une clé à un objet
  - Évaluation
    - L'évaluation a été réalisé en comparant *DottedDB* à *MerkleDB*
      - *MerkleDB* un fork de *DottedDB* utilisant un /Merkle Tree/ pour le mécanisme d'anti-entropie et /Dotted Version Vectors/ comme mécanisme de traçage de la causalité
    - Taille des méta-données
      - Mécanisme de causalité
	- La taille de /NC/ de *DottedDB* augmente principalement lors de la perte de messages
	  - On peut compacter les /dots/ contigus d'un noeud en ne stockant que la /nodeClock/ la plus élevée
	  - Lorsqu'un message non-contigu est reçu, on doit alors stocker sa /nodeClock/ séparément
	  - Ces méta-données supplémentaires sont supprimées dès que les noeuds convergent
	- Tandis que la taille du /Dotted Version Vector/ dans le cas de *MerkleDB* croît avec le nombre noeuds vus, et donc avec le /node churn/
      - Anti-entropie
	- Avec *MerkleDB*, la taille des méta-données liées au système d'anti-entropie croît avec le nombre de clés (les feuilles de l'arbre)
	- Avec *DottedDB*, la taille des méta-données augmente dans le cas de divergences entre noeuds mais diminue à chaque synchronisation
	  - Une fois le système stable, seuls /NC/ (un version vector) et /WM/ (une matrice de version vectors) ne sont pas vides
	- L'empreinte réseau semble plus faible avec *DottedDB* (de 10 à 100 KB/s) comparé avec *MerkleDB* (de 100 à 700 KB/s)
  - Remarques
    - Dans ce contexte d'utilisation, il n'y a pas besoin d'obtenir toutes les versions d'un objet
    - Seule la dernière version nous intéresse
    - Comparer leur mécanisme à /causal barrier/ n'est donc pas adapté
      - On ne veut pas imposer une livraison causale des opérations
- Meeting
  - Last week
    - Solved issue in /mute-core/ which resulted in the different digests for same content
    - Caused by the delivery mechanism
    - Was delivering messages in reverse order in some particular cases
    - Deployed the version to test with 6-7 users
  - This week
    - Start the PhD !
    - Had a reunion this morning with Olivier & Gérald to discuss where to start, how to start
    - Start reading *Perspectives on the CAP Theorem*
      - Some interesting results about consensus in eventually synchrone network
	- How many failures it can support
	- How many rounds does it need
      - Some trouble to understand /synchrone network/
	- Which properties does it need to respect ?
	- How does it behave ?
	- How does it recover ?
	- Need to dig deeper on it
    - Also start reading *DottedDB: Anti-Entropy without Merkle Trees, Deletes without Tombstones*
      - Dynamo-like key-value store
      - Instead of adding all the metadata to each object, they put it in the node to be able to factorize/compact it
	- Only add a dot (a couple made of the /nodeId/ and the /nodeClock/) per version of the object
      - Compare to version vector instead of causal barrier, which is weird
	- It blames version vector for its ever-growing size
	- Which is a flaw of version vector addressed by causal barrier
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Lire *DottedDB: Anti-Entropy without Merkle Trees, Deletes without Tombstones*
- Disponible ici : http://haslab.uminho.pt/tome/files/dotteddb_srds.pdf
**** DONE DISTRIBUTED SYSTEMS : Lire *Perspectives on the CAP Theorem*
- Disponible ici : https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf
** Semaine du <2017-10-09 Mon> au <2017-10-13 Fri>
*** Done
- TEACHING : Notes sur le TD1
  - Je viens de remarquer que les exercices 3 et 4 du TD1 portent sur les notions abordées dans le cours 2
  - Les retirer du TD1 ?
  - Oui, il s'agit d'un oubli
  - [X] Déplacer les exercices 3 et 4 du TD1 au TD2
  - [X] Mettre à jour le sujet du TD1 sur Arche
  - [X] Mettre un placeholder pour /min()/ et /max()/ dans e1.js
  - [X] Mettre à jour le squelette HTML sur Arche
  - [X] Déposer le sujet du TD2 sur Arche
- <<dynamo>> DISTRIBUTED SYSTEMS: Lire *Dynamo: amazon’s highly available key-value store*
  - *Dynamo* est un key-value store décentralisé fortement disponible
    - Sacrifie la consistence pour assurer cette disponibilité
  - Laisse à l'application la possibilité de tweaker le compromis entre consistence et disponibilité
  - Les données sont partitionnées et répliquées en utilisant /consistent hashing/
  - Un /version vector/ est attaché à chaque version d'un objet
  - *Dynamo* permet de répondre aux besoins suivants
    - Système simple de requêtes
      - Généralement besoin de récupérer ou de mettre à jour une valeur associée à une clé
      - Pas besoin d'un système relationnel ou de pouvoir effectuer une requête sur un ensemble d'objets
      - Pas besoin non plus d'un mécanisme de transactions
	- Ceci nuirait à la disponibilité du système, mais offrirait une plus grande consistence
    - Hautes performances
      - Les services reposant sur *Dynamo* devant fournir une réponse rapide aux clients, *Dynamo* se doit d'être efficace pour ne pas nuire aux performances des applications
      - C'est d'autant plus vrai que la réponse à la requête d'un client peut être composée des résultats de plus de 150 services
    - Flexible
      - Ajouter/retirer un noeud doit pouvoir être fait facilement et ne pas impacter les performances du système
    - Symétrie
      - Chaque noeud doit avoir le même rôle, afin de ne pas complexifier le système
    - Compatible avec l'hétérogénité des noeuds
      - Les noeuds peuvent disposer d'une puissance de calcul différente
      - Il faut donc que *Dynamo* permette de répartir la charge efficacement sur les différents noeuds en fonction de leurs ressources
    - Hypothèses
      - /Amazon/ déployant et manageant ses instances de *Dynamo*, on considère que l'environnement est non-hostile
  - Pour augmenter la disponibilité du système, repose sur des mécanismes de réplication optimiste
  - Mais nécessité de résoudre les conflits à un moment donné, lors du /read/ ou du /write/
  - Ils expliquent que le mécanisme de résolution de conflits peut faire échouer l'opération si il est executé lors du /write/, notamment dans le cas d'une partition réseau
    - De quel mécanisme de résolution de conflits parlent-ils ?
  - Pour ne pas perdre d'opérations d'utilisateurs, ils déplacent donc la résolution de conflits sur l'opération de /read/
  - Nécessité aussi de déterminer qui se charge de résoudre les conflits : l'application ou le système
    - *Dynamo* propose des mécanismes de résolution de conflits par défaut (LWW par exemple)
    - Mais permet à l'application d'utiliser son propre mécanisme à la place
  - API du système
    - /get(key)/
      - Détermine les noeuds possédant une réplique de l'objet
      - Récupère chaque version de l'objet ainsi que leur contexte respectif
      - Renvoie la ou les versions de l'objet ainsi que les contextes
    - /put(key, context, object)/
      - Détermine les noeuds auxquels ils faut fournir une réplique de l'objet
      - Stocke la nouvelle version de l'objet ainsi que le contexte attaché dans chacun de ces noeuds
  - Partitionnement des données
    - Repose sur /consistent hashing/
    - Les noeuds sont placés sur un anneau
    - Les clés, grâce à une fonction de hash, sont mappés à une position /p/ sur cette anneau
    - Les /N/ premiers noeuds se trouvant après la position de la clé vont se voir assigner l'objet à répliquer, /N/ étant le /replication factor/
    - Lorsqu'un nouveau noeud est démarré
      - Son arrivée décale le rang/classement des autres noeuds par rapport à une position donnée et donc par rapport à une clé
      - Les noeuds qui "perdent" la responsabilité de clés et d'objets suite à ce déclassement transmettent ces données au nouveau noeud
    - Possibilité d'utiliser des noeuds virtuels
      - Possibilité de générer un ou plusieurs noeuds virtuels par noeud physique
      - Permet ainsi de répartir la charge plus équitablement entre des noeuds ayant des ressources différentes
    - Notion de /coordinator/
      - Le noeud le plus proche de la position d'une clé est le /coordinateur/
      - C'est lui qui se charge de répliquer l'objet sur les /N-1/ noeuds suivants
    - Notion de /preference list/
      - La /preference list/ répertorie les noeuds en charge de stocker un objet
      - Chaque noeud du système peut générer/obtenir cette liste pour une clé donnée
      - La /preference list/ contient en fait plus que /N/ noeuds, afin de résister aux pannes
  - Versionnement
    - Pour un même objet, plusieurs versions peuvent être stockés
      - Exemple du panier d'un client (/v0/)
      - Si un client met à jour son panier (/v1/) mais qu'une partition réseau empêche les noeuds de se synchroniser
      - Il est possible que la prochaine lecture renvoie une version dépassée (/v0/)
      - Puisqu'on souhaite ne perdre aucune modification effectuée par le client, le client doit pouvoir continuer de mettre à jour le panier (/v1'/)
      - L'ensemble de ces versions pourront être récupérées lors d'une prochaine opération /get()/, si la partition réseau est resolue
    - Un /vector clock/ est attaché à chaque version de l'objet (dans son contexte)
    - En comparant les /vector clocks/, le système est capable de déterminer si des versions sont causalement dépendantes ou concurrentes
    - Le système peut ainsi rejeter une version dépassée si elle est tout de même retournée par /get()/
    - Pour éviter que le /vector clock/ ne croisse indéfiniement, sa taille est limitée à 10 entrées
      - À chaque pair /node, clock/, un timestamp est associé
      - Lorsque le /vector clock/ atteint sa taille limite, on retire l'entrée correspondante au timestamp le plus ancien
      - Peut perdre des informations de causalité à cause de ce troncage
      - Mais ce risque est mitigé, puisque ce sont les /coordinators/ qui sont chargés normalement de faire les mises à jour de l'objet
      - Les entrées les plus anciennes ont donc de grandes chances de correspondre à des noeuds de back-up
  - Mécanisme de cohérence
    - *Dynamo* utilise un protocole similaire à ceux utilisé pour les quorums, d'après le papier
    - Ce protocole possède deux variables
      - /R/: Le nombre de noeuds minimum nécessaire pour effectuer une opération de lecture
      - /W/: Le nombre de noeuds minimum nécessaire pour effectuer une opération d'écriture
    - Si on a /R + W > N/ (où /N/ est le nombre de noeuds), on obtient un système /quorum-like/
    - Pour des raisons de performances, généralement /R/ et /W/ sont inférieurs à /N/
  - /Hinted Handoff/
    - Afin de tolérer les pannes, une écriture ne se fait pas sur les /N/ premiers noeuds de la /preference list/ mais les /N/ premiers noeuds en vie (où /N/ est le /replica factor/)
    - Si un noeud de back-up reçoit une écriture (à cause d'une panne par exemple), une méta-donnée indiquant le destinataire initial y est ajoutée
    - Périodiquement, ce noeud va vérifier s'il peut transmettre au destinataire initial l'écriture
  - Anti-entropie
    - Repose sur un /Merkle Tree/
    - Chaque noeud possède un /Merkle Tree/ contenant les objets appartenant à un /key range/, pour chaque /key range/ auquel il appartient
    - Puisqu'on repose sur /consistent hashing/, ce nombre correspond au /replication factor/
  - Membership
    - Démarrer/Arrêter un noeud sont des opérations manuelles
    - Chaque noeud a la liste des noeuds du système
    - Lorsqu'un nouveau noeud est démarré, un noeud est chargé de mettre à jour la liste des noeuds du système
    - Via un algorithme de /gossiping/, cette modification est transmises aux autres noeuds
    - Les données indiquant à quelle partition de l'anneau un noeud appartient sont aussi transmises en parallèle via le même mécanisme
  - Seed
    - Pour éviter que des partitions logiques ne soient créées (démarrage de 2 noeuds en concurrence), des noeuds font jouer le rôle de seed
    - Les seeds sont découverts par un moyen alternatif
  - Détection de panne
    - Dès qu'un noeud B ne répond plus aux messages du noeud A, A en déduit que B rencontre une panne
    - A va ensuite recontacter B périodiquement pour vérifier s'il la panne est résolue
  - Stratégies de partitionnement avec /consistent hashing/
    - Pour assurer une répartition uniforme de la charge sur les différents noeuds, /Amazon/ a étudié plusieurs méthodes de partitionnement de l'anneau
    - Stratégie 1 : /T random tokens per node and partition by token value/
      - Stratégie naïve décrite précédemment
      - Problème du bootstrapping
	- Au démarrage, un noeud doit "voler" des clés aux autres noeuds
	- Mais pendant ce temps, ces noeuds doivent toujours assurer leurs tâches
	- Cet échange de clé doit donc se faire en tâche de fond et avec une faible priorité
	- Ceci a conduit à des scénarios où un noeud mettait 1 journée pour démarrer
      - Problème de la regénération du /Merkle Tree/
	- Lors de l'arrivée ou du départ d'un noeud, les groupes de clés gérés par l'ensemble des noeuds changent
	- Il faut donc regénérer les /Merkle Trees/ en conséquence
	- Cette tâche peut s'avérer gourmande en ressource
      - Problème de l'archivage
	- Les clés étant éparpillés sur les différents noeuds de façon aléatoire, il n'y avait pas de moyen simple de faire un snapshot
	- Le seul moyen consistait à contacter chacun des noeuds pour récupérer les clés
      - Problème du couplage entre /data partitioning/ et /partition placement/
	- Insérer des noeuds modifient le partitionnement de l'anneau et provoquent des transferts de données, ce qui génère de la charge supplémentaire
    - Stratégie 2 : /T random tokens per node and equal sized partitions/
      - Divise l'anneau en /Q/ partitions de même taille
	- Avec /Q >> N/ où /N/ est le /replication factor/
	- Avec /Q >> S*T/ où /S/ est le nombre de noeuds et /T/ le nombre de tokens par noeud
      - Permet de découpler le nombre de partitions de leur placement
      - Des partitions peuvent ne pas avoir de noeud d'assigné
    - Stratégie 3 : /Q/S tokens per node, equal-sized partitions/
      - Divise l'anneau en /Q/ partitions de même taille
      - Chaque noeud se voit attribuer /Q / S/ tokens au démarrage
	- Il peut les voler aux autres noeuds
      - Ces tokens sont redistribués aux autres noeuds à son départ
      - Offre les meilleures performances de /load balancing/
      - Puisque les partitions sont connues, peut facilement conserver la liste des clés pour chacune d'entre elles
      - Permet un bootstrapping plus performant (pas besoin de récupérer la liste des clés en contactant chaque noeud)
      - Permet du coup de faire des snapshots du système pour archiver l'ensemble des données
      - Par contre, plus difficile à gérer pour maintenir les bonnes propriétés
  - Remarques / Questions
    - Méta-données
      - /vector clock/
	- *Dynamo* ajoute un /vector clock/ est attaché à chaque version d'un objet
	- À chaque entrée du /vector clock/, un timestamp est aussi associé
	- Sa taille est limitée à 10
	  - Dès qu'elle dépasse 10, l'entrée la plus ancienne est retirée
      - /Merkle Trees/
	- *Dynamo* doit conserver un /Merkle Tree/ pour chaque /key range/ auquel il appartient
	- Ce nombre correspond au /replication factor/
	- La taille de chaque /Merkle Tree/ dépend directement du nombre d'objets stockés dans le système
      - Membership
	- Chaque noeud doit conserver la liste des noeuds du système
	- Il doit aussi pouvoir récupérer la /preference list/ pour chaque clé
    - /Garbage collection/
      - Ne mentionne pas de mécanisme de /garbage collection/
      - Mais qu'y aurait-il à supprimer ?
    - Mécanisme de résolution de conflits
      - Ils expliquent que le mécanisme de résolution de conflits peut faire échouer l'opération si il est executé lors du /write/, notamment dans le cas d'une partition réseau
      - De quel mécanisme de résolution de conflits parlent-ils ?
    - Ajout/Suppression de noeud
      - Le papier explique qu'il faut ajouter un round de confirmation à la suite du transfert de clés lors de l'ajout d'un noeud pour éviter que des objets ne soient transférés en double
      - Comment ce scénario est-il possible ?
      - Voir si /consistent hashing/ aborde ce problème
    - Tombstones
      - Il n'y a aucune mention de tombstones dans le papier
      - D'un autre côté, il n'y a pas d'opération /delete()/
      - Pour supprimer un objet, est-ce qu'ils ne génèrent pas une nouvelle version vide de l'objet avec /put()/ ?
*** Planned
**** DONE DISTRIBUTED SYSTEMS: Lire *Dynamo: amazon’s highly available key-value store*
- *DottedDB* ([[dotteddb]]) est une base de données clé-valeur /Dynamo-like/
- Lire ce papier me permettrait p-e de mieux comprendre *DottedDB*
- Disponible ici : https://dl.acm.org/citation.cfm?id=1294281
** Semaine du <2017-10-16 Mon> au <2017-10-20 Fri>
*** Done
- <<dotted-version-vector>> DISTRIBUTED SYSTEMS : Lire *Scalable and Accurate Causality Tracking for Eventually Consistent Stores*
  - Propose une nouvelle /logical clock/ : /Dotted Version Vectors/
  - Modèle du système
    - /dynamo-like key-value store/
    - API du système
      - /get(key)/
	- Renvoie un tuple /(version(s), context)/
      - /put(key, value, context)/
	- Le /context/ doit contenir suffisamment d'informations pour pouvoir établir la relation /happen-before/ entre 2 opérations
    - Système distribué où les noeuds communiquent par le biais de messages asynchrones
    - Nb clients >> Nb noeuds
    - Nb noeuds >> /replication factor/
    - Pas d'affinités entre clients et noeuds
      - Les requêtes successives d'un client peuvent s'exécuter sur des noeuds différents
    - Pas de /byzantine failures/
      - Les noeuds peuvent rencontrer une panne mais arriveront par la suite à se rétablir dans un état consistent
  - *Les structures de données présentées correspondent qu'à une seule clé du /key-value store/*
    - Il faut en construire et maintenir une *pour chaque objet* du système
  - Problèmes adressés
    - Besoin d'attacher une horloge logique aux écritures pour détecter la causalité et la concurrence des différentes opérations
    - Les mécanismes d'horloges logiques existants ne scalent pas
      - Par exemple, dans /version vectors/, on stocke une entrée par replicas
      - La taille du vecteur croît donc de façon linéaire avec le nombre de replicas
    - Pour pallier à ce problème, certains systèmes choisissent de retirer au cours du temps des informations de l'horloge afin de conserver une taille constante
      - Voir [[dynamo]]
  - /Dotted Version Vectors/
    - Notée /dvv/
    - /dvv = ((i, n), v)/ où
      - /i/ représente l'identifiant du replica
      - /n/ représente la valeur du compteur du replica /i/
      - /v/ représente le /version vector/ correspondant au contexte causal de l'opération
	- Pour /dvv = ((i, n), v)/, /v[i] = n - 1/
    - Indique ainsi l'entrée du /version vector/ qui a été mise à jour
    - Permet de conserver les informations de causalité nécessaires pour comparer 2 versions et déterminer si elles sont concurrentes ou si une l'emporte sur l'autre
    - Permet de comparer deux horloges en temps constant (au lieu d'en temps linéaire pour les /version vectors/)
      - /((i, n), u) < ((j, m), v)/ si /n <= v[i]/)
      - si /!(dvv < dvv') && !(dvv' < dvv)/, les deux évènements sont concurrents
  - Cependant, dans les applications, on va se retrouver à conserver et à traiter plusieurs versions concurrentes d'un même objet
  - Chacune de ces versions concurrentes va conserver un /dvv/
    - /{ (dvv1, vers1), (dvv2, vers2), ... }/
  - Pour éviter de dupliquer inutilement des informations, ils proposent une structure adaptée pour gérer ce cas de figure : /Dotted Version Vectors Set/
    - Notée /dvvs/
    - /dvvs = { (i, n, [vers1, vers2]), (j, m, [vers3]), ... }/
    - Permet de factoriser les dots des opérations concurrentes effectuées sur le même noeud
    - La position de la /vers/ dans la liste permet de déterminer son /dot/
      - Dans l'exemple précédent, on a :
	- le /dot/ de /vers1/ est /(i, n)/
	- le /dot/ de /vers2/ est /(i, n-1)/
	- le /dot/ de /vers3/ est /(j, m)/
  - Propose un framework pour traiter les opérations /get()/ et /put()/ en se basant sur les horloges logiques
    - Ce framework propose 4 opérations
      - /sync()/ :
	- Prend en paramètres deux sets de clocks
	- Renvoie un nouveau set composé uniquement des clocks n'étant pas obsolètes
      - /join()/ :
	- Prend en paramètre un set de clocks
	- Renvoie une nouvelle clock correspondant au contexte commun de toutes les clocks du set initial
      - /discard()/ :
	- Prend en paramètre un set de clocks /S/ et une clock /C/
	- Supprime de /S/ toutes les clocks qui sont couvertes par /C/
      - /event()/ :
	- Prend en paramètre un set de clocks /S/, une clock /C/ et un identifiant de noeud /r/
	- Renvoie une nouvelle clock (ne rentrant pas en conflit avec S), l'emportant sur C, représentant la nouvelle version
    - Traitement de /get(key)/
      - Récupère les différentes versions de l'objet auprès des replicas
      - Les fusionne au besoin en appliquant /sync()/ 2 à 2
      - Ne conserve que leur contexte causal commun avec /join()/
      - Renvoie un tuple /(version(s), context)/
    - Traitement de /put(key, value, context)/
      - Le noeud traitant la requête /put()/ effectue les opérations suivantes
	- Il supprime ses versions de l'objet rendues obsolètes par le nouveau contexte
	- Génère une nouvelle clock /c/ grâce à /event()/
	- Ajout de l'entrée /(c, value)/ dans le set des versions courantes de l'objet
      - Il obtient en résultat de ce traitement un nouveau set /S/ des versions courantes de l'objet
      - Il transfère alors /S/ aux autres réplicas
      - Ces derniers utilisent alors /sync()/ pour mettre à jour leur(s) version(s) de l'objet
  - Évaluation
    - Ils ont comparé /Dotted Version Vectors/ et /Dotted Version Vectors Set/ avec d'autres mécanismes (/LWW/, /VV/)
      #+CAPTION: Évaluation de Dotted Version Vectors
      #+NAME:   fig:dotted-vv-evaluation
      [[file:img/dotted-vv-evaluation-org.png]]
    - Intéressant de noter que la complexité de /dvv/ et de /dvvs/ croît linéairement avec le nombre de réplicas /R/ et le nombre de versions concurrentes /V/
    - Alors que /vv/ croît de façon linéaire avec le nombre de clients /C/ et de façon quadratique avec le nombre de versions concurrentes /V/
- Réunion mécanismes de gestion de clés de groupe
  - n-party DH
    - Dispose en anneau les noeuds
    - Les noeuds conviennent d'un générateur /g/
    - Les noeuds disposent d'un secret
    - 1er round
      - Les noeuds mettent successivement à l'exponentielle le générateur
	- Le noeud A va calculer /g^a/ puis va transmettre la valeur au noeud B qui va calculer /g^ab/
      - Lorsque le dernier noeud E reçoit la valeur intermédiaire, on passe au round 2
    - 2nd round
      - Le noeud E broadcast la valeur /g^abcd/
      - Chaque noeud va retirer son exponentielle de la valeur et la retourner à E
	- Le noeud A va renvoyer /g^bcd/ par exemple
    - 3ème round
      - Le noeud E va mettre à l'exponentielle toutes les valeurs reçues dans le round précédent et renvoyer la valeur au noeud correspondant
	- Il va calculer /g^bcde/ et envoyer cette valeur au noeud A
      - Le noeud A peut alors remettre à l'exponentielle cette valeur avec son secret pour obtenir la clé de groupe
  - Version centralisée
    - Génère une clé de groupe via une fonction /f(K1, K2)/
    - /K1/ n'est connue que des membres du groupe
    - Ajoute un serveur tiers qui a pour responsabilité de générer /K2/
    - Les membres de groupe ont la possibilité de récupérer /K2/ en effectuant une requête à ce serveur
    - Le serveur peut récupérer les clés
    - Lors d'un changement du groupe, /K2/ doit être modifiée pour assurer la confidentialité des prochains messages
    - Le serveur ne doit jamais avoir accès à /K1/ pour pas qu'il ne puisse obtenir la clé de groupe
    - Nécessaire de mettre en place un mécanisme de /membership/
      - Pour que le serveur puisse s'assurer qu'un utilisateur ait le droit de récupérer /K2/
      - Par exemple, un utilisateur peut obtenir /K1/ en étant invité à une collaboration sur un document
      - Si il est exclus par la suite, il ne doit plus pouvoir générer la clé de groupe
      - Il doit donc ne plus avoir accès aux valeurs de /K2/ pour ce document
- Meeting
  - Read *Scalable and Accurate Causality Tracking for Eventually Consistent Stores* (DAIS, 2014)
    - Introduce /Dotted Version Vector (dvv)/
      - Logical clock to track causality of the versions of an object
      - Achieve better scaling than /Version Vectors/ without sacrifying causality
    - Introduce /Dotted Version Vector Set (dvvs)/, an optimized data structure to manage several concurrent versions of an object
      - Since applications usually have to keep several concurrent versions
    - Propose a framework to serve /get/ and /put/ requests in /Dynamo-like key-value stores/
      - Composed of several kernel operations relying on a logical clock
      - Implement these operations for /dvv/ and /dvvs/
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Lire *Scalable and Accurate Causality Tracking for Eventually Consistent Stores*
- Ce papier propose une nouvelle /logical clock/, /Dotted Version Vectors/, permettant de capturer la causalité des opérations dans un système distribué
- Cette clock serait plus scalable que les solutions existantes telles que les /version vectors/ sans sacrifier les informations de causalité
** Semaine du <2017-10-23 Mon> au <2017-10-27 Fri>
*** Done
- TEACHING : TD3
  - [X] Préparer le TD3
    - [X] Mettre à jour la correction pour ES6
    - [X] Utiliser des noms plus explicites pour les classes
    - [X] Faire un squelette pour les étudiants
    - [X] Voir si certaines consignes d'exercices doivent être retravaillées
      - Ajout d'un exercice pour commencer à ajouter des handlers avant de manipuler les classes
      - Ajout de /rappels/ pour leur donner des indices
  - [X] Voir si certaines slides ont besoin d'être retravaillées
    - Modification de l'ordre des slides sur la sélection d'élements dans le *DOM*
      - Déjà avec /getElementById(), .../, ensuite avec /querySelector()/
    - Ajout d'une slide dédiée aux /CSS Selectors/
    - Découpage de la slide sur le parcours du *DOM* en plusieurs
  - [X] Déposer les ressources sur Arche (Cours, TD, Squelette)
- <<interval-tree-clock>> DISTRIBUTED SYSTEMS : Lire *Interval Tree Clocks: A Logical Clock for Dynamic Systems*
  - Propose une nouvelle /logical clock/ : /Interval Tree Clock/
  - Problèmes adressés
    - Dans les systèmes distribués dynamiques, le groupe de participant n'est pas fixe
    - Au fur et à mesure, de nouveaux participants vont rejoindre le groupe tandis que d'autres vont le quitter
    - Les mécanismes de causalité existants gèrent mal ce /churn/
      - /Version Vector/ va conserver ad vitam æternam une entrée pour chaque noeud
      - Certains systèmes comme [[Dynamo]] tronquent les /version vectors/ pour conserver une taille fixe
      - Mais ceci entraîne une perte d'informations de causalité qui peut ainsi conduire à la répétition d'une ancienne opération
      - Des travaux ont tenté de répondre au problème de la suppression des noeuds inactifs, sans succès
	- Certains mécanismes utilisent un /agreement/ au niveau du groupe pour déterminer si l'on peut supprimer une entrée de l'horloge
	- Mais il ne suffit alors que d'un seul noeud défaillant pour qu'on ne puisse plus supprimer d'entrées
  - /Fork-Event-Join Model/
    - Représente les mécanismes de causalité à l'aide des opérations /fork/, /event/ et /join/ qui s'appliquent sur des /stamps (logical clocks)/
    - /logical clock/
      - De la forme /(i, e)/ où
	- /i/ représente l'identifiant de l'utilisateur et de son horloge logique
	- /e/ représente les informations de causalités connues
      - Pour comparer 2 horloges logiques, on compare leurs informations de causalité
	- Un ordre partiel doit être établi pour comparer 2 contexte causal : /(E, ≤)/
    - /fork/
      - Permet de générer une nouvelle /clock/ à partir d'une autre
      - /fork(i, e) = ((i1, e), (i2, e))/ avec /i1 ≠ i2/
	- Généralement /i1 = i/
	- /i2/ est une nouvelle identité
    - /peek/
      - Cas particulier de /fork/
      - Permet de créer un snapshot d'une clock
      - Génère une clock anonyme immutable
      - /peek(i, e) = ((i, e)(0, e))/ où /0/ est une identité "null"
      - Permet d'attacher cette clock à des messages ou faire des snapshots pour débugger
    - /event/
      - Permet de faire avancer une clock en faisant évoluer son contexte causal /e/
      - /event(i, e) = (i, e')/ avec /e < e'/
      - Le nouveau contexte causal /e/ ne doit ni dominé ni être dominé par des contextes concurrents
	- si /e1 ≰ e2/ alors /e1' ≰ e2/
	- si /e2 ≰ e1/ alors /e2 ≰ e1'/ avec /e1' ≠ e2/
    - /join/
      - Fusionne deux clocks en une nouvelle
      - /join((i1, e1), (i2, e2)) = (i3, e3)/ avec
	- /e1 ≤ e3/ et /e2 ≤ e3/
	- /e3/ ne doit ni dominé ni être dominé par des contextes concurrents
      - L'identité obtenue doit être fonction des identités fournies et doit de nouveau être unique
	- /i3 = f(i1, i2)/
	- Généralement, on se contente de réutiliser une des identités fournies en paramètre
	  - /f(i1, i2) = i1/ par exemple
	- Mais on peut utiliser une fonction plus complexe, notamment si on souhaite pouvoir recycler les identifiants
      - Cas particuliers
	- /join((i1, e1), (0, e2)) = (i1, e3)/ représente la réception d'un message
	- /join((0, e1), (0, e2)) = (0, e3)/ représente l'aggrégation de messages
    - Les opérations classiques des systèmes peuvent ainsi être composées à partir de ces opérations élémentaires
    - /send/
      - Se décompose en un /event/ suivi d'un /fork/
      - /send(i, e) = ((i, e'), (0, e'))/
    - /receive/
      - Se décompose en un /join/ suivi d'un /event/
      - /receive((i, e1), (0, e2)) = (i, e3')/
	- /event(join((i, e1), (0, e2))) = event(i, e3)/
    - /sync/
      - Se décompose en un /join/ suivi d'un /fork/
      - /sync((i1, e1), (i2, e2)) = ((i1, e3), (i2, e3))/
	- /sync((i1, e1), (i2, e2)) = fork(join((i1, e1), (i2, e2)) = fork(f(i1, i2), e3) = ((i1, e3), (i2, e3))/
  - /Function space based Clock Mechanisms/
    - On dispose d'une fonction O tel que /O(x) = 0/
    - L'idée est d'utiliser une fonction charactéristique /i/ comme identité
      - /i(x) = 1 ∀x ∈ A, A ⊂ dom(f)/
      - /i(x) = 0 sinon/
    - L'identité permet de déterminer l'ensemble des entrées du contexte causal que l'on peut incrémenter lors d'un évènement
      - Les entrées d'un /vector clock/ par exemple
    - La fonction servant d'identité doit donc satisfaire plusieurs contraintes
      - /∀i1 ≠ i2, i1 * i2 = O/
      - Concrètement, aucun autre utilisateur ne doit pouvoir modifier la même entrée que moi dans le contexte causal
    - On peut ainsi redéfinir les opérations élémentaires avec cette nouvelle fonction servant d'identité
    - /join/
      - /join((i1, e1), (i2, e2)) = (i1 + i2, e1 ∪ e2)/
      - Fusionner 2 clocks permet d'obtenir une nouvelle identité correspondant à la composition de /i1/ et /i2/
      - Au cours d'un prochain /event/, on pourra donc incrémenter une ou plusieurs entrées du contexte couvertes par /i1/ et /i2/
    - /fork/
      - /fork(i, e) = ((i1, e), (i2, e))/ avec
	- /i = i1 + i2/
	- /i1 * i2 = O/
      - On répartit la responsabilité des éléments du contexte entre les nouvelles identités
      - On s'assure qu'aucun élément du contexte ne peut être modifié par chaque nouvelle identité
    - /peek/
      - /peek((i, e)) = ((O, e), (i, e))/
      - Comme /O/ renvoie 0 pour n'importe quelle valeur, la clock /(O, e)/ ne peut donc pas produire d'évènements
    - /event/
      - /event((i, e)) = (i, e + f * i)/
	- Avec /f/ une fonction telle que /f * i ⊃ O/
	- C'est à dire que /range(f * i) = {0, 1}/
	  - /f ⊃ g/ signifie que /range(f) ⊃ range(g)/
  - /Interval Tree Clocks/
    - Représente /id/ à l'aide d'un arbre
      - /(1, (0, 1)/ est un exemple d'identité valide
    - Représente /e/ à l'aide un arbre binaire valué
      - /(0, (0, 0, 0), 1) est un exemple de contexte valide
    - Dispose d'une fonction /norm/ permettant de simplifier les arbres binaires
      - Essaie de remonter les valeurs des feuilles dans les noeuds supérieurs pour pouvoir à terme représenter l'arbre à l'aide d'un entier
      - Exemples
	- /norm((2, 1, 1)) = 3/
	- /norm((2, (2, 1, 0), 3)) = (4, (0, 1, 0), 1)/
    - Essaie au maximum de limiter la taille des arbres
      - Lors d'un /event/, en fonction de l'identité de l'utilisateur
	- Essaie de combler les "trous" de l'arbre de façon à pouvoir le normaliser
	- Ou alors incrémente un des noeuds les plus proches possibles de la racine
      - Lors d'un /join/, normalise l'arbre
  - Remarques
    - Faiblesses du système d'identités
      - Le mécanisme d'identité repose sur un espace qui va être au fur et à mesure divisé et partagé entre les participants
      - Il est nécessaire qu'un utilisateur divise l'espace qui lui est alloué pour qu'un autre utilisateur puisse obtenir une identité
      - Concrètement, plusieurs façons de fournir l'identité au nouveau participant
	- Un utilisateur doit être présent au moment où le nouveau participant se connecte pour lui donner une identité
	  - Problème de disponibilité, un utilisateur doit toujours être présent
	- L'identité est fournie dans le lien de l'invitation
	  - Problème d'usabilité, chaque utilisateur doit être invité via un lien unique
      - Nécessaire de répartir la responsabilité d'attribuer une identité
	- À chaque attribution d'identité à un nouvel utilisateur, un ancien utilisateur disponible divise en 2 son espace d'identifiants et en donne la moitié au nouvel arrivant
	- En fonction de l'espace utilisé pour les identifiants, on peut rapidement arriver à un point où un utilisateur n'a plus la possibilité de diviser son identité
	- Si un utilisateur devient indisponible, il scelle tout un espace d'identifiants et réduit le nombre de participants possibles
	- C'est d'autant plus vrai si l'utilisateur fait partie des premiers participants
    - Pour normaliser l'arbre des évènements et ramener sa représentation à un entier
      - Il est nécessaire que chaque utilisateur effectue le même nombre d'opérations depuis le dernier /fork/
      - Plus le nombre d'utilisateurs est conséquent, plus cela semble difficilement atteignable
    - Différences entre /vector clocks/ et /version vectors/
      - /vector clock/ identifie un évènement
      - /version vector/ identifie l'état sur lequel s'est appliqué l'évènement
    - Du mal à voir à quoi on pourrait transposer ce mécanisme de récupération des identifiants et ce que ça apporterait
    - Dans *LogootSplit*, récupérer des identifiants nous permettrait de pouvoir réutiliser blocs existants
      - Plutôt que de générer de nouveaux blocs lors d'insertions, on pourrait effectuer des appends/prepends
      - Le gain semble alors plutôt faible comparé au coût nécessaire pour mettre en place la solution
	- L'utilisation et la gestion d'un arbre binaire pour représenter l'identité des utilisateurs
      - Si on arrive à mettre en place un mécanisme de fusion des blocs basé sur l'ownership, ça pourrait devenir intéressant
    - À voir si d'autres CRDTs profiteraient davantage de ce mécanisme
- Meeting
  - Read *Interval Tree Clocks: A Logical Clock for Dynamic System*
    - Present a new logical clock : /Interval Tree Clock/
    - Adress several issues of logical clocks
      - No need for a global id
      - Can retire id and reuse them
      - Size of the data structure grows and shrinks according to the numbers of users
    - Use binary trees as identity and the actual clock
      - Increase the depth of the identity tree when adding a user
      - Decrease it when a user leave
      - Each user can only update a precise part of the clock tree according to its identity
    - Some questions left
      - On which conditions does the clock shrink ?
      - Can we determine the numbers of events separating two clocks ?
	- Would be useful to compare my clock to the one attached to a message
	- If yes, could be use as it is in operation-based systems
	- Otherwise, would have to store the last seen ITC of each user and replay the inflation to ensure FIFO delivery
	- Anyway can use it in state-based systems
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Lire *Interval Tree Clocks: A Logical Clock for Dynamic Systems*
- Disponible ici : http://gsd.di.uminho.pt/members/cbm/ps/itc2008.pdf
** Semaine du <2017-11-02 Thu> au <2017-11-03 Fri>
*** Done
- MUTE : Préparer la démo pour la réunion avec Marius Shekow
  - Répétition du scénario pour ECSCW 2017
    - Des utilisateurs répartis sur 2 réseaux collaborent sur un document
    - Une partition se produit, coupant le groupe en 2 sous-groupes
    - Ils continuent à collaborer en local
    - Puis la partition se résout
    - L'ensemble des modifications est partagé entre les sous-groupes
    - Tout le monde converge
  - Fonctionne toujours
- CONSENSUS : Lire *The part-time parliament*
  - J'ai commencé à lire le papier
  - Il s'avère plutôt compliqué
  - Guillaume m'a suggéré de chercher des articles traitant de Paxos plutôt que d'essayer de décortiquer le papier
  - Ça me paraît être une bonne idée
- <<2pc>> CONSENSUS : Two-phase commit (2PC)
  - En parcourant des articles sur [[paxos]], je suis tombé sur des articles traitant de 2PC
    - 2PC s'avère être un cas particulier de Paxos
  - Autant prendre notes sur le sujet tant que j'y suis
  - Problème adressé
    - Problème du consensus
      - Faire en sorte qu'un ensemble de noeuds d'un système distribué s'accordent sur une valeur, une action ou autre
    - Possibles utilisations
      - Décider de valider ou d'abandonner une transaction dans une base de données distribuée
      - Élire un leader
      - Avancer à la prochaine étape d'un algorithme distribué (/replicated state machine/ approach)
    - Mais le consensus est connu comme étant impossible à obtenir dans le cas d'un système avec réseau asynchrone si les noeuds peuvent tomber en panne
      - Voir FLP theorem
  - Formalisation
    - Safety properties
      - Agreement : Chaque noeud du système choisit la même valeur
      - Validity : La valeur choisie a été proposée par un des noeuds
    - Liveness property
      - Termination : Chaque noeud prend finalement une décision
  - Protocole
    - Un noeud a le rôle de coordinateur
      - C'est lui qui va s'assurer du bon déroulement du protocole
      - Le noeud coordinateur n'a pas besoin d'être élu, n'importe quel noeud peut spontanément démarrer un 2PC
    - Les autres noeuds jouent le rôle de participants
    - Le protocole se décompose en 2 étapes
    - 1. Proposition
      - Le coordinateur propose une valeur à l'ensemble des participants
      - Les participants renvoie au coordinateur leur réponse
	- Un simple booléen pour indiquer si oui ou non le noeud accepte cette valeur
    - 2. Validation ou abandon
      - Si le coordinateur reçoit une réponse favorable de la part de chaque participant
	- Il indique à l'ensemble des participants que la valeur a été choisie
      - Sinon abandonne le 2PC
  - Limites
    - Comme dit précédemment, le consensus n'est pas possible si un des noeuds peut tomber en panne
    - Ce résultat s'applique ici aussi
    - On rencontre plusieurs problèmes, principalement de terminaison, en fonction du rôle du noeud qui tombe en panne et du moment de la panne
    - Le coordinateur tombe en panne au cours de la phase 1, alors qu'il envoyait les propositions
      - Seulement un sous-ensemble des participants reçoivent la proposition et votent
      - Ces derniers se retrouveront alors bloqués, attendant le résultat du vote
      - Ils ne peuvent pas timeout de façon sûre puisqu'il existe (pour eux) la possibilité que le coordinateur se réveille et passe à l'étape de validation
    - On retrouve le même problème dans le cas d'une panne du coordinateur durant la phase 2
    - Pour contourner problème, on peut ajouter un 3ème type de noeud jouant le rôle fallback en cas de panne du coordinateur
      - Reprend la gestion de la transaction dans le cas d'une panne du coordinateur
      - Recontacte chaque noeud pour récupérer de nouveau son vote effectué lors de la phase 1
	- Nécessité pour les noeuds de conserver une trace de leurs votes
      - Cependant, si un participant vient à crasher à ce moment, on ne peut plus résoudre le consensus
	- Peut pas annuler car p-e que le participant a lui validé
	- Peut pas valider car p-e que le participant a lui refusé lors de la phase 1
  - Ressources
    - http://the-paper-trail.org/blog/consensus-protocols-two-phase-commit/
- <<3pc>> CONSENSUS : Three-phase commit (3PC)
  - Il s'agit d'une évolution de [[2pc]]
  - Vise à fournir un protocole plus résistant aux défaillances pour obtenir le consensus
  - Ajoute pour cela une phase intermédiaire à 2PC
  - Protocole
    - 1. Proposition
      - Même étape que dans 2PC
    - 2. Préparation à validation
      - Le coordinateur envoie ce message à tous les participants s'ils ont voté unanimement "oui"
      - Permet d'informer les participants du vote de son résultat
      - Permet aux participants de verrouiller la transaction, mais sans modifier de façon irréversible leur état
      - Les participants notifient ensuite le coordinateur qu'ils sont prêts à valider la transaction
      - Ainsi, si n'importe quel noeud rencontre une panne, l'état du protocole sera toujours disponible via un des noeuds
    - 3. Validation ou abandon
      - Même étape que dans 2PC
  - Lors de la récupération d'un crash
    - Si tous les noeuds disponibles indiquent avoir reçu le message 2 alors on peut poursuivre et valider la transaction
    - Sinon, on peut annuler la transaction de façon sûre puisque le coordinateur ne serait pas passé à l'étape 3
  - Limites
    - Une panne du coordinateur + une partition des participants peuvent provoquer une divergence
      - Si l'ensemble des noeuds d'une partition ont passé l'étape 2, alors le mécanisme de recovery va poursuivre et valider la transaction
      - Alors qu'il suffit qu'un seul noeud du second sous-groupe n'ait pas reçu le message de l'étape 2 pour que le mécanisme de recovery décide d'annuler la transaction
      - La fusion des sous-groupes lors de la résolution de la partition entraînera donc l'apparition d'une divergence entre les participants
    - Une divergence est possible dans un autre scénario où le coordinateur récupère d'une panne et interfère avec le coordinateur de fallback
      - Si le coordinateur plante après avant d'avoir reçu toutes les réponses au message "prepare-to-commit"
      - Le coordinateur de fallback va prendre la main, récupérer l'état de la transaction et la poursuivre jusqu'à sa validation
      - Cependant, si le coordinateur initial recouvre de sa panne, il va annuler la transaction (n'ayant pas reçu les réponses au "prepare-to-commit")
      - Les participants vont donc valider ou annuler la transaction en fonction du message qu'ils reçoivent en 1er
  - Ressources
    - http://the-paper-trail.org/blog/consensus-protocols-three-phase-commit/
*** Planned
**** DONE MUTE : Préparer la démo pour la réunion avec Marius Shekow
- Dans le cadre de la rencontre avec Marius Shekow, nous allons présenter les travaux de l'équipe
- Une de ces présentations va consister en une démo de MUTE
- Vérifier que le scénario de la démo de ECSCW 2017 fonctionne toujours
**** CANCELLED CONSENSUS : Lire *The part-time parliament*
- *Spanner* a l'air de reposer fortement sur des /Paxos state machines/
- Lire le papier sur *Paxos* permettrait p-e de faciliter la compréhension de *Spanner*
- Disponible ici : https://dl.acm.org/citation.cfm?id=279229
** Semaine du <2017-11-06 Mon> au <2017-11-09 Thu>
*** Done
- <<paxos>> CONSENSUS : Paxos
  - L'approche Paxos se décompose en plusieurs protocoles
    - Basic Paxos qui adresse le problème du consensus
    - Multi Paxos qu combine plusieurs rounds de Basic Paxos pour obtenir un consensus sur plusieurs valeurs et sur leur ordre
  - Basic Paxos
    - Assez proche de [[2pc]]
    - Requirements
      - Safety
	- Une seule valeur est choisie
	- Aucun noeud n'obtient une valeur qui n'a pas été choisie
      - Liveness
	- Hypothèses : une majorité de noeuds sont disponibles et peuvent communiquer dans des délais raisonnables
	- Une valeur est finalement choisie
	- Si une valeur est choisie, tous les noeuds l'obtiennent en finalité
    - Rôles
      - Proposer
	- Gère la requête du client
	- Va soumettre au vote des Acceptors une valeur candidate
	- Va connaître la valeur choisie
      - Acceptors
	- Se contente de voter sur la valeur transmise par le Proposer
	- Va conserver des informations sur les propositions reçues, les réponses effectuées et les valeurs choisies
	- Ne participe pas forcément à tous les votes
	- Va donc chercher à obtenir la valeur choisie
      - Un noeud peut potentiellement cumuler ces rôles
    - Ordonnancement des proposals
      - Le réseau étant asynchrone, l'ordre de réception des propositions peut être perturbé
      - Ceci peut conduire à des divergences
      - Les propositions doivent être ordonnables de façon à pouvoir accepter que la plus récente
      - Donne un identifiant unique à chaque proposition
	- Permet de comparer et d'ordonner les propositions avec leur identifiant
	- 2 noeuds ne doivent pas pouvoir générer le même identifiant
	- Un noeud doit toujours pouvoir générer un identifiant plus grand pour pouvoir écraser une ancienne proposition
      - Identifiant de la forme /<round, serverId>/ où
	- /round/ est le numéro du round actuel
	- /serverId/ est l'identifiant unique du noeud
      - Chaque noeud stocke /maxRound/ qui est la plus grande valeur de /round/ observée
      - Quand un noeud génère un nouvel identifiant, il génère le couple /<maxRound + 1, server>/
    - Protocole
      - Données stockées par noeud
	- /minProposal/ : Le numéro de la plus petite proposition que ce noeud va accepter, 0 s'il n'a toujours pas reçu de proposition
	- /acceptedProposal/ : Le numéro de la dernière proposition acceptée, 0 initialement
	- /acceptedValue/ : La valeur associée à la dernière proposition acceptée, null initialement
	- /maxRound/ : Le numéro du plus grand round observé
      - Le protocole se décompose en 2 étapes
      - 1. Proposition
	- Le Proposer
	  - génère un nouveau proposal number /n/
	  - broadcast un message /Prepare(n)/ à tous les noeuds
	- Les Acceptors recevant ce message
	  - Mettent à jour leur variable /minProposal/ si /n > minProposal/
	  - Répondent avec /<acceptedProposal, acceptedValue>/
      - 2. Validation
	- Quand le Proposer reçoit une majorité de réponses positives des Acceptors
	  - Récupère la valeur /acceptedValue/ pour laquelle /acceptedProposal/ est maximum
	  - Broadcast le message /Accept(n, value)/
	- Les Acceptors recevant ce message
	  - Si /n ≥ minProposal/
	    - /acceptedProposal = n/
	    - /minProposal = n/
	    - /acceptedValue = value/
	  - Dans tous les cas, renvoie ensuite /Return(minProposal)/
	- Quand le Proposer reçoit une majorité de réponses des Acceptors
	  - Si une seule réponse renvoie une valeur /result/ tel que /result > n/, alors on recommence le protocole
	  - Sinon, la valeur est choisie
      - Représenté slide 12 de [[file:resources/paxos.pdf]]
    - Limites
      - Le protocole peut ne pas finir dans certains cas
      - Notamment le cas où deux Proposers se battent pour avoir le /proposalNumber/ le plus élevé
	- Si leurs messages /Prepare/ s'entrecroisent, chaque /Prepare/ d'un Proposer va déprécier le message /Accept/ de l'autre Proposer
	- Tant que les deux Proposers ne se seront pas observés et qu'un d'entre eux n'aura pas céder sa place, aucune valeur ne sera choisie
	- Voir slide 16 de [[file:resources/paxos.pdf]]
      - Finalement, seul le Proposer sait de manière sûre si une valeur a été choisie
      - Pour obtenir cette information, chaque Acceptor doit exécuter /Basic Paxos/ à son tour
  - Multi Paxos
    - Pas spécifié dans la littérature, contrairement à Basic Paxos
    - Il n'y a donc pas de preuve non plus
    - Utilise plusieurs rounds de Basic Paxos pour sélectionner une valeur pour chaque entrée d'un log
    - Apporte plusieurs changements à Basic Paxos afin de le rendre utilisable et plus performant dans ce scénario
    - Peut exécuter en concurrence le protocole pour des entrées différentes
      - Ajoute un index aux messages /Prepare/ et /Accept/ pour indiquer l'entrée du log concernée par ce round
    - Protocole
      - Lorsqu'une requête d'un client est reçue
	- Détermine la 1ère entrée du log qui n'est pas choisie
	  - Mais on peut déjà posséder une valeur pour cette entrée
	- Exécute /Basic Paxos/ pour cette entrée avec la valeur du client
	- Si une des réponses au message /Prepare/ contient une /acceptedValue/
	  - Choisit cette valeur pour cette entrée
	  - Recommence le protocole afin de traiter la requête du client
	- Sinon poursuit le protocole l'exécution de Basic Paxos avec la valeur du client
      - Décrit slide 20 de [[file:resources/paxos.pdf]]
    - Élection de leader
      - Utiliser Basic Paxos tel quel est inefficace
	- Possible d'avoir plusieurs Proposers concurrents qui vont générer des conflits
	- Besoin de 2 rounds de query/response pour chaque valeur (/Prepare/, /Accept/)
      - Élire un leader permet
	- De réduire à 1 le nombre de Proposer à un moment donné, et donc limiter les possibilités de conflits
	- D'éliminer la nécessité de messages /Prepare/ dans la majorité des cas
      - Protocole d'élection
	- Le serveur avec l'identifiant maximum à un moment donné obtient le rôle de leader
	- Chaque serveur envoie un heartbeat de façon périodique à tous les autres serveurs
	- Si un serveur ne reçoit aucun heartbeat d'un autre serveur possédant un ID supérieur depuis un certain temps, il devient le leader
	- Présenté slide 23 de [[file:resources/paxos.pdf]]
    - Suppression de messages /Prepare/
      - /Prepare/ permet de
	- Bloquer les anciennes propositions
	- Obtenir les valeurs potentiellement choisies
      - On peut ainsi éliminer /Prepare/ si
	- Les numéros de propositions ne sont pas par entrée mais pour le log entier
	- On ajoute à la réponse à /Prepare/ un booléen /noMoreAccepted/
	  - Indique si pour toutes les entrées suivantes, aucune proposition n'a été acceptée
      - Ainsi, si un Acceptor répond à /Prepare/ avec le flag /noMoreAccepted/
	- Plus besoin de /Prepare/ avec cet Acceptor
      - De plus, si une majorité d'Acceptors ont répondu avec ce flag
	- Plus besoin de /Prepare/ pour l'ensemble des noeuds
    - Partage des informations
      - Plusieurs données ne sont pas partagées avec l'ensemble des noeuds
	- Seulement une majorité des noeuds reçoit la valeur choisie pour une entrée, et non pas tous les noeuds
	- De plus, seul le Proposer sait si cette entrée a été choisie
      - Dans le cas d'une /Replicated State Machine/, les commandes composant le log doivent être appliquées dans l'ordre
      - Les slides 25 à 27 de file:resources/paxos.pdf décrivent comment répliquer ces informations
  - Ressources
    - http://the-paper-trail.org/blog/consensus-protocols-paxos/
    - https://www.quora.com/In-distributed-systems-what-is-a-simple-explanation-of-the-Paxos-algorithm
    - https://www.youtube.com/watch?v=JEpsBg0AO6o
    - https://ramcloud.stanford.edu/~ongaro/userstudy/
- PHD DAY : Rédiger un abstract sur le sujet de thèse
  - Disponible à https://github.com/MatthieuNICOLAS/phd-day-2017/blob/master/abstract/main.pdf
*** Planned
**** DONE ADMINISTRATIF : Remplir le formulaire *Charges d’enseignement pour Doctorant contractuel*
**** DONE PHD DAY : Rédiger un abstract sur le sujet de thèse
     SCHEDULED: <2017-11-15 Wed>
- Doit fournir le sujet de thèse et un abstract (environ 200 mots) pour le PhD Day
  - Emphasize on what is your research problem, who else are working on the same/similar problem, what is your specific approach, and what is/are your expected results
** Semaine du <2017-11-14 Tue> au <2017-11-17 Fri>
*** Done
- <<raft>> CONSENSUS : Raft
  - Adresse le problème du consensus pour répliquer un log
    - Permet de répliquer une machine à état
  - Conçu pour être simple à comprendre
  - Tout en offrant les mêmes garanties et performances que Paxos
  - Afin de simplifier son fonctionnement, Raft repose sur l'utilisation d'un leader
    - Supprime les conflits
  - Rôles
    - Leader
      - Gère les requêtes des clients
      - Envoie des messages /AppendEntries/
	- Permet de répliquer son log
	- Sert aussi de /heartbeat/ pour maintenir son leadership
    - Candidate
      - État intermédiaire
      - Envoie des messages /RequestVote/ afin d'être élu comme leader
    - Follower
      - Se contente de traiter et de répondre aux différents messages qu'il reçoit
    - Dès qu'un noeud découvre un noeud avec un terme supérieur, il redevient un Follower
    - Représenté slide 10 de https://raft.github.io/slides/uiuc2016.pdf#page=10
  - Termes
    - L'exécution du protocole se fait au cours de termes successifs
    - Un terme est composé
      - D'une phase d'élection
      - D'une phase d'opérations normales
    - Si l'élection échoue, le terme n'a pas de leader
      - Aucune requête ne peut être traité durant ce terme
    - Chaque noeud conserve la valeur du terme courant
      - Joint à chaque message
      - Si un noeud reçoit un message contenant un identifiant de terme plus grand, redevient un follower
      - Si un noeud reçoit un message contenant un identifant de terme plus petit, répond une erreur
    - Permet d'identifier les messages obsolètes
    - Représenté slide 11 de https://raft.github.io/slides/uiuc2016.pdf#page=11
  - Propriétés du log
    - Si différents noeuds possèdent une entrée du log avec le même index et le même terme
      - Ces entrées correspondent à la même commande
      - Les entrées précédentes des logs sont identiques
    - Si une entrée du log est /committed/
      - Toutes les entrées précédentes le sont aussi
  - Protocole
    - Le protocole se décompose en plusieurs sous-problèmes
    - 1. Leader election
      - Décrit slide 12 de https://raft.github.io/slides/uiuc2016.pdf#page=12
      - Propriété de safety
	- Au maximum un seul leader peut être élu par terme
	- Pour assurer cette propriété
	  - Un noeud ne peut voter qu'une fois par terme
	  - Un noeud a besoin d'une majorité de votes positifs pour remporter l'élection
      - Propriété de liveness
	- Un candidat doit être élu finalement
	- Pour assurer cette propriété
	  - Le délai observé par les noeuds avant de passer au terme suivant n'est pas le même pour chacun
	  - Ainsi, dans le cas où une élection échoue à cause de l'absence d'une majorité, un noeud démarre une nouvelle élection avant les autres et parvient généralement à être élu
    - 2. Normal operation (basic log replication)
      - Le leader traite les requêtes envoyées par le client
      - Ajoute la commande à son log
      - La partage avec les autres noeuds via un message /AppendEntries/
      - Si l'entrée /e/ et l'entrée suivante /e+1/ sont connues par une majorité de noeuds, alors /e/ est dite /committed/
      - Une fois l'entrée /committed/
	- Le leader exécute la commande sur sa machine à état et renvoie le résultat au client
	- Le leader indique via les prochains messages /AppendEntries/ que l'entrée est /committed/
	- Les followers peuvent à leur tour exécuter la commande /committed/ à la réception d'un des prochains messages
    - 3. Safety and consistency after leader changes
      - Une fois une opération /committed/, tous les leaders futurs devront posséder cette entrée
	- Permet d'assurer qu'un nouveau leader ne va pas rentrer en conflit avec le log
      - Pour empêcher les noeuds en retard d'être élu
	- Chaque candidat inclut l'index et le terme de sa dernière entrée du log dans les messages /RequestVote/
	- Lorsqu'un noeud reçoit un message /RequestVote/ avec un terme ou un index obsolète, il le rejette
	- Ainsi, seul un noeud possédant la dernière opération /committed/ peut être élu
      - Le log du leader est considéré comme le log véritable
      - Les logs inconsistents vont être corrigés par le biais des messages /AppendEntries/
	- Le message contient le couple <index, terme> de l'entrée précédante les nouvelles
	- Si le noeud ne possède pas l'entrée correspondante, il rejecte la requête
	- Le leader va remonter dans le log jusqu'à trouver la dernière opération commune et va lui envoyer le log à partir de celle-ci
	- Algorithme décrit slide 18 de  https://raft.github.io/slides/uiuc2016.pdf#page=18
    - 4. Neutralizing old leaders
      - Un leader peut perdre son rôle autrement qu'à cause d'un crash
      - Notamment dans le cas d'une partition réseau
      - Pendant ce temps, le groupe élit un nouveau leader et traite les requêtes des clients
      - À la reconnexion de l'ancien leader, il faut l'empêcher de rentrer en conflit avec le reste du groupe
      - Utilise la valeur de son terme courant
	- Joint à chaque message
	- Un noeud recevant un de ses messages va donc lui répondre que sa valeur de terme est obsolète
	- L'ancien leader va redevenir un follower et va mettre à jour sa valeur de terme
    - 5. Client interactions
      - Envoie ses requêtes au leader
      - Si une requête timeout
	- Par exemple si le leader crash
	- Contacte un autre noeud
	- Va éventuellement être redirigé vers le nouveau leader
	- Va pouvoir resoumettre sa requête
      - Mais une requête peut timeout après que la commande ait été /committed/ et exécutée mais avant que la réponse ne soit envoyée au client
	- Afin d'éviter d'exécuter 2 fois la même commande, le client ajoute un id unique à chaque commande
	- Cet id est conservé dans l'entrée du log
	- Lorsque le leader reçoit une requête, il vérifie que l'id de la commande n'est pas déjà présent dans le log
	- Si oui, se contente de renvoyer le résultat de cette entrée
      - Permet d'assurer une /exactly-once delivery/
    - 6. Configuration changes
      - Voir slides 27 à 30 de https://ramcloud.stanford.edu/~ongaro/userstudy/raft.pdf#page=27
  - Ressources
    - https://raft.github.io
    - https://ramcloud.stanford.edu/~ongaro/userstudy/
    - https://www.infoq.com/presentations/raft
    - https://raft.github.io/slides/uiuc2016.pdf
*** Planned
** Semaine du <2017-11-20 Mon> au <2017-11-24 Fri>
*** Done
- <<spanner>> DISTRIBUTED SYSTEMS : Lire *Spanner: Google’s Globally Distributed Database*
  - Distributed multiversion database
    - General-purpose transactions (ACID)
    - SQL query language
    - Schematized tables
    - Semi-relational data model
  - Allow to shard and replicate the data
  - Achieve lock-free distributed read transactions
    - Able to do so thanks to Spanner's property : distributed transactions are externally consistent
  - External consistency
    - Commit order of the transactions is the same as the order in which you observe the transactions
  - Use 2 Phase-Locking (2PL) to ensure serializability of the write on the database
  - Add a timestamp to each transaction
  - Synchronizing snapshosts
    - Use a global wall-clock time
      - Equivalent to the External Consistency
      - Commit order respects global wall-time order
      - Use timestamp and maintain timestamp order === commit order
  - TrueTime
    - Global wall-clock time with bounded uncertainty
    - /TT.now().latest/ returns the maximum timestamp given /now()/ and the bounded uncertainty
    - /TT.now().earliest/ returns the minimum timestamp given /now()/ and the bounded uncertainty
  - Generating timestamps
    - Assign timestamp while two-phase locking is ongoing
    - At the beginning, once the locks acquired, pick /s = TT.now().latest/
    - Do not release the locks until /TT.now().earliest > s/
  - Able to mask the commit wait by performing the consensus or the 2PC meanwhile
  - Ressources
    - https://www.youtube.com/watch?v=NthK17nbpYs
- CRDT : Idée pour le renommage
  - Modèle du système
    - Système distribué à large échelle
    - Réseau asynchrone
    - Partition-tolerant
    - Eventual consistent
    - Préservation de l'intention des utilisateurs
  - Opération de renommage
    - Fusionne tous les éléments existants d'un état donné en un seul nouvel élément ayant pour identifiant /id'/
    - /rename(state) = (state', id')/
      - L'état peut être associé à /version vector/
    - Génère donc un nouvel état de la structure de donnée avec le même contenu mais des méta-données différentes et compactées
    - Un seul noeud peut effectuer l'opération de renommage
    - Des noeuds peuvent être déconnectés lors de la génération de cette opération
    - Nécessite une livraison causale
      - Ne peut effectuer l'opération de renommage que si on possède toutes les opérations précédentes
    - Marque le début/fin d'une époch
      - Permet de filtrer les opérations obsolètes ou trop avancées pour notre copie
  - Application de l'opération de renommage
    1. Dépile/Undo les opérations connues en local qui sont concurrentes avec l'opération de renommage, récupère les opérations textes correspondantes
    2. Obtient alors l'état sur lequel a été effectué l'opération de renommage
    3. Remplace le modèle courant par celui de l'opération de renommage
    4. Rempile/Redo les opérations textes sur le nouvel état
    5. Propage les opérations ainsi générées
  - Problèmes
    - Si chaque noeud se contente uniquement de rejouer ses opérations, on peut perdre au cours du renommage des opérations
      - Potentiellement, un noeud peut ne jamais se reconnecter et donc jamais rejouer ses opérations
      - L'opération de renommage pourrait aussi être interprétée par les utilisateurs comme plusieurs opérations de suppression, puisque des éléments vont disparaître momentanément
      - Ce qui peut entraîner des réactions provoquant une duplication de l'intention
	- Ré-insertion d'un mot qui a été "supprimé" au cours du renommage
    - Si chaque noeud rejoue les opérations de tous les utilisateurs, on risque de dupliquer l'intention
      - Si plusieurs noeuds ont observé la même opération concurrente à l'opération de renommage, ils vont chacun la ré-appliquer
  - Idée
    - On peut modifier la stratégie de génération de l'identifiant lorsqu'on rejoue les opérations concurrentes
    - Si on génère de façon déterministe l'identifiant obtenu en rejouant l'opération, on ne duplique plus l'intention
      - Chaque utilisateur va générer la même opération distante pour une opération concurrente donnée
  - Reformulation du problème de recherche
    - Peut-on proposer une ou des stratégies déterministes de génération des identifiants pour une opération rejouée ?
  - Générer de manière déterministe un identifiant pour une opération donnée
    - Données communes
      - État commun
	- Commence à rejouer les opérations sur un état commun (état obtenu après le renaming)
      - Ancien identifiant de l'élément
	- Peut tout utiliser ou juste une partie
    - Données pouvant varier
      - La séquence d'opérations
	- Potentiellement, des opérations communes à plusieurs noeuds peuvent avoir été observées dans des ordres différents
	- La séquence peut aussi des comporter des opérations n'apparaissant pas chez tous les noeuds
    - À partir de ces données, doit définir une stratégie déterministe de génération du nouvel identifiant
  - Formalisation
    - Étant donné
      - un état S
      - une opération /view(S)/ permettant de récupérer le contenu de la structure de données pour un état donné
    - Peut-on définir des opérations /undo/ et /redo/ tels que
      - /undo(S, remoteOp) = (S', redoOp)/
	- Annuler une opération distante change l'état et renvoie une opération locale enrichie de façon à pouvoir la ré-appliquer
      - /redo(S, redoOp) = (S', remoteOp)/
	- Ré-appliquer une opération locale enrichie
      - Vérifiant les invariants suivants
	1. /undo/ puis /redo/ une opération doit préserver le contenu de la structure de données
	   - Permet de s'assurer qu'on ne perd pas l'intention au cours du /undo/ /redo/
	   - Par contre, l'état final ainsi que l'opération regénérée peuvent être différent de l'état de départ et de l'opération annulée
	   - /view(S) = view(S')/ où
	     - /(S_temp, redoOp) = undo(S, op)/
	     - /(S', _) = redo(S_temp, redoOp)/
	2. L'état obtenu après avoir /undo/ une séquence d'opérations ne doit pas dépendre de l'ordre
	   - Permet de remplacer l'état obtenu par l'état fourni par l'opération de renommage de façon sûre
	   - /S = S'/ où
	     - /(S1, _) = undo(Sinit, op_1)/
	     - /(S2, _) = undo(Sinit, op_2)/
	     - /S, _) = undo(S1, op2)/
	     - /S', _) = undo(S2, op1)/
	3. L'état final et les opérations regénérées après avoir /undo/ une séquence d'opérations puis les avoir /redo/ ne doivent pas dépendre de l'ordre de la séquence
           - Permet de s'assurer que l'état final est le même pour chaque noeud et que les opérations communes aux noeuds sont regénérées de manière déterministe
	   - /S = S'/ où
	     - /(S_1, redoOp_1) = undo(S_init, op_1)/
	     - /(S_2, redoOp_2') = undo(S_init, op_2)/
	     - /S_before-rename, redoOp_2) = undo(S_1, op_2)/
	     - /S_before-rename, redoOp_1') = undo(S_2, op_1)/
	     - /view(S_before-rename) = view(S_rename)/
	     - /(S_3, op_2') = redo(S_rename, redoOp_2)/
	     - /(S_4, op_1') = redo(S_rename, redoOp_1')/
	     - /(S, op1') = redo(S3, redoOp_1)/
	     - /(S', op2') = redo(S4, redoOp_2')/
      - Est-ce suffisant ?
	- C'est pas sur /undo/ et /redo/ séparément que doivent porter les contraintes
	- Mais sur leur combinaison
  - Points négatifs
    - Besoin de conserver le log des opérations ou du moins une partie
      - Afin de pouvoir undo les opérations en respectant l'ordre imposée par la couche livraison
      - Peut retirer de façon sûre des opérations du log une fois qu'elles sont dans le contexte causal
    - Résoudre plusieurs opérations de renommage concurrentes me paraît difficile
      - Pour le moment, se contente d'éliminer la possibilité d'avoir des opérations de renommage concurrentes
      - Dans le système décrit ici, on établit comme hypothèse qu'un seul noeud peut effectuer une opération de renommage (owner ? bot dédié ?)
      - Single Point Of Failure
      - Peut suggérer l'utilisation de rôles et d'un méchanisme de consensus pour permettre de décentraliser la responsabilité
	- En dehors du scope pour le moment
  - Réflexions
    - La complexité est proportionnelle au nombre d'opérations concurrentes à l'opération de renommage
    - En fonction de la stratégie de génération des identifiants choisie, les identifiants des opérations concurrentes peuvent grossir par rapport à leur identifiants initiaux
      - Peut argumenter que ces derniers seront réduits lors du prochain renommage
    - Pas forcément gourmand pour le réseau
      - Opération de renommage consiste juste en un identifiant associé à une version du modèle
      - L'état du document est obtenu lors de la phase 2
      - Taille de l'opération de renommage proportionnel au nombre de noeuds
- Meeting
  - Study the past weeks consensus algorithms
    - Paxos
    - Raft
  - Also took a look at Spanner, Google's Globally Distributed Database
    - Distributed multiversion database
    - Achieve lock-free distributed read transactions
    - Thanks to a global wall-clock time
  - Start writing down some ideas for the renaming problem
    - One of them seems to be a good lead
    - Will discuss about it with Gérald and Olivier to see if it is actually a good idea
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Lire *Spanner: Google’s Globally Distributed Database*
- Disponible ici : https://dl.acm.org/citation.cfm?id=2491245
** Semaine du <2017-11-27 Mon> au <2017-12-01 Fri>
*** Done
- PHD DAY : Préparer une présentation sur le sujet de thèse
  - Disponible ici : https://github.com/MatthieuNICOLAS/phd-day-2017/blob/master/presentation/presentation.pdf
  - Réalisation d'une 1ère version
  - Réunion avec Olivier et Gérald
    - Ajouter logos LORIA et CNRS
    - Slide 2
      - Remplacer les utilisateurs par des noeuds
      - Griser un noeud pour le faire apparaître comme déconnecté
    - Entre slide 2 et 3
      - Ajouter une slide avec un réseau plus complexe de noeuds, avec partitions et/ou noeuds déconnectés
    - Slide 3
      - Ajouter contrainte d'unicité
      - Remplacer "..." par "Many others"
    - Slide 4
      - Retravailler le "research problem" sous l'angle de immutabilité/mutabilité
      - Immutabilité nous a permis d'avoir des identifiants globaux permettant des opérations concurrentes tout en assurant la convergence
      - Mais nous a donné des identifiants à taille non-bornée
      - On voudrait pouvoir renommer les identifiants pour réduire leur taille
      - Mais cela viendrait à réinsérer une notion de mutabilité dans les identifiants
      - Est-ce une bonne idée ?
    - Slide 5
      - La supprimer
- CRDT : Rédiger un rapport sur l'idée pour le renommage
*** Planned
**** DONE PHD DAY : Préparer une présentation sur le sujet de thèse
     SCHEDULED: <2017-12-07 Thu>
- Dans le cadre du PhD Day, je dois effectuer une présentation de 5min
  - Emphasize on what is your research problem, who else are working on the same/similar problem, what is your specific approach, and what is/are your expected results
** Semaine du <2017-12-04 Mon> au <2017-12-08 Fri>
*** Done
- PHD DAY : Préparer une présentation sur le sujet de thèse
  - Prise en compte des retours de Olivier et Gérald
  - Répétition avec les autres doctorants de l'équipe
  - Retours
    - Les couleurs des schémas sont p-e un peu faibles et peuvent gêner la lecture (surtout pour les gens daltoniens)
  - Modification des couleurs utilisées
- CRDT : Rédiger un rapport sur l'idée pour le renommage
  - Refonte de la partie sur les états, identifiants et /add/
  - Complétion de la section /Discussion/
  - Trouvé un contre-exemple pour l'algorithme proposé en 2.4
    - Si /op2/ et /op3/ en concurrence et que /id2 < id3/
    - Noeud B va obtenir l'état suivant avant la réception de l'opération renommage : /[(id1, elt1), (id2, elt2), (id3, elt3)]/
    - Mais jouer l'opération de renommage va conduire à l'état /[(id'1, elt1), (id'3, elt3), (id'2, elt2)]/
    - Puisqu'on va rejouer l'opération /addLocal(1, elt3)/
    - On vient donc de perturber l'ordre initial des éléments
  - Le mécanisme des /undo/ permettait de /transformer/ l'opération locale par rapport à l'état courant
- ECSCW2017 : Ajouter le papier sur HAL
  - François s'en est chargé
*** Planned
**** CANCELLED ECSCW2017 : Ajouter le papier sur HAL
**** DONE CRDT : Rédiger un rapport sur l'idée pour le renommage
**** DONE CRDT : Ajouter le contre-exemple au rapport sur le renommage
** Semaine du <2017-12-11 Mon> au <2017-12-15 Fri>
*** Done
- CRDT : Mettre à jour rapport sur le mécanisme de renommage
  - Nouvel algorithme
    - Si on possède une opération /renameRemote(mapIds, causalContext)/, il est possible d'effectuer le renommage de la façon suivante
      - Pour chaque opération concurrente /remoteOp/
	- Si /id/ n'est pas présent dans /mapIds/
	  - Trouver son prédecesseur /prevId/ et son image /prevId'/ dans /mapIds/
	  - Générer /id'/ à partir de /prevId'/ (une simple concaténation de /prevId'/ et de /id'/ suffit)
	  - Ajouter /id -> id'/ dans /mapIds/
	  - Propager /remoteOp'/ (nécessaire ?)
      - Pour chaque /(id, elt)/ de l'état
	- Remplacer /id/ par /id'/ (un simple parcours de la structure suffit)
    - Discussion
      - Comment gérer les opérations concurrentes à l'opération de renommage lorsqu'on a déjà appliqué l'opération de renommage
	- Soit on rejette les opérations concurrentes et on attend qu'un autre pair nous envoie la version modifiée de l'opération
	  - Si un pair nous l'envoie, il va probablement pouvoir nous envoyer sa transformée
	  - Mais rajoute du délai
	- Soit on accepte l'opération et on la transforme nous-même
	  - Pour ça, besoin de récupérer la /mapIds/
	    - Besoin de la stocker ou de la regénérer
      - Quelles versions des opérations stocker dans le log ?
	- Est-ce qu'on stocke les opérations initiales ?
	- Ou les opérations transformées ?
	- Ou un mix des 2 ?
      - Peut réduire la quantité d'information contenue dans l'opération de renommage au prix de calcul supplémentaire
	- À partir du contexte causal, un noeud peut regénérer /mapIds/
	  - En rejouant les opérations appartenant au contexte causal, on récupère l'état sur lequel a été appliqué l'opération de renommage
	  - En effectuant une opération de renommage local sur cet état, on obtient /mapIds/
	  - Ceci fonctionne tant que l'opération de renommage local est déterministe pour un état donné
    - Limites
      - Nécessité d'une livraison causale de l'opération de renommage
      - Ne gère pas les renommages en concurrence
	- Un noeud peut avoir un rôle particulier lui permettant de déclencher cette opération
	- Ce rôle peut être attribué de manière fixe
	- Ou un mécanisme d'élection de leader peut être utilisé pour déterminer qui possède ce rôle
  - Mise à jour du rapport
    - Suppression du contenu relatif à la proposition initiale
      - Figures
      - Discussion
      - Reformulation du problème
      - Contre-exemple
    - Ajout du nouvel algorithme pour /renameLocal/
    - Ajout du nouvel algorithme pour /renameRemote/
  - Nouvel discussion avec Gérald
    - Les algorithmes ont l'air de fonctionner
      - On a revu les algorithmes proposés
      - On a pas pu trouver de contre-exemple aboutissant à une divergence
    - Le problème porte sur /mapIds/
      - Cette map est volumineuse et coûteuse, notamment à broadcast
      - Elle contient tous les anciens identifiants (gros) et les nouveaux (petits)
    - L'optimiser améliorerait la solution
      - Les autres noeuds ont besoin de pouvoir la regenérer de leur côté, peu importe leur état courant
	- Pour ça, ils ont juste besoin de la liste des identifiants à renommer et de leur index
	- Un identifiant peut être compressé pour ne conserver que son /id_site/ et sa /clock_site/
      - Au lieu de /mapIds/, on peut donc envoyer une séquence de couple /(id_site, clock_site)/
    - Prochaines étapes
      - Rédiger partie sur l'optimisation du mécanisme en terme de bande passante
      - Adapter le mécanisme pour les blockwise CRDTs
      - Adapter le mécanisme pour un système distribué
	- Supprimer la contrainte du super-noeud étant responsable du renommage
    - Questions restantes sur la solution actuelle
      - Gestion des opérations concurrentes à un renommage lorsqu'on a déjà effectué le renommage
	- Peut juste les refuser et attendre qu'un utilisateur nous envoie leur version modifiée
	- Ou peut effectuer soi-même la transformation
	  - Besoin de conserver la map pour cela
	  - Quand peut-on garbage collect la map de façon sûre ?
	    - Dès qu'on a, pour chaque utilisateur, une opération qui est de la nouvelle /epoch/ et qu'on a reçu toutes ses opérations précédentes
      - État du log
	- En fonction de la gestion des opérations concurrentes
  - Mise à jour du rapport
    - Ajout de la partie sur l'optimisation du mécanisme
*** Planned
**** DONE CRDT : Mettre à jour rapport sur le mécanisme de renommage
**** DONE CRDT : Ajouter la partie sur l'optimisation de la solution dans le rapport
** Semaine du <2017-12-18 Mon> au <2017-12-22 Fri>
*** Done
- CRDT : Adaptation du mécanisme de renommage aux blockwise-CRDTs
  - Ajout d'une section détaillant *Identifier* et *IdentifierInterval* dans les blockwise-CRDTs
  - Ajout des algorithmes adaptés pour les blockwise-CRDTs
  - Ajout des explications de chaque algorithme
*** Planned
**** DONE CRDT : Adaptation du mécanisme de renommage aux blockwise-CRDTs
** Semaine du <2018-01-08 Mon> au <2018-01-12 Fri>
*** Done
- MUTE : Corriger l'assertion sur l'append vide
  - Ajout d'un test /append-replayed-as-insert/ violant cette assertion
  - Modification de /addBlockRec()/ pour ne plus tenter d'append un texte vide à un bloc
  - Ajout d'un test /prepend-replayed-as-insert/ violant cette assertion
  - Modification de /addBlockRec()/ pour ne plus tenter de prepend un texte vide à un bloc
- MUTE : Corriger /createBetweenPosition()/
  - Modification de la méthode de génération du /random/ pour que /random ∈ ]tuple1.random, tuple2.random[/
  - Ajout d'un commentaire expliquant pourquoi /tuple2.random/ est exclue de l'interval
- MUTE : Corriger l'export du log et de l'arbre d'un document
  - Modification de *StorageService* pour ajouter /getDocBodyAsBlob()/, fonction permettant de récupérer le log d'un document
  - Modification de *DevLabelComponent*
    - Pour utiliser /getDocBodyAsBlob()/ pour récupérer le log du document courant
    - Déclencher correctement le téléchargement du log ou de l'arbre
- MUTE : Réflexion sur /replicaNumber/, /clock/ et identité de l'utilisateur
  - On peut, grâce au mécanisme d'authentification, obtenir de façon déterministe un /replicaNumber/ pour une identité
  - On pourrait donc partager ce /replicaNumber/ entre plusieurs machines
  - Mais la /clock/ pose problème
  - Celle-ci va évoluer en fonction de mes opérations
  - Si je change d'appareil, je peux récupérer via le système d'authentification mon /replicaNumber/
  - Mais je ne suis pas capable de récupérer la dernière valeur de ma /clock/ dans un environnement entièrement distribué
    - Les collaborateurs connectés à ce moment n'ont pas forcément observés mes dernières opérations
  - Pour assurer l'unicité des identifiants générés et pour assurer la disponibilité du système, *je ne peux pas conserver mon /replicaNumber/ d'une machine à l'autre*
  - On peut par contre mettre en place et gérer une *Map* liant mon identité à la liste de mes /replicaNumbers/
    - Cette *Map* devrait être répliquée chez chaque utilisateur et devrait être fusionnable avec celles des autres
- Meeting
  - Before holidays, present a first draft of a renaming mechanism to Gérald
  - It seems to work
  - Will now implement it to evaluate it
  - So have been working on MUTE to prepare its prototyping
  - Have branch corresponding to the refactoring of identifiers
  - Was never merged
    - Were not able to test it because of the eduroam issue
  - Updated it
  - Fixed parts of the code which triggered assertions
  - Deployed it on dev
  - Will organize a session to perform some user testing next week
- Réplication et cohérence de données : Cours 1
  - Reliability
    - La durée qu'un système fonctionne sans tomber de panne
  - Availability
    - Probabilité que le système soit accessible à un moment donné
  - Depandability
    - Reliability * Availability
  - Sequential Consistency
    - À partir des observations de tous les sites, on peut construire un entrelacement des opérations qui
      - Respecte l'ordre des opérations locales
      - Respecte le résultat des opérations
    - et qui serait exécuté par tous les sites
    - Coûteuse à assurer
      - Besoin de donner la priorité aux opérations d'écriture
      - Les lectures sont mises en attente pendant que les écritures sont effectuées globalement
      - Diminue la disponibilité du système
  - Linearizability
    - Modèle de cohérence reprenant Sequential Consistency, mais en plus strict
    - Ajoute un timestamp aux opérations
    - Les opérations doivent être ordonnées dans la séquence telles que les timestamps des opérations soit croissants
  - Modèles de cohérence vs. protocoles de cohérence
    - Les protocoles décrivent une implémentation d'un modèle de cohérence donné
  - Remarques
    - Slide 32 : faire apparaître qu'on a plus de dépendance/d'ordre entre /R(x)a/ et /W(x)b/ ?
    - Slide 34 : les 2 processus ne peuvent pas aussi se tuer mutuellement avec la causalité ?
*** Planned
**** DONE MUTE : Corriger l'assertion sur l'append vide
- L'assertion suivante déclenche dans un cas une erreur : https://github.com/coast-team/mute-structs/blob/rework-identifiers/src/ropesnodes.ts#L154
- On essaie donc d'append un texte vide
- Ce scénario arrive dans le cas suivant :
  1. Un utilisateur A créé un bloc
  2. Il append du texte à ce bloc
  3. Il split le bloc à l'endroit du append
  4. Un utilisateur B reçoit les opérations dans l'ordre 1, 3 puis 2
- Modifier le code pour ne plus essayer d'ajouter un texte vide à un bloc
**** DONE MUTE : Corriger /createBetweenPosition()/
- On a récemment retravailler /createBetweenPosition()/ de façon à autoriser la valeur de /tuple2.random/ comme valeur de random pour le nouveau tuple
- Cette modification entraîne la possibilité de générer un nouvel identifiant /newId/ à partir de /id1/ et /id2/ tel que /id1 < id2 < newId/
  - Puisqu'on garantit que /newId.random <= id2.random/
  - Mais potentiellement /newId.replicaNumber > id2.replicaNumber/ ou /newId.replicaNumber === id2.replicaNumber && newId.clock > id2.clock/
- Corriger cette fonction
**** DONE MUTE : Corriger l'export du log et de l'arbre d'un document
- Les fonctionnalités d'export du log et de l'arbre d'un document ne fonctionnent plus
- Vu qu'on souhaite tester la nouvelle version des identifiants, pouvoir récupérer les logs en cas de bugs est indispensable
- Corriger cette
** Semaine du <2018-01-15 Mon> au <2018-01-19 Fri>
*** Done
- MUTE : Mettre en place *Protractor* pour réaliser des tests E2E
  - Le projet était déjà configuré pour utiliser *Protractor* pour réaliser des tests E2E
  - Ajout de tests
    - Stockage du document
      - Créé un document
      - Modifie le document
      - Refresh la page
      - Vérifie qu'on a bien récupéré le contenu du document
    - Broadcast temps réel des opérations
      - Créé un document avec un premier pair
      - Ouvre le document avec un second pair
      - Modifie le document avec le premier pair
      - Vérifie que le second pair observe les modifications
      - Modifie le document avec le second pair
      - Vérifie que le premier pair observe les modifications
  - Modification de la config pour démarrer le navigateur utilisé pour les tests en mode headless
    - Voir les exemples de configuration
      - http://www.protractortest.org/#/browser-setup#using-headless-chrome
      - http://www.protractortest.org/#/browser-setup#using-headless-firefox
  - Ajout de Travis comme système de CI
    - Ajout du fichier /.travis.yml/ contenant la configuration requise pour exécuter ces tests
    - Par défaut, si Travis détecte un fichier /yarn.lock/, il utilise *Yarn* comme gestionnaire de dépendances
    - Dans notre cas, on avait un fichier /yarn.lock/ mais qui n'était pas utilisé et donc pas à jour
    - Des hooks permettant de build le projet n'étaient pas non plus déclenchés par *Yarn*
    - On obtenait l'erreur suivante en résultat
      #+BEGIN_BLOCK
        ERROR in ./node_modules/css-loader?{"sourceMap":false,"import":false}!./node_modules/postcss-loader/lib?{"ident":"postcss","sourceMap":false}!./node_modules/sass-loader/lib/loader.js?{"sourceMap":false,"precision":8,"includePaths":["/home/travis/build/coast-team/mute/src/assets"]}!./src/styles.scss
        Module build failed: Error: Can't resolve '~normalize.css/normalize.css' in '/home/travis/build/coast-team/mute/src'
          at onError (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:61:15)
          at loggingCallbackWrapper (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/createInnerCallback.js:31:19)
          at runAfter (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:158:4)
          at innerCallback (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:146:3)
          at loggingCallbackWrapper (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/createInnerCallback.js:31:19)
          at next (/home/travis/build/coast-team/mute/node_modules/tapable/lib/Tapable.js:252:11)
          at /home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/UnsafeCachePlugin.js:40:4
          at loggingCallbackWrapper (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/createInnerCallback.js:31:19)
          at runAfter (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:158:4)
          at innerCallback (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:146:3)
          at loggingCallbackWrapper (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/createInnerCallback.js:31:19)
          at next (/home/travis/build/coast-team/mute/node_modules/tapable/lib/Tapable.js:252:11)
          at innerCallback (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/Resolver.js:144:11)
          at loggingCallbackWrapper (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/createInnerCallback.js:31:19)
          at next (/home/travis/build/coast-team/mute/node_modules/tapable/lib/Tapable.js:249:35)
          at resolver.doResolve.createInnerCallback (/home/travis/build/coast-team/mute/node_modules/enhanced-resolve/lib/DescriptionFilePlugin.js:44:6)
          @ ./src/styles.scss 4:14-191
          @ multi ./src/styles.scss
      #+END_BLOCK
    - Suppression du fichier /yarn.lock/
    - On arrivait toujours pas à exécuter les tests E2E
      - On obtenait les erreurs suivantes
	- /E/runner - Unable to start a WebDriver session./
	- /E/launcher - Error: WebDriverError: unknown error: Chrome failed to start: crashed/
    - Afin de mieux diagnostiquer ces erreurs, utilisation des images Docker de Travis pour effectuer des tests
      - Les images Docker sont disponibles ici : https://docs.travis-ci.com/user/common-build-problems/#Troubleshooting-Locally-in-a-Docker-Image
    - Réalisation de tests en local
      - L'erreur était dûe à l'utilisation de Chrome au sein d'un conteneur
      - En lançant Chrome, on obtient l'erreur suivante
	- /Failed to move to new namespace: PID namespaces supported, Network namespace supported, but failed: errno = Operation not permitted/
      - Cette erreur m'a permis de trouver la page suivante
	- https://hub.docker.com/r/armbues/chrome-headless/
      - Bien tenté d'utiliser le profil de sécurité indiqué, mais sans succès
      - Ajout de l'option /--no-sandbox/ pour le moment
    - Redéploiement des tests sur Travis
    - Cette fois-ci, on démarre bien un navigateur et on le manipule pour effectuer les tests E2E
    - Mais le résultat n'est pas déterministe
      - Le ou les tests qui échouent ne sont pas les mêmes après plusieurs exécutions du même job
    - Test avec Firefox au lieu de Chrome
      - Le *GeckoDriver* n'a pas l'air de fonctionner avec la version de *Protractor* du projet
	- Voir https://github.com/angular/protractor/issues/4253
      - Possible de mettre à jour *Protractor*
      - Mais on obtient une nouvelle erreur
	- POST /session/ac0e051d-adc7-8247-b983-635cad11933d/keys did not match a known command
      - Celle-ci semble lié à *WebDriver* et *Selenium*
- MUTE : Supprimer la boucle de rafraîchissement infinie de Angular
  - Ajout de tests où on se contente d'ouvrir Mute dans le navigateur et d'appeler /waitForAngular()/
  - Ce test fonctionne sur la page /docs/
  - Ce test échoue sur la page /doc/
  - La boucle de rafraîchissement est donc dûe à un des *Components* ou *Services*
  - Voir si on peut mocker les composants pour essayer d'identifier plus précisement l'origine de cette boucle
- MUTE : Organiser une session de test sur mute-structs@0.4.0
  - Malgré quelques problèmes de connectivité, a quand même pu collaborer et récupérer des logs
  - Des divergences ont encore eu lieu
  - Dûes semblerait aux problèmes de causalité
  - N'ayant pas détecter d'autres erreurs, a mergé le modèle
- Réplication et cohérence de données : Cours 2
  - Exemple de Not Strong Eventual Consistency
    - Une fois toutes les opérations délivrées, une copie contacte un tier (autre copie ou arbitre) pour résoudre certains conflits
  - Remarques
    - Slide 14 : Une légende des schémas peut être utile
    - Slide 15 : C(b) = 40.2 (et pas C(a))
    - Slide 30 : On appelle ça un ordre lexicographique ?
    - Slide 35 : Tu peux pas simplifier le schéma avec juste 2 sites ?
- Meeting
  - Crash test session
    - Thanks again for your participation
    - Was able to gather some test samples
    - Did not find any other errors that the causality-related ones
    - So was finally able to release the new model
    - PLEASE EXPORT YOUR MUTE DOCUMENTS since the new model is not backward compatible
  - Related to the causality issue
    - Victorien proposed a simple way to add a lightweight causality tracking mechanism to MUTE
    - Implemented it
  - While i was still on MUTE
    - Worked on adding E2E tests using Protractor (automatically open a browser and interact with MUTE and check that a feature is still working)
    - Implement a few of them
    - Tricky part was to run them on the CI tool Travis (weird behavior of browser in container)
    - Unfortunately, tests are not deterministic
    - No way currently to wait for asynchronous task to end before moving to the next one
    - So wait using timeout
    - Works great on dev environment, not so much on Travis
*** Planned
**** DONE MUTE : Mettre en place *Protractor* pour réaliser des tests E2E
- *Protractor* permet d'effectuer des tests E2E
- On pourrait l'utiliser afin d'automatiser certains tests que nous faisons actuellement manuellement (connexion des pairs, broadcast des opérations, stockage du log)
- Voir pour setup l'outil
**** DONE MUTE : Organiser une session de test sur mute-structs@0.4.0
- Afin de pouvoir implémenter le mécanisme de renommage sur de bonnes bases, il est nécessaire de tester la validité de cette branche
**** DONE MUTE : Enrichir *SyncService* pour assurer une livraison causale
- Disclaimer: algo de Victorien
- Algorithme
  - Ce mécanisme suppose qu'on a déjà une livraison FIFO
  - On pourrait, au moment où l'on supprime un élément, ajouter à l'opération de suppression /op2/ un couple <replicaNumber, networkClock>
  - Ce couple serait utilisé pour représenter la dernière opération /op1/ que l'on a reçu du pair dont on supprime un bloc
  - En recevant /op2/, il faudrait vérifier si on a déjà délivré /op1/
    - On peut vérifier cela simplement en checkant le *StateVector*
  - Si on l'a déjà reçue et délivrée, on peut délivrer /op2/
  - Sinon, on met l'opération en attente
- Remarques
  - Délivre pas les messages de façon optimale
  - Puisque on va potentiellement faire attendre la livraison d'une opération qui n'a aucun lien avec l'opération de suppression
  - Mais au moins assure une livraison causale des suppressions
** Semaine du <2018-01-22 Mon> au <2018-01-26 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Implémentation de /generateRenamingSequence()/
  - Implémentation de /renameLocal()/
  - Implémentation des tests correspondants
    - Ajout de /generateStateA()/, /generateStateB()/ et de /generateStateC()/
    - A correspond au scénario où 2 utilisateurs écrivent à tour de rôle
    - B correspond au scénario où 1 utilisateur insère au sein de son dernier bloc de façon répétée
    - C correspond au scénario où 2 utilisateurs écrivent à tour de rôle mais avec des suppressions
    - Renvoie l'état du document, mais aussi la séquence de *DottedInterval* correspondante
  - Implémentation de /recomputeMapBlocks()/
- Réplication et cohérence de données : Cours 3
  - Slide 57 : besoin d'un ordre causal nécessairement ? On parle pas plutôt d'un ordre partiel de livraison ? (Potentiellement FIFO donc)
  - Slide 69 : pourquoi la version optimisé de LogootSplit ?
*** Planned
** Semaine du <2018-01-29 Mon> au <2018-02-02 Fri>
*** Done
- MUTE : Supprimer la boucle de rafraîchissement infinie de Angular
  - Philippe a réussi à trouver et à patcher l'origine de la boucle
- MUTE : Implémenter le mécanisme de renommage
- MUTE : Intégration des tests E2E
  - La méthode /waitForAngular()/ fonctionne donc désormais correctement
  - Ré-écriture des tests E2E pour l'utiliser plutôt que /sleep()/
  - Mais les tests ne sont toujours pas déterministes dans l'environnement Travis
  - /waitForAngular()/ retourne avant que le stockage ait eu lieu
  - Pour simplifier l'implémentation des tests liés au stockage, devrait émettre un évènement lorsqu'une nouvelle version du document est stocké
  - En attendant, re-utilise /sleep()/
  - Obtient des erreurs dont je ne comprends pas l'origine
    #+BEGIN_SRC
      mute App should broadcast updates to peers
      Failed: unknown error: Cannot read property 'CodeMirror' of null
    #+END_SRC
  - Ceci intervient après /waitForAngular()/
  - Cela signifierait que cette fonction retourne avant même que l'éditeur ne soit chargé
  - Si finalement /waitForAngular()/ n'attend pas qu'Angular ait fini de render l'application, je ne sais pas quoi faire
  - Laisse tomber personnellement les tests E2E pour le moment, perd trop de temps dessus
- DISTRIBUTED SYSTEMS : Se documenter sur IPFS
  - Whitepaper disponible à https://github.com/ipfs/papers/blob/master/ipfs-cap2pfs/ipfs-p2p-file-system.pdf
  - InterPlanetary File System
  - Un système de fichiers distribué
  - Adresse des problèmes similaires à nos travaux
    - Partition réseau
    - Disparition du endpoint
    - Censure
  - Vise à remplacer HTTP par un protocole plus puissant
  - Identifie dorénavant les données grâce à leur contenu
  - Overview
    - Une application web cherche à utiliser une ressource
    - Un service de nommage IPNS permet d'obtenir à partir du nom human-readable de la ressource son identifiant
    - L'identifiant de la ressource permet de récupérer son Merkle DAG
    - Le Merkle DAG permet d'identifier la version de la ressource à récupérer
    - La version de la ressource est échangée en utilisant BitTorrent
    - Des DHTs sont utilisées pour déterminer les noeuds disposant de la ressource
      - http://www.scs.stanford.edu/%7Edm/home/papers/kpos.pdf
  - Une ressource est donc répliquée chez les noeuds qui sont intéressés par celle-ci
  - Permet facilement d'assurer la disponibilité de sites statiques par exemple
  - Mais encore du mal à imaginer le fonctionnement avec des données dynamiques
  - Une discussion a justement eu lieu concernant ce type de données qui a conduit à une étude des CRDTs
  - Ressources utiles
    - Discussion initiale: https://github.com/ipfs/notes/issues/40
    - Notes sur les CRDTs: https://github.com/ipfs/research-CRDT
    - Pourquoi utiliser des CRDTs avec IPFS: https://www.youtube.com/watch?v=2VOF-Z-nLnQ
    - Construire un éditeur collaboratif reposant sur IPFS: https://www.youtube.com/watch?v=-kdx8rJd8rQ
    - Éditeur collaboratif temps réel reposant sur IPFS: https://peerpad.net/
    - Librairie de réseau P2P: https://libp2p.io/
- MUTE : Ajouter des tests pour *SyncService*
  - Était déjà fait depuis quelques temps, mais n'avait pas marqué la tâche comme complétée
- PLM
  - Annulation des tâches liées à PLM qui traînaient depuis des années
*** Planned
**** CANCELLED MUTE : Intégration des tests E2E
- Maintenant que la boucle Angular est supprimée, on peut reprendre les tests E2E pour les rendre déterministes
**** CANCELLED PLM : Gérer les exercices n'ayant pas d'entité solution dans le langage de programmation par défaut
- Actuellement, on considère que l'ensemble des exercices possèdent une entité solution en Java
- Lors de leur instanciation à partir des sources, on essaie de calculer le(s) monde(s) objectif(s) à partir de cette entité solution
- Ajouter un mécanisme de fallback vers un autre langage si l'entité solution Java n'est pas trouvée
- Déclencher une erreur si aucune entité solution n'est trouvée
**** CANCELLED PLM : Ajouter des tests d'intégration pour vérifier la sérialisation JSON
- Semblerait que la sérialisation JSON foire de temps en temps
  - Le JSON généré ne contient pas les opérations de la solution
- Implémenter des tests d'intégration
  - Vérifier si la sérialisation JSON existe
  - Vérifier si la sérialisation JSON peut être désérialiser correctement
  - Vérifier si l'instance obtenue possède des opérations solutions
  - Vérifier si les opérations solutions permettent bien d'atteindre l'état objectif ?
    - Je ne suis pas sûr qu'on puisse rejouer les opérations côté serveur actuellement
**** CANCELLED PLM : Corriger l'exercice Polygon360
- L'exercice Polygon360 ne peut pas être résolu actuellement
- Lorsqu'on soumet le code de la correction, on n'arrive pas à atteindre le monde objectif
  #+BEGIN_SRC
  Le monde 'Polygon360' diffère: x1 diffère. (trouvé Line (x263.445 y135.028 / x264.177 y149.000 / black) au lieu de Line (x263.968 y154.996 / x264.177 y149.000 / black) )
  #+END_SRC
- L'erreur a été reportée ici : https://github.com/BuggleInc/webPLM/issues/130
**** CANCELLED PLM : Trouver l'origine du crash de webPLM du <2016-10-12 mer.>
- L'application a de nouveau eu un crash le <2016-10-12 mer.>, de 17:54 jusqu'à 18h:26
- Il a fallu que je redémarre manuellement le container de webPLM
  - Pour une raison inconnue, il n'arrivait pas à redémarrer correctement
  - Suppression du container et déploiement d'un nouveau
- Voici les métriques récupérées par New Relic au moment du crash
  #+CAPTION: Informations sur les transactions web au moment du crash
  #+NAME:   fig:transactions.png
  [[file:img/crash-webplm-2016-10-12T17:54/transactions.png]]
  #+CAPTION: Informations sur les ressources utilisées par le serveur au moment du crash
  #+NAME:   fig:server.png
  [[file:img/crash-webplm-2016-10-12T17:54/server.png]]
  #+CAPTION: Informations sur les ressources utilisées par les différents processus au moment du crash
  #+NAME:   fig:processes.png
  file:img/crash-webplm-2016-10-12T17:54/processes.png
  #+CAPTION: Informations sur les ressources utilisées par les différents containers au moment du crash
  #+NAME:   fig:dockers.png
  [[file:img/crash-webplm-2016-10-12T17:54/dockers.png]]
- Les logs ne montrent aucune erreur expliquant le crash, seulement des requêtes d'exécution
  #+BEGIN_SRC
  2016-10-12 15:53:11 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8700 - Received a message
  2016-10-12 15:53:11 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8700 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo", "exerciseID":"recursion.logo.koch.Koch","code":"def snowFlake (levels:Int, length:Double) {\n    snowSide(levels, length);\n    right(120);\n    setColor( Color.blue);\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.orange);\n    snowSide(levels, length);\n    right(120);\n}\ndef snowSide(levels:Int, length:Double) {\n  if(levels==0){\n    return;\n  }else{\n    avance(length);\n    snowSide(levels-1,length/3);\n  }\n}"}}
  2016-10-12 15:53:11 +0000 [ERROR] from application in ForkJoinPool-3-worker-15 - PLMActor: executionActor ? StartExecution timeout
  2016-10-12 15:53:12 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - Received a message
  2016-10-12 15:53:12 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - {"cmd":"getExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.HexaKoch"}}
  2016-10-12 15:53:15 +0000 [ERROR] from application in ForkJoinPool-3-worker-23 - PLMActor: executionActor ? StartExecution timeout
  2016-10-12 15:53:15 +0000 [ERROR] from application in ForkJoinPool-3-worker-29 - PLMActor: executionActor ? StartExecution timeout
  2016-10-12 15:53:17 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8651 - Received a message
  2016-10-12 15:53:17 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8651 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.spiral.Spiral","code":"var length : Int = 5\n\nbaisseCrayon()\ndef spiral(steps:Int, angle:Int, length:Int, increment:Int) {\n avance(length)\n  gauche(45)\n  spiral(0,8, length+3, increment+1)\n  \n}"}}
  2016-10-12 15:53:18 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8695 - Received a message
  2016-10-12 15:53:18 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8695 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.Koch","code":"def snowFlake (levels:Int, length:Double) {\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.blue);\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.orange);\n    snowSide(levels, length);\n    right(120);\n}\n\ndef snowSide(levels:Int, length:Double) \n{\n  \n  if(length == 0)\n  {\n    return;\n  }\n  \n  else\n  {\n    avance(200);\n  }\n  \n}"}}
  2016-10-12 15:53:22 +0000 [ERROR] from application in ForkJoinPool-3-worker-5 - PLMActor: executionActor ? StartExecution timeout
  2016-10-12 15:53:23 +0000 [ERROR] from application in ForkJoinPool-3-worker-1 - PLMActor: executionActor ? StartExecution timeout
  2016-10-12 15:53:23 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8692 - Received a message
  2016-10-12 15:53:23 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8692 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.SquareKoch","code":"void snowSquare (int levels, double length) {\n    squareSide(levels, length);\n    right(90);\n    setColor(Color.blue);\n    squareSide(levels, length);\n    right(90);\n    setColor(Color.orange);\n    squareSide(levels, length);\n    right(90);\n    setColor(Color.magenta);\n    squareSide(levels, length);\n    right(90);\n}\nvoid squareSide(int levels, double length) {\n  if(levels==0){avance(length);}\n  else{\n    squareSide(levels-1,length/2);\n    gauche(90);\n    squareSide(levels-1,length/2);\n    droite(90);\n    squareSide(levels-1,length/2);\n    droite(90);\n    squareSide(levels-1,length/2);\n    gauche(90);\n    squareSide(levels-1,length/2);\n  }\n}"}}
  2016-10-12 15:53:24 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8695 - Received a message
  2016-10-12 15:53:24 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8695 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.PentaKoch","code":"void pentaKoch(int levels, double length) {\n  if(levels==0){\n    avance(length);\n  }\n  else{\n  pentaKoch(levels-1,length*0.4);\n    left(180-(360/5);\n    pentaKoch(levels-1,length*0.4);\n    right(360/5);\n    pentaKoch(levels-1,length*0.4);\n    right(360/5);\n    pentaKoch(levels-1,length*0.4);\n    right(360/5);\n    pentaKoch(levels-1,length*0.4);\n    left(180-(360/5);\n    pentaKoch(levels-1,length*0.4);\n  }\n}"}}
  2016-10-12 15:53:25 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - Received a message
  2016-10-12 15:53:25 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.Koch","code":"def snowFlake (levels:Int, length:Double) {\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.blue);\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.orange);\n    snowSide(levels, length);\n    right(120);\n}\ndef snowSide(levels:Int, length:Double) {\n  avance(length)\n  snowSide(levels, length)\n  right(120)\n  snowSide(levels, length)\n  right(120)\n  snowSide(levels, length)\n}"}}
  2016-10-12 15:53:27 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - Received a message
  2016-10-12 15:53:27 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - {"cmd":"runExercise","args":{"lessonID":"recursion.logo","exerciseID":"recursion.logo.koch.Koch","code":"def snowFlake (levels:Int, length:Double) {\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.blue);\n    snowSide(levels, length);\n    right(120);\n    setColor(Color.orange);\n    snowSide(levels, length);\n    right(120);\n}\ndef snowSide(levels:Int, length:Double) {\n  if(levels==0){\n    return;\n  }else{\n    avance(length);\n    snowSide(levels-1,length/3);\n  }\n}"}}
  2016-10-12 15:53:30 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - Received a message
  2016-10-12 15:53:30 +0000 [DEBUG] from application in application-akka.actor.default-dispatcher-8689 - {"cmd":"stopExecution","args":{}}
  2016-10-12 16:27:11 +0000 [INFO] from application in application-akka.actor.default-dispatcher-2 - Execution Mode: TRIBUNAL
  #+END_SRC
**** DONE MUTE : Supprimer la boucle de rafraîchissement infinie de Angular
- À l'heure actuelle, Angular rafraîchit de façon infinie l'interface, même si aucun évènement n'a lieu
- *Protractor* permet d'attendre que le rafraîchissement s'interrompe avant de déclencher les instructions suivantes
- Puisqu'il ne s'interrompt jamais, on ne peut pas utiliser cette méthode
- On utilise donc des /sleep(duration)/ en attendant
- Mais rien ne nous garantit que l'évènement a bien lieu avant la fin du /sleep()/
- Afin de rendre le comportement de nos tests déterministes, il est nécessaire de supprimer ce cycle de rafraîchissement infini
**** DONE DISTRIBUTED SYSTEMS : Se documenter sur IPFS
- https://ipfs.io/
**** DONE MUTE : Ajouter des tests pour *SyncService*
- *SyncService* est un composant critique de MUTE
- Écrire des tests pour *SyncService* permettrait:
  - S'assurer de son bon fonctionnement
  - Prévenir toute régression
- Il n'est cependant pas facile à tester
- Besoin de le refactorer ?
** Semaine du <2018-02-05 Mon> au <2018-02-09 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Ajout de la fonction /completeRenamingMap()/
    - Génère la map de renommage pour tous les noeuds du modèle
      - Le pair ayant déclenché le renommage l'a effectué sur un état donné
      - Mais je peux avoir d'autres noeuds dans mon modèle
      - Il faut aussi renommer de façon déterministe ces noeuds
    - Prend la map de renommage globale et la liste des insertions concurrentes en paramètre
    - Extrait la liste des intervals d'identifiants de la map de renommage globale
    - Pour chaque insertion concurrente
      - Récupère l'identifiant précédent avec /findPrevious()/
      - Renomme l'identifiant par rapport au précédent avec /renameIdentifier()/
      - Génère l'interval d'identifiants correspondant au nouvel ident
  - Ajout de la fonction /findPrevious()/
    - Prend en paramètre un identifiant et une liste d'intervals d'identifiants
    - Trouve l'identifiant dans la liste d'intervals qui précéde le noeud inséré
    - Soit on insère un nouveau noeud entre 2 autres (on considère qu'il y a un bloc minimum)
      - Dans ce cas, l'identifiant précédent est le dernier identifiant du noeud précédent
    - Soit on insère un nouveau noeud en splittant un autre
      - Dans ce cas, l'identifiant précédent est un préfixe de l'identifiant courant
      - On peut le récupérer en tronquant l'identifiant courant
    - Pas de garantie sur l'ordre des intervals dans la liste
      - Peut pas interrompre la boucle de recherche si le noeud est inséré entre 2 autres
      - Par contre, dans le cas d'un split, on peut interrompre la boucle de recherche de façon safe
	- Un noeud ne peut pas être contenu dans 2 autres noeuds à la fois
    - Renvoie l'identifiant précédent ainsi que l'interval le contenant
- CRDT : Lire *A Study of CRDTs that do Computations*
*** Planned
**** DONE CRDT : Lire *A Study of CRDTs that do Computations*
- Disponible ici : https://dl.acm.org/citation.cfm?id=2745948
** Semaine du <2018-02-12 Mon> au <2018-02-16 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - A trouvé une erreur dans l'algorithme de /renameRemote()/
    - On regénère /renamingMap/ telle qu'elle a été générée par le site ayant déclenché le renommage à partir de l'état courant et des opérations de suppressions concurrentes
    - On génère une nouvelle map /completedRenamingMap/ à partir de /renamingMap/ et des opérations d'insertion concurrentes
    - On génère un nouvel état vide
    - On parcourt chaque noeud de l'ancien état et pour chacun
      1. On extrait l'interval d'identifiants du noeud
      2. On récupère les éléments correspondants
      3. On cherche dans /renamingMap/ le nouvel interval d'identifiants correspondants
      4. On insère dans le nouvel état un nouveau noeud généré à partir de ces données
    - Mais l'étape 2 peut échouer
      - Si le noeud a été modifié en concurrence (append, preprend, split ou suppression partielle)
      - Dans ce cas, l'interval d'identifiants du noeud ne correspond plus à celui d'enregistré dans /completedRenamingMap/
      - Par exemple, on cherche à renommer un noeud correspondant à l'interval /a[0,6]/ alors que dans /completedRenamingMap/ on a les entrées /a[0,5]/ et /a[6,6]/
	- Dû à une insertion en concurrence de /a6/
      - En utilisant simplement /a[0,6]/ en clé, on n'obtiendra pas de résultats
    - L'algo doit donc être corrigé
    - Plusieurs possibilités
      - [ ] Modifier la façon dont est générée /completedRenamingMap/
	- "Appliquer" les opérations concurrentes sur /renamingMap/
	- Permettrait d'obtenir une /completedRenamingMap/ qui colle exactement à notre état
	- Cependant, ça impliquerait d'implémenter la logique de manipulation d'intervals dans *RenamingMap*
	  - Insérer un nouveau bloc => Ajouter une entrée dans la map
	  - Append/Prepend => Modifier une entrée dans la map
	  - Split un bloc => Supprimer l'entrée correspondante et insérer les entrées correspondantes aux 2 morceaux du blocs résultant du split ainsi que le split
	  - Supprimer un bloc => Supprimer l'entrée correspondante
	  - Supprimer un morceau de bloc => Réduire la taille de l'interval d'une entrée
	- Ceci serait lourd et error-prone à mon sens
      - [ ] "Undo" les opérations concurrentes puis rejouer leur transformation
	- L'algorithme consisterait à
	  1. Revenir à l'état sur lequel a été effectué le renommage
	  2. Appliquer l'opération de renommage
	     - Permet de réutiliser le code de /renameLocal()/
	  3. Appliquer les opérations concurrentes
	     - Permet de réutiliser le code utilisé pour appliquer les opérations concurrentes à un renommage après qu'on ait appliqué le renommage
	- Pour l'étape 1, il faut donc "undo" les opérations concurrentes qui ont été appliquées à la copie locale
	  - "Undo" une insertion consiste simplement à supprimer l'interval inséré
	  - "Undo" une suppression est plus problématique
	    - Il s'agit d'insérer de nouveau l'interval supprimé, mais aussi son contenu
	    - Dans notre cas, nous n'avons plus accès au contenu
	    - Mais comme on va de toute façon re-supprimer l'interval en réappliquant l'opération ensuite, on peut se contenter d'insérer des caractères aléatoires
        - Cette solution me paraît intéressante car elle permet de factoriser du code et reste simple
	- Mais "undo" les opérations est une étape dont j'aimerai bien me passer
      - [X] Générer un état correspondant mais avec des éléments factices et remplacer ses éléments par ceux de l'état original
	- Au cours du renommage, on ne modifie que la structure de données (les blocs *RopesNodes*), pas ses éléments (/doc.str/)
	- Mais les deux sont intraséquement liés par design, on ne peut pas modifier la structure sans modifier les éléments
	  - Ce qui est pertinent d'un point de vue conceptuel
	  - Mais qui devient gênant dans le cas présent
	- Pour cela qu'on "undo" les opérations concurrentes dans la solution précédente, afin de revenir à un état où structure de données et éléments concordent
	- On peut cependant éviter de "undo" les opérations en utilisant des éléments factices
	- L'algorithme est le suivant
	  1. Générer un état vide
	  2. Appliquer une opération d'insertion du bloc résultant du renommage, avec des éléments factices
	  3. Appliquer les opérations concurrentes
	  4. Remplacer les éléments de la structure de données obtenue par ceux de l'ancien état
	- Offre de meilleures performances que la stratégie "undo"
	  - Au lieu d'appliquer /k/ opérations concurrentes nécessitant de parcourir la structure de données en /O(log(n))/
	  - On a juste besoin de remplacer /newDoc.str/ par /doc.str/
	  - Si l'implémentation évolue et qu'on répartit les éléments dans les blocs, on aura juste besoin de parcourir la structure de données complète en /O(n)/ pour remplacer les éléments
	- Cette solution permet d'améliorer les performances de l'opération de renommage tout en restant simple
- CRDT : Lire Legion
  - Approche intéressante
  - Plutôt que de construire un nouveau système entièrement distribué reposant sur des CRDTs, du P2P
  - Ils prennent un système centralisé, Google Realtime API
    - https://developers.google.com/google-apps/realtime/overview
  - Et l'améliore en déléguant une partie de ses responsabilités aux utilisateurs
  - Google Realtime API
    - Propose de manipuler une structure de données répliquées
    - Utilise un serveur centralisé
    - Lorsqu'un utilisateur effectue une modification, l'applique à sa copie puis l'envoie au serveur
    - Le serveur, en utilisant OT, transforme l'opération par rapport à sa copie et partage la modification aux autres participants
  - Legion
    - Override Google Realtime API
    - Plutôt que de manipuler la structure de données initiale, manipule un CRDT
    - Met en place et utilise un réseau P2P pour partager les modifications
    - Un pair est élu leader et se charge de mettre à jour la copie du serveur
      - À partir des changements effectués sur le CRDT depuis la dernière synchro, génère des opérations "standards" de Google Realtime API
      - À partir des changements entre la copie du serveur et sa version précédente, génère des opérations correspondantes pour le CRDT
    - Permet à des clients utilisant uniquement Google Realtime API de toujours participer à la collaboration
  - Permet d'améliorer les performances du système
    - Meilleure vitesse de propagation des changements
      - Puisqu'ils sont partagés directement entre utilisateurs
      - Plutôt que de passer par le serveur à chaque fois
    - Meilleure disponibilité
      - Si le serveur plante, les utilisateurs peuvent toujours collaborer
  - Tout en limitant l'effort d'adoption
    - Seulement 2 lignes de code à modifier semblerait pour utiliser Legion dans une application utilisant Google Realtime API à la base
      - Import du script de Legion
      - Utilisation du constructeur de Legion au lieu du constructeur de Google Realtime API
    - Collaboration possible avec les "Legacy clients"
  - Mais le serveur représente toujours un /Single Point Of Failure/
    - Se charge d'authentifier les utilisateurs
    - A la responsabilité des droits d'accès à la ressource
    - Joue le rôle de /signaling/
    - Conserve la clé de chiffrement symétrique des données au sein de la structure
  - Il est donc nécessaire de faire confiance au serveur avec cette approche
- CRDT : Lire *Consistency without concurrency control in large, dynamic systems*
  - Se base sur *Treedoc*
  - Problème adressé
    - Dans *TreeDoc*, le TID d'un élément (son identifiant) représente un chemin dans l'arbre
    - Au fur et à mesure que l'on insère des éléments dans l'arbre, sa hauteur augmente et donc la taille des identifiants
    - Mais supprimer des éléments ne réduit pas la hauteur de l'arbre, notamment dans le cas où l'élément supprimé est un noeud intermédiaire
  - Solution proposée
    - L'idée est de rebalancer l'arbre afin d'éliminer les "trous" dans celui-ci
    - La difficulté consiste à rebalancer l'arbre de façon uniforme sur toutes les copies
    - Ainsi que de gérer les opérations concurrentes à un rebalancement
    - Distingue 2 types de sites : les sites appartenant au /core/ et ceux appartenant à la /nebula/
    - Les sites appartenant au /core/ sont en charge d'effectuer le rebalancing
    - Permet de limiter le mécanisme lourd du rebalancing à un nombre restreint de sites stables grâce au /core/
    - Tout en permettant une collaboration dynamique à grande échelle grâce à la /nebula/
  - Rebalancing de l'arbre par les sites /core/
    - Doivent se mettre d'accord sur quand/quel état déclencher un rebalancing
    - Pour cela, utilisent un 2PC
    - Si un site /core/ reçoit une opération concurrente durant le protocole de consensus, celui-ci échoue
      - Priorité des opérations concurrentes
    - Chaque rebalancing fait évoluer l'/epoch/ de l'arbre
    - L'/epoch/ permet d'identifier le contexte dans lequel une opération a été générée
    - Une fois qu'un rebalancing a été effectué, les sites de la /nebula/ doivent exécuter localement le rebalancing sur leur copie
  - Renommage des opérations concurrentes au rebalancing par les sites /nebula/
    - Identifie les noeuds qui ont été rebalancés
    - Génère l'arbre balancé à partir de ces noeuds
    - Attache les noeuds qui ont été ajoutés en concurrence à leur parent précédent
    - Identifie le nouveau chemin menant au noeud concurrent
  - Remarques
    - Le rebalançing de l'arbre permet effectivement de garbage collecter la structure de données et de réduire la taille des identifiants utilisés
    - Mais il nécessite dans ce système un consensus entre les sites du /core/ pour être effectué
    - Voir si on arrive à faire sauter cette limitation et à proposer un mécanisme entièrement distribué
  - Questions
    - Est-ce qu'adapter ce mécanisme à *LogootSplit* est une contribution suffisante ?
    - Ou faut-il améliorer l'existant ?
*** Planned
**** DONE CRDT : Lire Legion
- Legion propose un système permettant de manipuler une variété de structures de données de manière décentralisée
- Voir https://legion.di.fct.unl.pt/
- Papier disponible ici : https://dl.acm.org/citation.cfm?id=3052673
**** DONE CRDT : Lire *Consistency without concurrency control in large, dynamic systems*
- Claudia m'a indiqué que le mécanisme de renommage lui rappelait ces travaux
- Présentation disponible ici : https://www.cs.cornell.edu/projects/ladis2009/talks/shapiro-talk-ladis2009.pdf
- Papier disponible là : https://www.cs.cornell.edu/projects/ladis2009/papers/letia-ladis2009.pdf
** Semaine du <2018-02-19 Mon> au <2018-02-23 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Suite à la lecture de *Consistency without concurrency control in large, dynamic systems*, il est nécessaire de revoir les objectifs de cette implémentation
  - La principale question est jusqu'où aller dans une 1ère étape ?
    - Proposer une adaptation du mécanisme pour *LogootSplit* ?
    - Proposer une amélioration du mécanisme en le rendant concurrence-safe ?
  - Si on se limite à une adaptation du mécanisme pour *LogootSplit*
    - Ne devrait-on pas implémenter un mécanisme d'élection de leader pour l'expérimentation ?
      - Afin de proposer une évaluation la plus rigoureuse possible
    - Rappel du FLP theorem: le consensus n'est pas atteignable dans un système avec réseau asynchrone et où les noeuds peuvent tomber en panne
    - Il s'agit d'une hypothèse forte et qu'on va être incapable de reproduire en milieu expérimental
  - On va plutôt réfléchir à comment rendre commutative l'opération de renommage avec elle-même
  - Finalement, il s'agit d'un cas particulier de OT
    - Ici, nous avons un système avec 3 opérations
      - Insertion
      - Suppression
      - Renommage
    - Seul l'opération de renommage modifie le contexte réellement
    - Donc seules les opérations concurrentes à une opération de renommage doivent être transformées
    - Transformer une opération de renommage /op1/ concurrente par rapport à une autre /op2/ consiste à
      - Ne rien faire si /op2/ a la priorité sur /op1/
	- L'opération de renommage n'a pas d'effet sur les éléments de la structure de données
	- On peut donc considérer qu'aucune intention de l'utilisateur n'est associée à cette opération
	- On peut donc ignorer ce type d'opération dans certains cas tout en continuant de respecter l'intention de l'utilisateur
      - Appliquer /op1/ sinon
	- Recréer l'état obtenu après le renommage
	- Rejouer, en les transformant, les opérations d'insertion et de suppression concurrentes
	- Mapper le contenu de l'ancien état au nouveau
    - Transformer une opération d'insertion ou de suppression /op1/ par rapport à une opération de renommage /op2/ dépend de son contexte de génération
      - Si le contexte de génération de /op1/ est inférieur au contexte résultant de l'application de /op2/
	- Alors il suffit de transformer l'opération comme décrit précédemment dans ce journal
      - Sinon, cela signifie que le contexte de génération de /op1/ est concurrent au contexte résultant de l'application de /op2/
	- Auquel cas, l'idée est d'utiliser des /backward/forward transformations/
	- On recherche le contexte ancêtre commun de /op1/ et de /op2/
	- On transforme en arrière /op1/ pour obtenir une opération équivalente /op1'/ applicable dans ce contexte ancêtre
	- Puis on applique /op1'/ simplement en la considérant comme une opération concurrente à /op2/
	- Là encore, on va utiliser la /renamingMap/ qui a servi à passer du contexte ancêtre au contexte de /op1/ pour calculer la transformée arrière
	  - Juste qu'on va l'utiliser en sens inverse
- IPFS : Lire *Paper review. IPFS: Content addressed, versioned, P2P file system*
  - Argumente que le problème que IPFS adresse, la bande passante limitée, n'est pas un problème
    - D'après lui, les CDNs existent pour résoudre le problème
    - Sauf que les CDNs ne réduisent pas la bande passante consommée, ils la répartissent juste
      - Plutôt que d'utiliser ma bande passante pour fournir une ressource, je vais utiliser celle des différents serveurs du CDN
    - De plus, c'est le /host/ de la donnée qui a la charge d'utiliser sur un CDN pour répliquer la ressource
      - Je peux tout à fait refuser d'utiliser un CDN pour fournir les ressources statiques de mon application
    - Et c'est ensuite au CDN d'en assurer la disponiblité
      - Donc finalement on déplace la complexité de mon application au CDN
    - Avec IPFS, chaque consommateur d'une ressource peut en plus héberger et fournir cette donnée par la suite
      - Ceci peut s'additionner à un CDN utilisé initialement pour fournir la ressource
  - Donc considère que IPFS complique juste le système inutilement
    - Effectivement, construire un système entièrement distribué peut s'avérer plus complexe que de construire son équivalent centralisé
  - Argumente que des systèmes centralisés qui scalent et qui sont hautement disponibles existent
    - Dropbox, Facebook
    - Selon moi, il ne faut toutefois pas oublier la complexité de ces systèmes et leurs coûts de fonctionnement
  - De mon point de vue, son analyse manque la cible
    - Un des intérêts de IPFS est de changer le mode d'adressage des données
      - On passe d'un adressage /host-based/ à un adressage /content-based/
      - Ceci permet d'éviter le phénomène de centralisation de l'Internet
      - On n'est alors plus dépendant d'un système tiers auquel on ne fait pas forcément confiance (privacy, disponbilité, ...) pour partager une donnée
      - Et on économise effectivement de la bande passante avec ce nouveau mode d'adressage
	- Pour partager une ressource dans un réseau local, il suffit qu'un seul des noeuds ait téléchargé la ressource
    - La réplication des données proposée par IPFS permet aussi le fonctionnement en /offline/
      - Puisque je possède la donnée, je n'ai pas besoin de passer par un service tiers pour y accéder
    - IPFS vise aussi à proposer un système qui met à disposition toutes les mécanismes de scalabilité et de disponibilité qu'il mentionne
      - Plutôt que chacun (ou du moins ceux qui ont les moyens) ne les réimplémente
      - L'idée est de les mettre à disposition de tous tout en cachant leur complexité
  - Une réponse d'un développeur de IPFS est disponible ici : https://news.ycombinator.com/item?id=16432502
  - Référence une bonne vidéo explicative sur les Merkle Trees
    - Disponible ici: https://www.youtube.com/watch?v=YIc6MNfv5iQ
*** Planned
**** DONE IPFS : Lire *Paper review. IPFS: Content addressed, versioned, P2P file system*
- Un blogger a rédigé un post sur son analyse de IPFS
- Peut être intéressant de lire son avis
- Disponible ici : https://muratbuffalo.blogspot.fr/2018/02/paper-review-ipfs-content-addressed.html
** Semaine du <2018-03-05 Mon> au <2018-03-09 Fri>
*** Done
- MUTE : Fix mécanisme de causalité
  - Le scénario permettant de générer ce bug est le suivant:
    - 2 collaborateurs, A et B, rédigent un document (insertions multiples)
    - Au bout d'un moment, A effectue une opération de suppression
    - A poursuit ensuite avec quelques opérations d'insertion
    - À ce moment, les 2 copies doivent converger
    - A se déconnecte
    - B recharge le document
    - B va alors obtenir l'état du document correspondant à toutes ses opérations et toutes les opérations de A précédant l'opération de suppression
  - Ce bug est dû à un problème de sérialisation/désérialisation
    - Le champ /dependencies/ de *RichLogootSOperation* est perdu au cours de la sérialisation/désérialisation
    - Dans le cas d'une opération *LogootSAdd*, cela ne pose pas de problème
    - Dans le cas d'une opération *LogootSDel*, l'opération n'est plus valide
  - L'opération désérialisée est ré-instanciée à 2 endroits du code
    - Dans *SyncMessageService*, lorsqu'on reçoit l'opération d'un autre pair
      - On regénère l'opération manuellement dans /deserializeRichLogootSOperation()/
      - Mais on ne traite pas le cas où l'opération n'est pas valide dans ce code
      - L'opération est donc délivrée correctement puis stockée
    - Dans *SyncStorageService*, lorsqu'on regénère le document à partir du log
      - Ici, on utilise /RichLogootSOperation.fromPlain()/ pour réinstancier l'opération
      - /RichLogootSOperation.fromPlain()/ détecte que l'opération est non-valide et renvoie alors la valeur *null*
      - *SyncStorageService* filtre les opérations *null* et ne délivre donc que les opérations valides
  - Il est intéressant de noter que ce bug affecte uniquement les opérations de suppression des autres utilisateurs
    - Les opérations d'insertion n'ont de toute façon pas de dépendances
    - Nos opérations de suppression sont stockées directement et ne perdent pas leur champ /dependencies/ au cours de la sérialisation/désérialisation
  - Ce bug serait donc similaire au cas où l'on ne stockerait pas les opérations de suppression générées par les autres utilisateurs
    - On ne peut dès lors plus rejouer les opérations suivantes de ces utilisateurs (delivery FIFO)
    - On ne peut potentiellement plus rejouer nos opérations si une d'entre elles est une opération de suppression dépendant d'une insertion distante bufferisée (delivery causal)
  - L'origine de ce problème provient de cette ligne : https://github.com/coast-team/mute/blob/a1ef5a94ddf8fd1bd741040a63ba8eb62f28ab88/src/app/doc/editor/cursor/cursor.proto#L2
    - Afin de réutiliser la configuration *Protobuf* écrite dans /mute-core/, on l'importe dans ce nouveau fichier
    - Cependant, importer dans *Protobuf* se traduit par regénérer le code correspondant à la configuration du package et l'exporter en potentiellement écrasant un objet existant
    - Ici, la configuration de /sync/ est plus récente que celle de /cursor/
      - J'ai regénéré les fichiers *Protobuf* dans /mute-core/, mais pas ceux dans /mute/
    - Ainsi, le code correspondant à la nouvelle configuration de /sync/ n'est pas pris en compte
    - Car il est écrasé par son ancienne version qui a été générée au préalable et incluse dans le code correspondant à la configuration de /cursor/
  - Plusieurs problèmes sont donc à corriger :
    - [X] Utiliser /RichLogootSOperation.fromPlain()/ dans /deserializeRichLogootSOperation()/
      - Permettrait de disposer d'un traitement uniforme des opérations, qu'elles soient reçues à l'instant d'un collaborateur ou qu'il s'agisse d'opérations provenant d'un log
      - Modification de la fonction pour utiliser /RichLogootSOperation.fromPlain()/
    - [X] Actualiser les fichiers *Protobuf* de /mute/
    - [X] Inclure l'actualisation des fichiers *Protobuf* dans le processus de build de /mute-core/ et de /mute/
      - Permettrait de s'assurer que les configurations *Protobuf* soient à jour
      - Ajout de la commande /npm run proto/ dans la phase /prebuild/ de /mute/ et de /mute-core/
- MUTE : Implémenter le mécanisme de renommage
  - L'algorithme pour transformer les opérations de suppression par rapport à un renommage était incorrect
    - Une opération de suppression peut supprimer des identifiants faisant parti du renommage
    - Mais aussi des identifiants qui ont été insérés en concurrence à l'opération de renommage
  - La marche à suivre dans ces 2 cas est différente
  - Pour les identifiants faisant parti du renommage
    - Pour déterminer quels identifiants sont concernés, il faut calculer l'intersection entre les identifiants supprimés et les identifiants renommés
    - Il suffit ensuite de trouver l'image de l'intersection dans /renamingMap/
  - Pour les identifiants insérés en concurrence
    - Pour déterminer quels identifiants sont concernés, il faut calculer la différence entre les identifiants supprimés et l'intersection calculée précédemment
    - Il faut ensuite calculer l'image de la différence dans le nouveau contexte
  - Ajout des fonctions /intersection()/ et /difference()/ dans *IdentifierInterval*
  - Modification de /findPredecessor()/ pour gérer le cas sans prédecesseur
    - Auparavant, initialisait /predecessorId/ et /predecessorIdInterval/ avec un des éléments de /renamedIntervals/
    - Mais potentiellement, aucun élément de /renamedIntervals/ n'est le prédécesseur de /id/
    - Dans ce cas, on pouvait retourner un /predecessorId/ tel que /predecessorId > id/, ce qui est incorrect
    - Dorénavant, /findPredecessor()/ retourne soit le couple /<predecessorId, predecessorIdInterval>/, soit /undefined/
  - Modification de /renameIdentifierInterval()/ pour gérer le cas sans prédecesseur
    - Puisque /findPredecessor()/ peut maintenant retourner la valeur /undefined/, il faut modifier /renameIdentifierInterval()/ en conséquence
    - Initialise /newPredecessorId/ avec /Id[-1, 0, 0, 0]/
      - Comme les éléments renommés commence à /Id[0, renamingReplicaNumber, renamingClock, 0]/, on est sûr que /newPredecessorId/ est plus petit que n'importe quel autre identifiant
    - Puis écrase cette valeur si /findPredecessor()/ a retourné un résultat
  - Étude d'une modification de /renameIdentifierInterval()/ pour limiter l'augmentation de la taille des splits effectués en concurrence d'un renommage
    - Dans le scénario suivant :
      #+CAPTION: Renommage d'un split concurrent
      #+NAME: fig:concurrent-split-renaming.jpg
      [[file:img/concurrent-split-renaming.jpg]]
      - Un site A effectue le renommage suivant : /a[0..5] -> a'[0..5]/
      - Un site B insére en concurrence du renommage l'identifiant /a1b0/
      - Lorsqu'on joue l'insertion de /a1b0/ sur le site A, on la transforme par rapport au renommage
      - On insère en finalité l'identifiant /a'1a1b0/
      - Mais si on insérait l'identifiant /a'1b0/, l'ordre obtenu serait identique
	#+CAPTION: Renommage optimisé d'un split concurrent
        #+NAME: fig:concurrent-split-better-renaming.jpg
        [[file:img/concurrent-split-better-renaming.jpg]]
      - De plus, /a1/ peut être en pratique un identifiant de taille conséquente
    - Le but est d'améliorer le mécanisme de renommage en omettant la particule qui a été renommé
    - L'algorithme est le suivant
      - Une fois qu'on a trouvé que /a1/ est le prédecesseur de /a1b0/
      - On vérifie si /a1/ est un préfixe de /a1b0/
      - Si c'est le cas
	- On tronque /a1b0/ pour ne récupérer que le reste: /b0/
      - On récupère l'image de /a1/ : /a'1/
      - On concatène /a'1/ et /b0/ et on obtient ainsi /a'1b0/
    - Mais un problème survient si le prédecesseur du split est aussi le dernier élément d'un interval renommé
      #+CAPTION: Contre-exemple de l'optimisation du renommage d'un split concurrent
      #+NAME: fig:counter-example-concurrent-split-better-renaming.jpg
      [[file:img/counter-example-concurrent-split-better-renaming.jpg]]
    - Il suffit juste de ne pas tronquer l'identifiant renommé dans ce cas
    - Pour expliciter le cas problématique :
    - Il ne faut pas que, suite à cette optimisation, on tombe dans un cas où l'on compare 2 tuples ayant une profondeur différente
      - On appelle profondeur d'un tuple sa position dans l'identifiant d'origine
      - On ne compare que des tuples de même profondeur
      - Ici, il faut s'assurer que l'optimisation ne va pas aboutir dans certains scénarios à une comparaison incohérente
      - C'est ce qu'il se passe notamment dans le contre-exemple
- IAEM : Réunion de rencontre doctorants 1A et Directeur ED IAEM
  - Comité de suivi de thèse
    - Rôle
      - Audition du doctorant conduisant la rédaction d'un rapport jugeant de la faisabilité de la thèse et donc de la capacité de réinscription du doctorant
    - Le rapport du comité de suivi doit être déposé par le doctorant sur ADUM
  - Formations
    - Doctoriales
      - Valide le quota de formations "professionnelles" à suivre
*** Planned
**** DONE MUTE : Fix mécanisme de causalité
- En effectuant des tests utilisateurs sur MUTE, on s'est aperçu que le mécanisme de causalité présente des problèmes
- On constate le bug en rechargeant un document
  - Au cours de l'initialisation du document à partir du log stocké, on n'arrive pas à rejouer toutes les opérations
  - Certaines opérations sont bufferisées car on n'aurait pas toutes les dépendances pour les jouer
  - On n'arrive donc pas à rejouer l'entièreté du log
  - Une synchronisation avec un autre pair peut débloquer la situation
  - Cependant, si ces opérations du log sont stockées dans cet ordre, cela veut dire qu'elles ont été délivrées dans cet ordre
  - Voir d'où provient ce bug
    - Est-ce qu'une opération n'est pas correctement loggée ?
    - Ou est-ce qu'une opération n'est pas détectée comme délivrable alors qu'elle est en vérité ?
** Semaine du <2018-03-12 Mon> au <2018-03-16 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Poursuite de l'étude de l'optimisation du renommage des splits concurrents
    - Y-a-t il un moyen d'assurer qu'on ne tombe pas dans le scénario problématique ?
    - Le raisonnement suivant suffit :
      - Le problème se produit lorsqu'on compare deux identifiants /id'1/ et /id'2/ entre eux afin de déterminer leur ordre avec
	- /id1/, identifiant généré au cours d'un split puis ensuite ré-écrit en /id'1/ de façon optimisé au cours du renommage
	  - Sa taille est donc inchangé dans le pire cas, réduite sinon
	- /id2/, identifiant généré au cours d'une insertion puis ensuite transformé en /id'2/ au cours du renommage
	  - Sa taille est donc augmenté de 1
      - Le simple fait qu'on ait pu effectuer l'optimisation implique que des identifiants /id3/ et /id4/ existent tels que /id3 < id1 < id4 < id2/
      - On a donc :
	- /id1/ étant un split, il se décompose en /id3 + x/ et se ré-écrit en /id'3 + x/
	- /id'2 = id'4 + id2/
      - Ainsi, /id'3 < id'1 < id'4 < id'2/
    - On peut donc bel et bien optimiser le renommage d'un split si celui-ci n'a pas eu lieu à la fin d'un interval renommé
  - Implémentation de l'optimisation du renommage des splits concurrents
    - Ajout d'une méthode /reverseTruncate()/ dans *Identifier*
    - Modification de /renameIdentifierInterval()/ pour tronquer la partie "renommée" dans split
  - Réflexion sur l'exécution d'une opération distante de renommage
    - Pour exécuter /renameRemote()/, j'ai besoin d'accéder à la liste des opérations concurrentes à l'opération de renommage
    - Pour pouvoir accéder à ce log, 2 approches sont possibles
      - *LogootSplit* maintient le log des opérations
      - On délègue la responsabilité de fournir le log, et donc de maintenir ce dernier, au composant qui délivre l'opération de renommage
    - Maintenir le log dans *LogootSplit*
      - Si on maintient le log dans *LogootSplit*, on a finalement que déplacé le problème
      - Auparavant on avait des identifiants qui grossissaient indéfiniment
      - Maintenant on a des tombstones sous la forme du log
      - Pour que la solution soit viable, il est nécessaire de garbage collecter les opérations
    - Garbage collection d'opérations
      - Dans *LogootSplit*, j'ai besoin de conserver une opération tant qu'un renommage en concurrence peut être générée
      - Dans le cas où seulement un pair peut effectuer le renommage
	- On peut garbage collecter une opération si la dernière opération de renommage dépend causalement sur celle-ci
      - Dans le cas où chaque pair peut déclencher un renommage
	- Je ne peux plus garbage collecter les opérations à moins de connaître avec certitude les membres du groupe
	- Si je connais la liste des membres du groupe
	  - Je peux garbage collecter une opération /op_i/ si je reçois de chaque membre une opération /op_k/ dépendant causalement d'une opération de renommage /op_j/ dominant /op_i/
	    - Possible d'obtenir cette information grâce à l'/epoch/ d'une opération
	- Si je ne connais pas les membres du groupe
	  - Il est toujours possible qu'un membre du groupe ait été invité sans que je ne le sache
	  - Il peut émettre des opérations concurrentes aux miennes et à celles des autres depuis un certain temps
	  - Il est donc possible qu'il émette un renommage qui soit concurrent et prioritaire par rapport à un renommage qui ayant été accepté par tous les autres collaborateurs
	- Scénario d'exemple :
	  #+CAPTION: Garbage collection rendant impossible un renommage concurrent
          #+NAME: fig:concurrent-renaming-incorrect-gc.jpg
	  [[file:img/concurrent-renaming-incorrect-gc.jpg]]
    - Déléguer la responsabilité
      - Je me retrouve dans ce cas avec un problème d'interface
      - La fonction /execute()/ de *LogootSOperation* prend en paramètre juste un *LogootSRopes* pour le moment
	- Puisque *LogootSAdd* et *LogootSDel* contiennent toutes les informations nécessaires à leur exécution
      - Dans le cas de *LogootSRename*, on se verrait contraint d'ajouter un paramètre supplémentaire, /log: LogootSOperation[]/
      - Comment spécifier une interface compatible ?
      - Possible de contourner le problème d'interface en incluant le log dans *LogootSRename* même
	- Lorsqu'on génère l'opération, on laisse ce champ /undefined/
	- Lorsqu'on reçoit une opération de renommage distante
	  - On la désérialise
	  - Puis on l'enrichit avec notre log d'opérations
	- Ça ne me paraît pas intuitif
	- Il faudrait ajouter des contraintes pour forcer l'enrichissement de l'opération avant de pouvoir l'appliquer
      - Autre possibilité, ajouter des interfaces *Transformable* et *Applicable*
	- Il existerait 2 classes pour l'opération de renommage
	- L'opération locale de renommage générerait une opération distante qui implémenterait *Transformable*
	- Cette opération serait partagée avec les autres collaborateurs
	- Et c'est en utilisant sa fonction /transform(doc: LogootSRopes, concurrentOps: LogootSOperation[])/ que les collaborateurs obtiendraient une opération de renommage implémentant *Applicable*
	  - Comment contraindre la pureté de la fonction /transform()/ ?
	- Et c'est grâce à la fonction /applyTo(doc: LogootSRopes)/ de cette dernière qu'ils mettraient à jour leur copie du document
	- Les opérations *LogootSAdd* et *LogootSDel* devraient aussi être refactorées de la sorte
	- De cette manière, toute la logique des opérations et de leurs transformations serait encapsulée dans /mute-structs/
	- Et la seule responsabilité de /mute-core/ serait de transformer les opérations avant de les délivrer
      - Des interfaces supplémentaires, *Compressable* et *Decompressable*, permettent de représenter le mécanisme d'optimisation de l'opération de renommage
	- Les opérations distantes implémenteraient l'interface *Compressable*
	- Les compresser renverraient une opération équivalent *Decompressable*
	- Ainsi, on les compresserait avant de les partager sur le réseau
	- Les sites distants les décompresseraient avant de les transformer par rapport aux opérations concurrentes  puis de les appliquer sur leur état
	- On peut généraliser ce comportement aux autres opérations *LogootSAdd* et *LogootSDel*
	  - Pour *LogootSDel*, on peut par exemple ne conserver que la partie unique des identifiants supprimés
	    - On gagne en bande passante mais on perd en calculs
	  - Pour *LogootSAdd*, les méthodes /compress()/ et /decompress()/ peuvent se contenter de renvoyer l'opération telle quelle
- Meeting
  - Now that i have figure out and written down the transformation functions of insertions and deletions concurrent to a renaming
  - Thinking about the correct way to implement it
  - At first, the renaming mechanism did not require much additional metadata
    - A log of operations
  - So thought about putting everything needed in /mute-structs/
  - But it was because only one node would be able to perform the renaming
  - Now that we want every node to be able to perform the renaming, we have to deal with concurrent renamings
  - It requires much more metadata
  - And some of these metadata are in /mute-core/ currently
  - So some reflexion to do a correct separation of concern
  - And how these two projects will interact together
*** Planned
** Semaine du <2018-03-19 Mon> au <2018-03-23 Fri>
*** Done
- Formation *Outils numérique pour la pédagogie*
  - Classe inversée
    - Au lieu de faire le cours en classe puis les exercices à la maison
    - Fait le cours à la maison (par le biais de vidéos éducatives notamment) puis les exercices en classe
    - Le rôle de l'enseignant devient donc d'aider chaque étudiant individuellement en fonction des difficultés qu'il rencontre
  - Studio Professeur
    - https://numerique.univ-lorraine.fr/audiovisuel/studio-professeur-pour-faire-evoluer-vos-formes-denseignement
  - Camtasia
    - Alternatives
      - OBS (pour recording)
      - OpenShot (pour montage)
  - Arche
    - Créer un cours
      - Éviter les formats /activité unique/ et /informel/
    - Ajout d'une ressource
      - Dans *Apparence*, utiliser /Nouvelle fenêtre/ comme /Affichage/ pour forcer l'ouverture du document dans un nouvel onglet
    - Gestion de groupes
      - Depuis la page des participants > Roue dentée > Groupes
      - /Créer des groupes automatiquements/ génère des groupes randoms
      - On peut importer des groupes via un fichier csv
	- Pas l'air de pouvoir ajouter les participants directement dans les groupes
- Rédaction du /research statement/ pour Satis 2018
  - Motivations
    - Plus en plus d'utilisateurs
    - Afin de supporter la charge et d'assurer une bonne qualité de service, les applications à large échelle doivent s'orienter vers les paradigmes décentralisé ou distribué
    - Mais ces paradigmes entraînent des problèmes de cohérence et de disponibilité
    - Comme indiqué dans le CAP Theorem, un système ne peut pas assurer les 2
    - Il est donc nécessaire de faire un choix
    - Choix de l'eventual consistency consiste à sacrifier temporairement la cohérence pour assurer la disponibilité
    - Mais nécessite un mécanisme de résolution de conflits
  - CRDT
    - Structures de données
    - Présentation de /State-based/ CRDTs
    - Présentation de /Operations-based/ CRDTs
  - Utilisation d'identifiants dans les CRDTs
    - Contraintes portant sur les identifiants
    - Ils grossissent en fonction du nombre de noeud mais aussi parfois en fonction du nombre d'éléments
    - Problème d'identifiants ayant une taille non-bornée
  - But du PhD
    - Proposer des spécifications d'identifiants plus efficaces
    - Proposer un mécanisme de renommage pour réduire la taille des identifiants
*** Planned
** Semaine du <2018-03-26 Mon> au <2018-03-30 Fri>
*** Done
- Rédaction du /research statement/ pour Satis 2018
  - Prise en compte des retours de la part d'Olivier, Victorien et Gérald
    - Développement de la section sur les identifiants
    - Développement de la section sur les pistes du PhD
*** Planned
**** DONE Rédaction de la candidature pour Satis 2018
** Semaine du <2018-04-03 Tue> au <2018-04-06 Fri>
*** Done
- COO : TP1
  - Environnement pour la génération de clé SSH
    - Démarrer un conteneur Docker sous Ubuntu en mode interactif
      - /docker run -it ubuntu:xenial/
    - Mettre à jour les paquets
      - /apt-get update/
      - /apt-get upgrade/
    - Installer git
      - /apt-get install git/
  - Commandes pour la démo de génération de clé SSH
    - *docker run -it git:xenial /bin/bash*
    - /ssh-keygen -t rsa/
    - /cd/
    - /ll/
    - /cd .ssh/
    - /ll/
    - /cat id_rsa.pub/
  - Slides d'introduction
    - Me présenter
      - Études
	- DUT suivi de TELECOM
      - Projets
	- PLM
	- MUTE
      - Présent
	- Doctorant en 1ère année
    - Rappeler le sujet de ce cours
      - Enseigner les bonnes pratiques à prendre en tant que développeur
	- L'utilisation d'un logiciel de gestion de versions
	- La réalisation de tests unitaires
	- Passer par une phase de conception et de modélisation avant de coder
    - Insister sur l'intérêt de git
      - Fusion automatique des modifications
      - Historique des versions du projet
      - Faciliter la collaboration pour vous permettre de vous concentrer sur le développement
    - Recommandations
      - Faire des petits commits, quitte à en faire beaucoup
      - Écrire des messages de commits simples et courts, mais complets
- COO : TP2
  - Slides
    - Clé d'inscription au module arche : groupe_S2C
    - Différences entre dossier courant, index et repo local
- Meeting
  - Result of my application to SATIS 2018 has been delayed until April 16
  - Read Anna: A KVS for any scale
    - Use CRDTs for values
    - Use the Coordination-free Actors paradigm
      - Each actor will handle requests for some keys
      - According to the replication factor, several actors will share keys
      - An actor share its value for a key with others so they can update their own value by merging both (since values are CRDTs)
    - Implement several consistency models
      - Causal order
      - Read your writes
    - Scale from a multicore machine to a geo-distributed deployment
    - Able to outperform Redis and Cassandra
      - Both in case of high contention of the updates and low contention
    - No details about the error management
      - If an actor fails after performing an update but before propagating its state to others, what happens ?
    - Interesting observation
      - "Most of the overhead goes to the network packet handling and message serialization and deserialization"
      - Not to gossiping the states from actors to others and merging them
      - Would be interesting to perform a profiling on MUTE to see how we fare
*** Planned
**** DONE CRDT : Lire Anna: A KVS for any scale
- Anna est un key-value store où l'accent est mis sur la scalabilité
- Anna fonctionne correctement sur une machine, mais l'application peut aussi être déployée de façon distribuée
- Anna semble utiliser des CRDTs en son coeur
- Étudier ça
- Disponible ici : https://blog.acolyer.org/2018/03/27/anna-a-kvs-for-any-scale/
** Semaine du <2018-04-09 Mon> au <2018-04-13 Fri>
*** Done
- COO : TP3
  - Questions
    - Les faire utiliser des attributs privés ?
      - Et donc les faire tester en utilisant les getters ?
    - Quels sont les 3 cas de tests attendus pour le constructeur avec paramètre ?
      - pv > 0
      - pv = 0
      - pv < 0
	- Dans ce cas, quoi tester ?
	- En fonction de comment est définie /etreMort()/, on peut accepter un nombre négatif de points de vie
    - Pareil pour /stockerNourriture()/
      - On peut envisager utiliser /stockerNourriture()/ avec un nombre négatif pour diminuer les réserves de nourriture de l'animal
    - À propos du poly, c'est quoi la classe *Addition* ?
- MUTE : Implémenter le mécanisme de renommage
  - Ajout des versions *Transformable* de *LogootSAdd* et de *LogootSDel*
  - Modification de /add()/ et /remove()/ de *RenamableReplicableList* pour renvoyer des *TransformableLogootSOperations*
  - Reste un problème de conception et de typage à résoudre
    - *ReplicableList* prend en entrée (opérations distantes) renvoie (opérations locales) des *LogootSOperations*
    - *RenamableReplicableList* prend en entrée (opérations distantes) des *ApplicableLogootSOperations* et renvoie (opérations locales) des *TransformableLogootSOperations*
    - On veut
      - Empêcher de fournir des *TransformableLogootSOperations* en entrée de *ReplicableList* et *RenamableReplicableList*
	- Besoin de transformer l'opération avant de pouvoir l'appliquer
      - Empêcher de fournir des *LogootSOperations* en entrée de *RenamableReplicableList*
      - Empêcher de fournir des *ApplicableLogootSOperations* en entrée de *ReplicableList*
- MUTE : Implémenter un log CRDT-agnostic
  - Données potentielles
    - Auteur
    - Horloge logique
    - Opération
      - Type d'opération
	- insert
	- delete
      - Offset
      - Content
	- Peut être obfusqué ?
      - Length
    - Contexte de génération
      - Vecteur d'état
    - Timestamp début de l'opération
      - Quand l'utilisateur tape sur son clavier
    - Timestamp fin de l'opération
      - Quand la modification est intégrée au modèle
  - Mock-up
    #+BEGIN_SRC
[
    {
        "author": 0,
        "clock": 0,
        "beginTimestamp": 1523531787342,
        "endTimestamp": 1523531787350,
        "operation": {
            "type": "insertion",
            "offset": 0,
            "content": "Hellox",
            "length": 6,
        },
        "context": {}
    },
    {
        "author": 1,
        "clock": 0,
        "beginTimestamp": 1523531787343,
        "endTimestamp": 1523531787348,
        "operation": {
            "type": "insertion",
            "offset": 0,
            "content": " world",
            "length": 6
        },
        "context": {}
    },
    {
        "author": 2,
        "clock": 0,
        "beginTimestamp": 1523531960875,
        "endTimestamp": 1523531960900,
        "operation": {
            "type": "deletion",
            "offset": 5,
            "content": "x",
            "length": 1
        },
        "context": {
            "0": 0,
            "1": 0
        }
    }
]
    #+END_SRC
  - Problème
    - Pour rendre ce log CRDT-agnostic, nous retirons les opérations distantes pour ne conserver que les opérations locales
    - Le problème est que rejouer ces opérations locales ne nous garantit pas le même résultat, spécifiquement dans le cas d'insertions concurrentes
    - Par exemple dans le mock-up précédent, les opérations <0,0> et <1,0> sont concurrentes
    - On peut donc se retrouver avec les états suivants, en fonction des identifiants générés par *LogootSplit*
      - Hellox_world
      - _worldHellox
    - Avec les informations disponibles dans le log à l'heure actuelle, nous avons pas de moyens d'enforcer un de ces états plutôt qu'un autre
    - Rejouer l'opération locale suivante, qui dépend causalement de ces 2 opérations concurrentes, aura donc un résultat différent en fonction de l'état obtenu précédemment
      - Potentiellement, nous ne respecterons pas l'intention de l'utilisateur
    - Ce log ne permet donc pas de reproduire de façon déterministe le scénario d'une collaboration
    - Pour cela, il faudra rajouter des informations supplémentaires
      - identifiants générés/supprimés
      - seed pour la factory des identifiants
      - texte du document au moment de l'opération
    - Mais ces informations sont spécifiques pour un algorithme donné
      - Les algorithmes OT ne sauront pas utiliser les identifiants *LogootSplit* pour ordonner des opérations concurrentes
	- Même *DottedLogootSplit* peut rencontrer des difficultés pour cela
      - Ajouter une donnée telle que la seed de génération des identifiants ne garantirait pas que les algorithmes l'utiliseraient et obtiendraient les mêmes résultats
    - On peut même se retrouver avec des scénarios qui ne sont pas reproductibles avec des algorithmes données
      - Certains algorithmes OT ordonnent des insertions concurrentes au même offset à l'aide de l'identifiant de l'utilisateur
      - Si les logs ont été générés avec *LogootSplit*, on peut avoir un bloc de l'utilisateur *A* qui se retrouve après un bloc de l'utilisateur *B* généré en concurrence
  - Conclusion
    - On peut difficilement utiliser ce type de log pour évaluer la correctness d'un algorithme donnée
      - Il faudrait pour cela y ajouter des données spécifiques à cet algorithme
    - Par contre, si on offusque le texte, on ne rencontre plus ces problématiques d'états ambigus
    - Ce type de log est donc plus adapté pour du benchmarking seulement IMHO
  - Questions ouvertes
    - Comment se comporterait le mécanisme de collecte des données ?
      - Le flag autorisant la collecte devrait être pour le document
      - Que faire dans les cas où
        - On ne collectait pas les données du document et maintenant quelqu'un active le flag
	  - On risque de ne pas collecter des opérations concurrentes à l'activation
	- On collectait les données du document et maintenant quelqu'un désactive le flag
	  - Potentiellement des opérations concurrentes à l'opération de désactivation du mécanisme de collecte vont être collectées
	  - On peut accepter ces opérations
	  - Ou on peut, avec un /stateVector/ tronquer le log à posteriori pour ne plus conserver ces opérations concurrentes
	- Après une désactivation du mécanisme de collecte, celui-ci est de nouveau re-activé
    - Que faire si une opération est manquante ?
      - Peut-on compenser une ou des opérations manquantes ?
  - Discussion avec Chahrazed
    - A effectué ses simulations multi-agent à l'aide de *JADE*
      - Se charge de démarrer les agents, de les faire s'enregistrer auprès d'un service de discovery et de créer les connexions pair-à-pair
      - Communications se font par le biais de message
      - Disponible ici : http://jade.tilab.com/
    - Pourrait potentiellement examiner des logs pour en extraire des comportements utilisateurs
      - Voir avec elle les méthodes utilisées
    - Commentaire intéressant : séparer les logs d'utilisateurs en plusieurs fichiers rendraient l'analyse plus complexe
      - Voir pour tout centraliser dans un fichier unique par document
    - À voir la quantité de données nécessaires
- Thèse : Réunion de suivi
  - Avancement
    - Intéressé
      - Protocoles de consensus
      - Utilisation des CRDTs
	- Base de données distribuées
	  - Dynamo
	  - Anna
	  - OrbitDB
	- IPFS
	  - Potentiellement besoin pour représenter les données dynamiques
	- Langage de programmation
	  - Lasp
    - Mécanisme de renommage
      - On peut proposer un mécanisme de renommage pour /Operations-based CRDTs/ si on arrive à proposer
	- Une opération de renommage commutative avec les opérations initiales du CRDT
	- Des fonctions de transformation des autres opérations par rapport à l'opération de renommage qui
	  - Conserve l'intention de l'utilisateur
	  - Sont déterministes
    - Implémentation en cours du mécanisme de renommage dans /mute-structs/
      - Implémenté
	- Génération d'une /renamingMap/
	- Renommage local de l'état
	- Génération d'une opération de renommage correspondante
	- Mécanisme de compression de cette opération de renommage
	- Mécanisme de décompression de cette opération de renommage
      - L'implémentation m'a permis de me rendre compte qu'on retombe dans les transformées opérationnelles
	- Pour appliquer une opération concurrente à un renommage, il est nécessaire de la transformer au préalable
	- De la même façon, pour appliquer une opération de renommage distante sur un état sur lequel on a déjà appliqué des opérations concurrentes
        - Permet de retrouver un cadre formel
	  - Peut vérifier la correctness de fonctions de transformations à l'aide de TP1 et TP2
      - Reste à implémenter
	- La transformation de l'opération de renommage par rapport aux opérations concurrentes
	  - Insertions
	  - Suppressions
	  - Renommages
    - Lu /Consistency without concurrency control in large, dynamic systems/
      - Propose un mécanisme de renommage pour le CRDT TreeDoc
	- La taille des identifiants augmente avec le nombre d'insertions
	- Mais la suppression d'éléments ne permet généralement pas de réduire la taille des futurs identifiants
      - Basé sur une architecture 2-tiers
	- Le /core/, un ensemble de noeuds stables et dans lesquels on a confiance
	- La /nebula/, le reste des noeuds qui peuvent être volatiles
      - La /nebula/ permet d'assurer une collaboration à grande échelle
      - Tandis que le /core/ se charge d'effectuer un renommage à l'aide d'un algorithme de consensus 2PC
      - Les noeuds de la /nebula/ ont un mécanisme de /catch-up/ pour mettre à jour leur structure suite à un renommage du /core/ et pour transformer leurs opérations concurrentes au renommage
  - Problèmatiques soulevées
    - Scope de la contribution
      - Est-ce qu'une adaptation du mécanisme de renommage de *Treedoc* pour *LogootSplit* suffirait ?
	- Oui ça suffit
      - Ou faut-il aller plus loin ?
	- Généralisation du mécanisme de renommage
	  - Voir par exemple Riak
	  - Voir le type de CRDTs utilisés
	  - Voir s'ils ont le problème des identifiants avec des contraintes qui grossissent
	  - Voir si on peut adapter le mécanisme pour ça
	- Suppression de la nécessité d'un consensus
	  - Possible, il suffit juste de définir un ordre total sur les opérations de renommage et en faire gagner une lorsqu'il y a une concurrence
	  - La difficulté porte alors les opérations qui dépendent causalement d'une opération de renommage qui "perd" face à une autre opération de renommage concurrente
	    - Besoin de les transformer en arrière pour obtenir une opération équivalente partageant le même contexte que l'opération de renommage qui "gagne"
    - Évaluation de la solution
      - Simulation
	- L'idée serait de rejouer des sessions de collaboration à l'aide de bots
	  - Mesurer l'évolution des ressources consommées en
	    - CPU
	    - RAM
	    - Bande passante
	  - Vérifier la correctness du mécanisme
	- Besoin de logs
	  - Récupérer des logs
	    - Mais où ?
	    - On veut autoriser de la concurrence dans le log
	      - Exclu Wikipédia
	    - On veut pouvoir insérer des opérations de renommage dans le log et que les opérations suivantes dépendent causalement dessus
	      - Exclu le log actuel de MUTE
	    - Peut ajouter un mécanisme de collecte des traces d'utilisation dans MUTE
	    - Mais besoin d'utilisateurs ensuite
	  - Générer des logs
	    - Bots qui écrivent aléatoirement
	      - Criticable d'un point de vue réalisme des traces et du coup des performances
	    - Étudier le comportement des utilisateurs dans des sessions d'édition collaborative pour générer des traces plausibles
	      - Toujours criticable d'un point de vue réalisme
	      - Time-consuming
	      - Pas forcément l'angle que j'ai envie de donner à ma thèse
        - Besoin d'un environnement de simulation
	  - Doit pouvoir démarrer une simulation à partir d'un ensemble de logs
	    - Un bot pour chaque utilisateur
	    - Les bots doivent être orchestrés au démarrage pour l'initialisation de la simulation
	    - Puis ensuite les bots agissent indépendemment à partir de leur log
	      - Permettrait d'autoriser de la concurrence par moments
	    - Puis de nouveau une orchestration finale pour récupérer les résultats
	  - Voir les travaux de Vinh et Quentin pour les expériences sur MUTE
      - Preuve
	- Comme on retombe dans du OT, peut tenter de prouver que les fonctions de transformations
	  - Conservent l'intention de l'utilisateur
	  - Respectent TP1 et TP2
	- Mais connaît peu le domaine des méthodes formelles
	- Aurait besoin de me former là-dessus
    - Déclenchement du renommage
      - La fréquence de déclenchement ainsi que le nombre de renommage concurrents influencent les performances du mécanisme
      - P-e intéressant d'essayer plusieurs stratégies pour déterminer laquelle est la plus performante
      - Stratégie de renommage
	- Quand le déclencher ?
	- Qui le déclenche ?
- Meeting
  - Renaming mechanism
    - Making progress
    - Still having some trouble to build a system preventing me from making mistakes while keeping it simple
    - Have to rework the class hierarchy
  - CRDT-agnostic log mechanism
    - Goal is to keep a log which would allow us to replay collaborative editing session without being tied to a conflict resolution mechanism
      - Want to be able to replay the collaborative editing session with *LogootSplit*, or *LogootSplit with Renaming*, or *DottedLogootSplit* or even OT algorithms
      - To perform benchmarks
    - Worked on it with Victorien to determine the data required
    - Realized that we won't be able to replay exactly a collaborative editing session as it was, to replicate it
      - Since algorithms like *LogootSplit* are not determinist
      - Even with the same algorithm, are not able to ensure the same result by replaying twice the same log
      - Won't be able to use it to check the correctness of a conflict resolution mechanism
    - Discussed with Le and Claudia to see if additional fields would be useful for their analysis
    - Now have to think about the implementation
*** Planned
** Semaine du <2018-04-16 Mon> au <2018-04-20 Fri>
*** Done
- COO : TP5
  - Questions
    - Cours
      - Slides 62-63 : slide dupliquée
      - Slide 67 : mentionne un polycopié, lequel ?
    - TD
      - Est-ce que l'on leur fait écrire du code Java sur feuille ?
	- Oui
      - Qu'est-ce qu'on entend par /descriptif/ dans l'exo 4 ?
	- À la 1ère lecture, j'ai cru qu'un /toString()/ suffisait
	- En voyant la correction, j'ai l'impression que tu attends plus
	- /toString()/ suffit
      - L'emploi du temps est-il aussi à rendre ?
	- Non
  - Remarques
    - Beaucoup de mal à garder l'attention des étudiants après l'exo 2
    - P-e revoir l'ordre des exercices
- MUTE : Implémenter un log CRDT-agnostic
  - Ajout de données supplémentaires
    - Évènements
      - Connexion
      - Déconnexion
      - Réception d'un évènement généré par un autre utilisateur
    - Données
      - Origine de l'évènement (local ou distant)
      - Envoyeur
      - Participants
      - Voisins
      - User-agent
  - Décomposition du travail
    - Établir les données du log
      - Faire un 1er jet pour les évènements
      - Documenter les différents champs
      - Publier ça sur Github
    - Mettre en place un log local
      - Écouter les différents évènements dans MUTE
      - Enregistrer leurs données dans une base de données locale
    - Mettre en place un service de collecte centralisé
      - Déployer un service permettant de centraliser les traces d'utilisations
      - Réfléchir à l'intégration de ce service avec l'application
	- Ajout d'une fonctionnalité permettant d'activer/désactiver le partage des traces pour le document
	- Déterminer que faire lorsqu'un utilisateur
	  - Désactive le partage
	  - Active le partage
    - Exploiter les traces
      - Développement de scripts permettant
	- D'extraire les données
	- De les formater
	- De filtrer des champs
	- De combler les opérations manquantes si besoin et si possible
  - C'est Cédric qui va se charger de réaliser l'implémentation du mécanisme
- Thèse : Réunion de suivi
  - Remarques
    - Manque de clarté sur les identifiants qui grossissent
      - Mal amené et mal justifié
      - Mieux développer cette partie à l'avenir
    - Motivations
      - Manque de contexte
      - Besoin d'étudier si c'est vraiment un problème
      - Dans quelle mesure
      - Dans quel cas
    - Regarder Cassandra
    - Mettre à jour le /research statement/ avec les fonctions de transformées
- Séminaire autour de la reproductibilité en recherche informatique
  - Un journal qui propose de publier des réplications d'articles
    - Se base sur le fait que certains articles ne sont pas forcément réutilisables
      - Code source non publié
      - Données non disponibles
      - Modèle pas clairement défini dans l'article
    - Mais que les éditeurs ne sont pas forcément intéressés par rendre ces articles réutilisables
  - Disponible à http://rescience.github.io
  - Tourne autour des 5 R
    - Re-runnable : Peut re-exécuter le programme
    - Repeatable : Obtient le même résultat à chaque exécution
    - Reproducible : Peut donner le programme à un autre pour qu'ils obtiennent les mêmes résultats
    - Replicable : Peut recréer le modèle/le programme à partir de l'article
    - Reusable : ?
- Meeting
  - Application to SATIS 2018 summer school accepted
  - Had a meeting with Gérald and Olivier
    - Have to dig a little bit about the problem adressed
    - Should look for real-case examples of the evergrowing size of metadata issue
  - Log mechanism for collaborative editing
    - Rework it according to feedbacks
    - Put a mock-up and its documentation on a github repo
    - Discussed with Cédric how to proceed for the implementation
  - Start to read *Context-Based Operational Transformation in Distributed Collaborative Editing Systems*
    - Since i need OT in my renaming mechanism, studying the field
    - Highlight that the causality relationship is actually not sufficient to determine when to transform an operation and according to which ones
    - Introduced the notion of context of generation of an operation to address this issue
*** Planned
**** DONE MUTE : Implémenter un log CRDT-agnostic
- Pour le besoin des expérimentations, nous avons besoin de traces d'utilisation
- Cependant le log d'opérations que nous stockons actuellement dans MUTE n'est pas adapté
  - Il n'offre qu'une linéarisation des opérations pour un utilisateur donné
    - On ne conserve pas les informations de concurrence et de causalité
  - On ne peut pas utiliser les opérations directement avec un CRDT
    - Les opérations étant des opérations *LogootSplit*
    - On peut s'en sortir en rejouant les opérations et en récupérant les opérations textes générées
- On pourrait, afin de construire un corpus, générer un autre log contenant
  - Les opérations textes de l'utilisateur
    - Afin de pouvoir les rejouer en utilisant un autre modèle
  - Avec pour chaque opération
    - Le vecteur d'état représentant le contexte de génération de l'opération
      - Afin de capturer les informations de causalité des opérations
    - Potentiellement, l'état après opération
      - Afin de vérifier la correction du modèle utilisé
      - Mais dans ce cas, il ne faudrait jouer strictement que les opérations distantes faisant partie des dépendances de la prochaine opération locale
- On pourrait ainsi rejouer fidèlement une collaboration à partir des logs de chaque participant
  - Un noeud par participant
  - Initialise un réseau P2P regroupant tous les noeuds
  - Dans l'ordre causal de ses opérations locales
    - Chaque noeud essaie de jouer sa prochaine opération
      - Il peut la jouer uniquement s'il a reçu toutes ses dépendances causales
      - Si le noeud ne possède pas toutes les dépendances de sa prochaine opération, il attend
    - Une fois l'opération locale jouée, il partage avec les autres noeuds l'opération distante correspondante
    - Potentiellement, vérifie après avoir appliqué chaque opération que l'état courant correspond à l'état enregistré
  - Doit s'assurer que
    - Le réseau P2P est correctement construit avant de donner le top départ aux noeuds
    - Chaque opération distante est délivrée /at-least-once/ à chaque noeud
- Reste à réfléchir à comment gérer la fin de la simulation
  - Ce n'est pas parce qu'un noeud n'a plus opérations locales à jouer que la collaboration est finie
    - D'autres noeuds peuvent avoir des opérations restantes de leur côté
    - Par exemple, un noeud peut avoir jouer toutes ses opérations au bout de 5min sur les 50min de collaboration
  - Faut-il spécifier un état final aux noeuds qu'ils doivent atteindre avant de quitter ?
    - Grâce au vecteur d'état obtenu en aggrégeant le nombre d'opérations par noeud
  - Ou finalement, chaque noeud peut quitter une fois son travail effectué ?
** Semaine du <2018-04-23 Mon> au <2018-04-27 Fri>
*** Done
- OT : Lire *Context-Based Operational Transformation in Distributed Collaborative Editing Systems*
  - Dans un système possèdant des opérations
    - /Originales/ et /transformées/
    - /Normales/ et /inverses/
  - La relation de causalité n'est pas suffisante pour déterminer quand transformer une opération et par rapport à lesquelles
    - La relation de causalité n'étant définie que pour les opérations originales normales
    - Les opérations transformées ou inverses en sont dénuées
    - Mais on peut avoir besoin de transformer une opération transformée ou inverse
  - Introduit la notion de contexte de génération d'une opération pour corriger cela
    - Toutes les opérations possèdent un contexte
    - Une opération ne peut être appliquée sur un état que lorsque le contexte de l'état correspond au contexte de l'opération
    - Une opération ne peut être transformée par rapport à une autre que lorsque leur contexte sont équivalents
    - Le contexte ne comprend que les opérations originales
  - Propose 6 conditions nécessaires pour l'écriture de fonctions de transformation correctes
    - CC1: Une opération ne peut être transformée que sur un état incluant son contexte
    - CC2: Une opération /O/ doit être transformée par rapport aux opérations inclues dans la différence /DS - C(O)/
      - Où /DS/ est l'ensemble des opérations observées à un état donné
    - CC3: Une opération ne peut être exécutée que sur un état équivalent à son contexte
    - CC4: Une opération ne peut être transformée par rapport à une autre que si son contexte est inclus dans celui de cette dernière
    - CC5: Une opération /Ox/ doit être transformée par rapport aux opérations inclues dans la différence /C(O) - C(Ox)/ avant d'être transformée par rapport à une opération /O/
    - CC6: Une opération /Oa/ ne peut être transformée par rapport à une opération /Ob/ que si leur contexte sont équivalents
  - Propose un /context vector/ pour représenter le contexte d'une opération
    - Un /state vector/ n'inclut pas les opérations inverses
    - Donc besoin d'un nouveau type de vecteur incluant ce type d'opérations
    - Permet de représenter le contexte de génération d'une opération
    - Et donc de déterminer par rapport à quelles opérations la transformer
  - Utilise un /operation version group/ pour améliorer les performances du système
    - Constate que les mêmes transformations d'une même opération originale sont calculées plusieurs fois au cours d'une session collaborative
    - Ces transformées sont coûteuses à regénérer
    - Propose de stocker les différentes versions d'une même opération afin de les réutiliser
    - Mais ceci est coûteux en mémoire
    - Propose des stratégies à adopter pour conserver uniquement les versions les plus utiles probablement
  - Remarques
    - Définir une fonction /Exclusion Transformation (ET)/ nécessite de respecter la /Reversibility Property (RP)/ entre IT et ET
      - Est-ce vraiment nécessaire dans mon cas ?
    - La condition CC3 semble trop contrainte pour mon système
      - Cela dépend la notion de contexte d'une opération que l'on définit
    - Si on considère que tous les types d'opérations utilisent la même notion de contexte
      - CC3 impose un ordre sur l'exécution des opérations qui est trop contraint par rapport à nos possibilités
	- Dans le cas où un site A génère une opération /o1/ puis une opération /o2/, tandis qu'en parallèle un site B génère une opération /o3/
	  - Avec /o1/, /o2/ et /o3/ des opérations qui peuvent commuter telles que des insertions
	- CC3 indique qu'on ne peut pas jouer l'opération /o2/ sur le site B directement, qu'il faut déjà avoir joué /o1/ pour cela
	- Alors que pourtant, on peut bel et bien jouer /o2/ puis /o1/
      - Dans ce cas, respecter CC3 assure que le système est correct, mais il ne s'agit pas de la condition optimale
      - Il faudrait adapter CC3 pour notre système
      - À partir de là, ne faudrait-il pas aussi retravailler les autres contraintes ?
    - Sinon, on peut générer des contextes différents pour différents types d'opérations
      - Pour les opérations d'insertions et de suppressions
	- Puisqu'on ne transforme ce type d'opérations que par rapport aux opérations de renommages
	- On ne pourrait répertorier dans le contexte que ces dernières
      - Pour les opérations de renommage
	- Celles-ci, on les transforme par rapport à toutes les autres opérations
	- Là on a besoin du /state vector/
      - La difficulté à résoudre dans ce cas est comment déterminer par rapport à quelles opérations transformer une opération ?
	- Dans le papier, ils proposent la formule /DS - C(o)/
	  - Où /DS/ est l'état courant du document, que l'on peut représenter dans notre cas avec un /state vector/
	- Dans le cadre d'une opération de renommage, cette formule est valide
	  - Puisqu'on transforme une opération de renommage par rapport à toutes les opérations concurrentes
	- Mais cela ne correspondrait pas dans pas le cas d'une opération d'insertion ou de suppression
	  - /DS - C(o)/ renverraient les opérations concurrentes, de renommage mais aussi d'insertion ou de suppression
	  - Nous avons juste besoin des opérations de renommage
	  - Il faudrait donc filtrer le résultat /DS - C(o)/ pour ne conserver que ces dernières
    - Finalement, gérer des contextes différents par type d'opérations ne réduirait pas t-il l'aspect générique de l'approche OT ?
      - À voir si on arrive à abstraire suffisamment pour retomber sur quelque chose de générique
- OT : Lire *OT FAQ*
  - Context-based Conditions
    - [[file:img/ot-context-based-conditions.png]]
  - ET
    - [[file:img/ot-et-pre-post-conditions.png]]
    - [[file:img/ot-et-property-pre-cond.png]]
  - Réflexions
    - Semblerait qu'on puisse appliquer ET pour retirer des opérations dont une opération /o/ dépend causalement pour retirer ces opérations du contexte de /o/
    - Pour respecter les /Context-based Conditions/, notamment CC3, on pourrait donc utiliser ET pour retirer des opérations du contexte des opérations d'insertion et de suppression
      - On pourrait retirer du contexte toutes les opérations qui commutent avec l'opération générée
      - Donc toutes les opérations d'insertion et de suppression depuis le dernier renommage
      - Et pour des raisons de performances, on pourrait argumenter que ça revient à générer l'opération /o/ d'insertion ou de suppression avec /C(o) = C(r) ∪ r/ où /r/ est la dernière opération de renommage
      - Ainsi, l'opération serait délivrable à partir du moment où le site distant a observé /r/
      - Mais ça demanderait au site distant de transformer l'opération /o/ par rapport à toutes les opérations ayant eu lieu depuis la dernière opération de renommage
	- Ce qui semble couteux
      - Mais dans les faits, ces transformations ne modifient pas l'opération donc on peut calculer cette "transformée" aisément
*** Planned
**** DONE OT : Lire *Context-Based Operational Transformation in Distributed Collaborative Editing Systems*
- Claudia m'a parlé de ce papier sur OT qui est l'état de l'art
- Le lire pour se remettre à niveau sur ce domaine
**** DONE OT : Lire *OT FAQ*
- Voir si les méthodes utilisées dans le mécanisme de renommage correspondent bien à des fonctions de transformations
- Voir dans ce cas quelles sont les propriétés qu'elles doivent vérifiées, et comment prouver qu'elles les vérifient bien
- Disponible ici : http://www3.ntu.edu.sg/home/czsun/projects/otfaq/#_Toc321146157
** Semaine du <2018-05-02 Wed> au <2018-05-04 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Discussion avec Gérald à propos de COT
    - Il serait partant pour adapter les /Context-based Conditions/ à notre système plutôt que d'adapter le système aux /Context-basaed Conditions/
    - Voir pour déterminer quelles sont les conditions à respecter et à retravailler
*** Planned
** Semaine du <2018-05-07 Mon> au <2018-05-11 Fri>
*** Done
- COO : TP7
  - Groupe Vernevaut - Jacquot a fait le TP7 ensemble
  - Mais Vernevautt était avec un autre pour le TP6
- COO : TP8
  - *Adapter*
    - Le nom d'objet *Lumiere* est ambigü
    - Les étudiants essaient de créer un lien avec *Lampe*
    - Il serait préférable de trouver un autre appareil
      - *Volet*
      - *Cafetiere*
      - *Climatisateur*
  - *Thermostat*
    - La fonction /toString()/ de *Thermostat* n'est pas redéfinie
    - Difficile de se rendre compte de l'évolution de la température avec les informations à disposition
    - Pas de /isAllume()/ aussi
    - Pour le moment, se contente de renvoyer toujours /true/
- Meeting
  - Study further OT
    - OT defines some constraints that a system has to comply with to ensure its correctness
    - But some of them are too strong for my system
      - For example, require causal delivery of all operations
      - In my system, i do not have to deliver /insert/ in their causal order
    - How to revamp these constraints to adapt them to my system ?
  - Wrote the progress report
- Progress report
  - Ajouter les enseignements au rapport
** Semaine du <2018-05-14 Mon> au <2018-05-18 Fri>
*** Done
- COO : TP9
  - *Lampe* et *Lumiere*
    - Maintenant il y a un lien entre *Lampe* et *Lumiere*
  - *Hifi*
    - On doit gérer le son dans *AdapterAppartHifi* ?
  - Besoin de cast les objets dans les *Adapters*
    - Je ne sais pas si c'est une bonne idée de les encourager à faire des casts
    - Qu'est-ce qu'on essaie de montrer avec /appartement/ finalement ?
      - Qu'une mauvaise conception implique une difficulté d'utilisation par la suite ?
- COO : TP10
  - Mémo PlantUML
    - Erreur dans le diagramme de séquence, flèche pleine pour un message de retour
  - Cours
    - Est-ce que les acteurs extérieurs ont aussi une période d'activité ?
*** Planned
** Semaine du <2018-05-21 Mon> au <2018-05-25 Fri>
*** Done
*** Planned
** Semaine du <2018-05-28 Mon> au <2018-06-01 Fri>
*** Done
- COO : TP14
  - Test diagramme de séquence
    - Mémo diagramme de séquence autorisé
    - Insister sur l'attribut /produits/
      - Doit avoir une entité /produits/ dans le diagramme de séquence
      - Doit avoir des appels vers cette liste
      - Dans question 3 : on n'attend de vous que vous indiquez comment récupérer le 1er élément puis le 2nd élément (pas besoin de boucle)
    - Tâche de corriger ça aujourd'hui de façon à ce que je puisse vous rendre ça demain pour avoir des dernières questions avant le DS
  - Partiel jeudi de 8 à 10
    - Droit
      - Au mémo diagrammes de classe
      - Au mémo diagrammes de séquence
      - Feuille de pompes manuscrite, recto-verso
  - Projet Zeldiablo
    - Précédemment la semaine dernière, vous avez
      - Décrit un 1er diagramme de classe basique
      - Lister les fonctionnalités de l'appli
      - Choisit et décrit plus précisement les fonctionnalités de la 1ère version
	- Particulièrement en définissant des critères de validation
    - Aujourd'hui
      - Conception de ces fonctionnalités
	- Diagramme(s) de séquence
	- Mise à jour du diagramme de classe
	- Répartition du travail à partir de ce dernier
      - Code
	- Mise en place des tests unitaires des fonctionnalités
	- Écriture du code correspondant
      - Objectif de fin séance
	- Une 1ère version fonctionnelle de votre application
    - Demain
      - Met à disposition un moteur de jeu
      - L'objectif sera de pluggé votre application dessus
- COO : Test diagramme de séquence
  - LISEZ L'ÉNONCÉ
  - Soyez cohérent sur les noms de variables et les types, d'une question à l'autre
    - Une fois, produits est une *List*
    - La fois d'après, produits est un *Produit*
  - La flèche de retour n'a que 3 significations possibles (ne correspond qu'à 3 morceaux de codes)
    - Fin de la méthode (dans ce cas, aucune valeur attachée à la flèche)
    - Return (dans ce cas, une variable ou une valeur attachée à la flèche)
    - Une exception levée
  - Dans le diagramme de séquence, nous avons une entité par instance
    - Pour un livre, je vais avoir qu'une seule entité (et pas une entité /p: Produit/ et une entité /l: Livre/)
- COO : Rappels examen
  - Portera sur tout
    - git
    - JUnit
    - Diagramme de classe
    - Diagramme de séquence
  - Droit
    - À une feuille de pompe recto-verso manuscrite
    - Aux mémos diagramme de classe, diagramme de séquence
- ECSCW 2018 :
  - Salles
    - B013 - M2 et M3
    - A006 - M1
    - C005 - Workshop
    - Posters and Demos - A008
- Meeting
  - Nothing interesting
  - Worked on the program for ECSCW 2018
  - Read a paper submitted to CSCW : *Real Differences between OT and CRDT for Co-Editors*
    - Authors compare OT and CRDT to find the common ideas to better understand their differences
    - Since i'm following a MOOC on scientific integrity currently, i can say that this paper does not fit the definition of integrity
      - The comparison is not really fair (non-accurate complexity of algorithms, present false issues, invalid arguments)
    - But at least it motivates my PhD topic
  - Start to read a paper *Conflict-Aware Replicated Data Types*
    - Propose to enhance CRDT with /guard conditions/ on operations
    - Propose a system detecting non-commutative operations from these /guard conditions/ and setting up a consensus to handle these operations
- MUTE : Divergence constatée sur les notes de la réunion
  - On a un bloc de texte qui n'était pas placé au même endroit dans une copie
  - Normalement, on a : received the first response from the journal Computer in Industry... major revision  in 2 months
  - Chez la copie défectueuse, on a : received the first response from the journal Computer in Industry...n  in 2 months major revisio
  - En examinant la structure de données, on observe qu'un bloc est positionné au mauvais endroit
  - Voir à quelle moment cette incohérence apparaît
    - Dès l'insertion ?
    - Ou plus tard suite à un ré-équilibrage foireux ?
*** Planned
**** DONE ECSCW : Compléter le programme de la conférence
- Version web disponible ici : http://ecscw2018.loria.fr/full-programme/
- Lien vers la version papier : https://v2.overleaf.com/16274874bvcwsjtzswvj
** Semaine du <2018-06-04 Mon> au <2018-06-08 Fri>
*** Done
- Lu *Data Laced with History: Causal Trees & Operational CRDTs*
  - Disponible ici : http://archagon.net/blog/2018/03/24/data-laced-with-history/
  - Propose
  - Quelques liens intéressants
    - À propos des différentes horloges logiques : https://queue.acm.org/detail.cfm?id=2917756
    - À propos du fondement mathématique des CRDTS (/monotonic semilattice/) http://jtfmumm.com/blog/2015/11/17/crdt-primer-1-defanging-order-theory/
    - Une autre approche de résolution de conflits, *Differential Synchronization* : https://neil.fraser.name/writing/sync/
    - Causal Tree : http://www.ds.ewi.tudelft.nl/~victor/articles/ctre.pdf
    - Replicated Object Notation : https://github.com/gritzko/ron
    - Pure Operation-based Replicated Data Types : https://arxiv.org/pdf/1710.04469.pdf
- MC : *Practice-based Computing as the Heart of CSCW Research*
  - Considère qu'un résultat est fortement lié au contexte
    - Que ça soit des études sociales
    - Mais aussi des programmes/applications
    - Mais un algo se veut générique
    - Voir pour repenser cette approche
  - Design Research : investigate activities of constructing artefacts which are implemented to achieve social ends
  - Suggère de s'intéresser à comment l'appropriation d'un nouvel artefact change les habitudes/le quotidien de ses utilisateurs
    - Contrairement à juste s'intéresser au caractère innovant de l'artefact comme actuellement
  - Références à regarder
    - Annotated protefolios (Bowers (2014), Gaver & Bowers (2012))
  - Existe un gap entre le design et l'appropriation d'un artefact
  - Design Case Studies
    - Se décompose en 3 étapes
      1. Étude des pratiques sociales d'un domaine
      2. Conception d'un artefact en fonction des pratiques observées précédemment
      3. Étude de l'appropriation de l'artefact
- IUT : Rencontrer Nadia pour décider de mon service l'an prochain
  - Mails envoyés
  - Rencontre prévue le <2018-06-21 Thu> à 10:00
  - Notes
    - Volume horaire 64h max
    - Si l'ensemble des modules n'atteint pas 64h, peut compenser avec des stages
    - Part sur le module d'algo (32h du 3 septembre au 28 Octobre, 4h par semaine)
    - Reste à
      - Fiche de voeux
      - La transmettre à Yolande
    - Aura un suivi de stage à faire
- SATIS : Faire l'ordre de mission et réserver les billets
  - <2018-08-03 Fri>
    - Train Nancy - CDG
      - [ ] 05:55 (arrivée à 07:57)
      - [ ] 06:55 (arrivée 09:09)
      - [X] 09:25 (arrivée à 11:52)
    - Avion CDG - Oslo
      - [ ] 11:00 (arrivée 13:15)
      - [X] 13:40 (arrivée 15:00)
  - <2018-08-09 Thu>
    - [ ] 08:09 (arrivée 09:50)
    - [ ] 09:55 (arrivée 11:45)
    - [X] 11:40 (arrivée 13:30)
    - [ ] 14:05 (arrivée 15:55)
    - [ ] 16:25 (arrivée 18:15)
  - <2018-08-17 Fri>
  - <2018-08-18 Sat>
    - Tromso - Oslo - CDG
      - [X] 08:30 (10:30) - 12:30 (14:55)
    - Train CDG - Nancy
      - [X] 17:43 (arrivée à 19:54)
      - [ ] 18:55 (arrivée à 21:24)
- COO : Corriger les copies du partiel
  - Question 1
    - Plusieurs associations de *ArmeEnchantee* à *Epee* et *Arc*
    - *ArmeEnchantee* possèdent plusieurs instances de *Arme*
    - *Epee* et *Arc* implémentent/héritent de *ArmeEnchantee*
    - Ajoute /munition/ en attribut de *Arme*
    - Oubli d'attributs
    - Formalisme incorrect
  - Question 2
    - Absence de *Arc*
    - Absence de *ArmeEnchantee*
    - Absence de *Main*
    - Diagramme de séquence incohérent avec le diagramme de classe proposé
    - Formalisme incorrect
  - Question 3
    - Ne multiplie pas les dégâts de l'arme
    - Fait un check avec /instanceOf/ pour déterminer le type de *Arme* afin de retirer une munition si besoin, alors qu'on appelle correctement ensuite /arme.utiliserArme()/
    - Code incohérent avec le diagramme de séquence proposé
    - Erreurs de syntaxe
  - Question 4
    - N'instancie pas de *Arc*
    - N'instancie pas de *ArmeEnchantee*
    - N'appelle pas /utiliserArme()/ sur la bonne instance
    - Code incohérent avec le diagramme de classe proposé
    - Erreurs de syntaxe
- COO : Encadrer les projets
  - Template
    - Groupes
      - *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - *CORRIGEUX, GAND, KICHNER, RISSE*
      - *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - *LIGEROT, MONTAUT, JEANDEL, ROBERT*
    - Groupe /noms/
      - État au début de l'itération
	- Programme
	- Conception
      - Itération courante
	- Liste des fonctionnalités
	- Diagrammes de séquence
	- Diagramme de classe
	- Implémentation
	- Tests unitaires
      - Remarques
  - Lundi matin
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - État au début de l'itération
	- Programme
	  - Fonctionnel
	  - Un personnage se déplace dans un labyrinthe
	- Conception
	  - Liste des fonctionnalités de la v1
	    - Liste
	  - Diagrammes de classe et de séquence manquants du repo
      - Itération courante
	- Liste des fonctionnalités
	  - Pas encore établie
	  - Mais souhaite ajouter la possibilité de récupérer des trésors, de tomber dans des pièges et de commencer les monstres
	- Diagrammes de séquence
	  - Pas fait
	- Diagramme de classe
	  - Pas fait
	- Implémentation
	  - Pas fait
	- Tests unitaires
	  - Pas fait
      - Remarques
	- Doivent mettre à jour leur repo pour ajouter les diagrammes de séquence et le diagramme de classe de l'itération 1
	- Pas penser à demander l'exécution des tests
	- Warning d'émis sur la fonctionnalité /labyrinthe procédural/
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - État au début de l'itération
	- Programme
	  - Personnage se déplace dans le labyrinthe
      - Itération courante
	- Liste des fonctionnalités
	  - Pas clairement définie
      - Remarques
	- Pas accès au repo
	- Warning d'émis sur la fonctionnalité /labyrinthe procédural/
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
      - État au début de l'itération
	- Pas de moteur de jeu en arrivant
	- A été ajouté depuis
	- Labyrinthe par défaut
	- Personnage qui peut se déplacer mais pas testé
      - Itération courante
	- Liste des fonctionnalités
	  - Pas clairement définie
	    - Labyrinthe plus grand
	    - Monstres immobiles
	    - Cases piègées
      - Remarques
	- Pas accès au repo
	- Warning émis sur le retard déjà pris sur l'itération 2
	- Peut potentiellement fusionner l'itération 3 et 4 pour remettre le projet carré
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - État au début de l'itération
	- Interface graphique
	- Personnage qui peut se déplacer
	- Présence de murs
	- Bug si on appuie sur plusieurs touches
	- Répartition du travail
	  - Guillaume : Laby
	  - Tom : IHM
	  - Maxime et Thibaut : Personnage
      - Itération courante
	- Liste des fonctionnalités
	  - Pas posée sur papier
	    - Peut passer d'un laby à l'autre
	    - Le personnage ne peut plus traverser les murs
	    - Sérialisation/Désérialisation de labyrinthe
      - Remarques
	- Travaillent déjà sur des mécanismes d'optimisation
	- Conseiller de plutôt se pencher sur des fonctionnalités tant que le besoin ne se fait pas particulièrement sentir
    - Groupe *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - État au début de l'itération
	- V1 finie
	- Mais moteur de jeu pas encore pluggé
	- V1
	  - Instancie un *Labyrinthe* avec *Personnage* qui peut se /deplacer()/
	- Répartition du travail
	  - Adrien
	    - Labyrinthe + tests
	  - Louis
	    - Personnage
      - Itération courante
	- Liste des fonctionnalités
	  - Ajout de monstres
      - Remarques
	- Comme ils étaient en retard sur l'itération 2, ils ont passé cette itération à plugger le moteur de jeu
	- Dit de prendre leur temps pour finaliser cette itération afin de repartir sur des bases saines pour la prochaine itération
	- Warning sur les fonctionnalités procédurales
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      - État au début de l'itération
	- Programme
	  - Construire le labyrinthe
	  - Construire le joueur
	  - Peut pas encore déplacer le joueur
	  - Pas pluggé le moteur de jeu
      - Itération courante
	- Ont passé leur matinée à plugger le moteur de jeu
	- Ont aussi travaillé sur la conception d'un nouveau labyrinthe plus complet
      - Remarques
	- Dit de prendre leur temps pour finaliser cette itération afin de repartir sur des bases saines pour la prochaine itération
  - Lundi aprem
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - État au début de l'itération
	- En chantier
	  - Difficultés pour agrandir le labyrinthe
	  - Difficultés pour transformer la gestion des cases
	  - En train de rattraper des diagrammes de séquences en retard
	  - En train de rattraper le diagramme de classe
	- Itération 3+4
	  - Manque la liste des fonctionnalités sur le repo
	  - N'ont pas fait les diagrammes de séquence, ont directement passé au code
	- Remarques
	  - Avertissement sur la présence des fichiers dans le repo
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - État au début de l'itération
	- Programme qui tourne
	- Diagrammes de séquence présents
	- Manque le diagramme de classe
      - Itération 4
	- Manque quelques fonctionnalités dans la liste
	  - Ajout d'un type de monstre
	  - Inventaire du personnage
      - Remarques
	- Pas de diagrammes de séquence, pas de diagramme de classe
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
      - État au début de l'itération
	- Instanciation d'un labyrinthe à partir d'un fichier qui était buggé
	- En retard sur les tests de cette itération
      - Itération 4
	- Manque la liste des fonctionnalités à ce stade
	  - Déplacement aléatoire des monstres
	  - Attaquer
	  - Subir dégâts
	- Les diagrammes de séquence sont présents par contre
	  - Juste une coquille sur /déplacer()/ : le monstre accède au labyrinthe de manière indéfinie pour le moment
      - Remarques
	- Bien rattrapé l'itération de ce matin
	- Continuer comme ça
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - État au début de l'itération
	- Conception finie des différentes fonctionnalités
	- L'implémentation des différents labyrinthes étaient toujours en cours
	- Un peu de peaufinage réalisé avec des tests
      - Itération courante
	- Liste des fonctionnalités mais courte
	- Quelques diagrammes de séquences, mais pas pour toutes les fonctionnalités
      - Remarques
	- Liste des fonctionnalités un peu courte : ajoutez une description et des critères de validation
	- Arriver plus vite aux diagrammes de séquences de façon à ce que les réunions soient plus productives
    - Groupe *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - État au début de l'itération
	- Fini de plugger le moteur graphique
	- Définit une liste des fonctionnalités
	- Réalisé les diagrammes de séquence correspondant
	- L'implémentation était finie pour 14h
	- Présence de tests unitaires
      - Itération 4
	- Mise en place d'une liste des fonctionnalités
	- Seulement un diagramme de séquence sur /déplacerMonstre()/
	  - Diagramme incomplet, manque l'interaction avec *Labyrinthe*
      - Remarques
	- Faire apparaître la répartition du travail sur la liste des fonctionnalités
	- Ne pas gagner du temps sur les diagrammes de séquence, mais sur la liste des fonctionnalités plutôt
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      - État au début de l'itération
	- Pas fini de plugger
	- Représentation du labyrinthe qui était encore buggée (pb de coordonnées)
	- Nouveau labyrinthe fini
      - Itération courante
	- Réalisation d'une liste des fonctionnalités
	- Mais seulement 1 diagramme de séquence disponible
	- Il s'agit du diagramme de séquence pour /Monstre.attaquer()/
	  - Le diagramme de séquence n'est pas précis sur comment le monstre récupère le joueur
      - Remarques
	- La liste des fonctionnalités est trop précise et rentre trop dans les détails de l'implémentation
	  - Retravailler pour décrire des fonctionnalités et non des classes
	- Finaliser cette version aujourd'hui
	- Avancer plus rapidement sur la liste des fonctionnalités et les diagrammes de séquence au cours des itérations suivantes
  - Mardi matin
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - État début itération
	- Comme itération 2
	- Manque les documents de conception de l'itération 3
      - Itération courante
	- Liste des fonctionnalités présente mais incomplète (description, critères de validation, responsable)
	- Documents de conception manquants
	- Application fonctionnelle
      - Remarques
	- Liste des fonctionnalités incompletes
	- Aucun documents de conception disponibles au cours de la réunion
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - Itération précédente
      - Itération courante
	- Liste des fonctionnalités
	- Diagrammes manquants
      - Remarques
	- Faire apparaître les responsables des fonctionnalités
	- Diagrammes manquants ou insuffisants
	- Voir s'ils arrivent à compléter l'itération pour cet aprem
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
      - Itération précédente
	- Application fonctionnelle
	- Documents de conception présents
	- Tag v4 présent
      - Itération courante
	- Liste des fonctionnalités présentes
	- Diagrammes de séquences réalisés
      - Remarques
	- Faire apparaître critères de validation et responsable de fonctionnalités dans la liste
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - État au début de l'itération
	- Documents de conception présents
	- Tag v4 présent
	- Programme exécutable et fonctionnel
	- Test échouent
      - Itération courante
	- Liste de fonctionnalités prête
	- Diagrammes de séquences réalisés sur papier
	  - Quelques remarques sur la responsabilité de *ZeldiabloJeu* qui fait beaucoup trop de choses
	  - Déléguer une partie de ses responsabilités à *Labyrinthe* et *Carte* notamment pour récupérer un *Monstre*, un *Objet*
    - Groupe *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - Début d'itération
	- Documents de conception présents
	- Tag v4 présent
	- Projet fonctionnel
      - Itération courante
	- Liste de fonctionnalités prête
	- Diagrammes de séquence réalisés
      - Remarques
	- Diagramme de séquence sur /changerMonde()/ incomplet
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      - État au début de l'itération
	- Projet fonctionnel avec interface graphique
      - Itération courante
	- Réalisation d'une liste des fonctionnalités
	- Diagrammes de séquence réalisés
      - Remarques
	- Travailler
  - Mardi aprem
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - Itération précédente
	- Ont retravaillé la liste des fonctionnalités
	- Ont ajouté des diagrammes de séquence
      - Itération courante
	- Liste des fonctionnalités présentes
	- Diagramme de séquence présent que pour une d'entre elles
	- Pas encore eu le temps de faire le diagramme de seq d'une autre
	- La flemme de faire le diagramme de séquence de /monstre.seDeplace()/
      - Remarques
	- Gestion des *Entites* et des *Bonus* semble très similaire
	  - Voir s'ils fusionnent ces concepts
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - Itération précédente
	- Ajout des diagrammes manquants lors de l'itération précédente
	- Retours effectués, pas forcément appréciés
      - Itération courante
	- Liste des fonctionnalités présentes
	- Pas des diagrammes de séquence pour l'ensemble des fonctionnalités
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
      - Itération précédente
	- Application fonctionnelle
	- Pris du retard sur l'implémentation des items
      - Itération courante
	- Liste des fonctionnalités présente
	- Diagrammes de séquences réalisés
	- Un diagramme de séquence sur l'instanciation et le comportement de *GenerateurSprite* serait cool
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - État au début de l'itération
	- Pris du retard sur l'implémentation des boutons déclencheurs
	- Mais le reste des fonctionnalités de l'itération précédente a été implémenté
      - Itération courante
	- Liste des fonctionnalités présente mais incomplète
	- Diagramme de séquence pour /attaquer()/
    - Groupe *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - Début d'itération
	- Un test unitaire n'était pas encore implémenté
      - Itération courante
	- Liste des fonctionnalités présente et correcte
	- Diagrammes de séquence présents, quelques retours à faire
      - Remarque
	- Adrien a fini rapidement sa fonctionnalité, voir pour en ajouter une autre
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      - Itération précédente
	- Fonctionnalités implémentées
	- Manque les tests
      - Itération courante
	- La liste des fonctionnalités est présente
	- Les diagrammes de séquence absents
      - Remarques
	- Possible de faire des diagrammes de séquence sur *Sprite*, *Sound*, fin de partie
  - Mercredi matin
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - Itération précédente
	- Pas pu implémenter la gestion des portes/clefs
	- Fonctionnelle, testée
      - Itération courante
	- Liste des fonctionnalités présentes
	- Diagrammes de séquences absents
      - Remarques
	- Ajouter les tags manquants
	- Ajouter responsables fonctionnalités
	- Warning sur les différentes fonctionnalités qu'ils veulent implémenter au cours de cette itération
	  - J'ai l'impression qu'ils sont trop gourmands
	  - Insisté sur le fait qu'on préfère avoir un projet peaufiné et documenté qu'un projet instable et/ou des documents de conception manquants
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
      - Itération précédente
	- Fonctionnalités implémentées, testées
	- Fenêtre inventaire un peu bugué
      - Itération courante
	- Liste des fonctionnalités établie
	- Peu de documents de conception
      - Remarques
	- Manque les diagrammes de séquence les plus intéressants
	- Prendre le temps de peaufiner le projet
	  - Ajout des diagrammes manquants
	  - Ajout des tests
	  - Ajout de la JavaDoc
	- Pas hésiter à sabrer une fonctionnalité si celle-ci n'est pas finie à temps
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
      - Itération précédente
	- Fonctionnalités implémentées, application fonctionnelle
	- Mise à jour des diagrammes de séquence par rapport aux retours
      - Itération courante
	- Ajout d'1 fonctionnalité : attaque directionnelle
	- Peaufiner l'interface graphique de votre application
	- Retravailler les diagrammes de séquence
      - Remarques
	- Ajouter les diagrammes de classe manquants
	- Documenter l'application
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - Itération précédente
	- Tout était implémenté et testé
      - Itération courante
	- Ajout documents de conception (séquences, classes)
	- Ajout JavaDoc
	- Correction de bugs
	- Préparation de la soutenance
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      - Itération précédente
	- Fonctionnelle (sauf *Donjon*)
	- Pas encore testée
	- Documents de conception manquants
      - Itération courante
	- Préparation de la soutenance
	- Nettoyage du code
	- Essaye de corriger la fonctionnalité /donjon/
	- Ajoute les diagrammes de séquence manquants
      - Remarques
	- Tags manquants
	- Ajouter les diagrammes de classe manquants
	- Warning sur *Donjon*
	  - Ne pas hésiter à supprimer ce début de fonctionnalité s'il pose trop de problèmes
  - Mercredi aprem
    - Groupe *VERNEVAUT, MATUCHET, PALAUT, PALMIERI*
      - B
    - Groupe *KELLER, CUNIN, GHESQUIERE, GREBERT*
      - B+
    - Groupe *DIGNIEL, FARRUGIA, GIBIAT et SELY*
      - C+
    - Groupe *LIGEROT, MONTAUT, JEANDEL, ROBERT*
      -
    - Groupe *CORRIGEUX, GAND, KICHNER, RISSE*
    - Groupe *BALLAND, CADET, CHEVRIER, DIDRY, HUBLAU*
*** Planned
**** DONE PhD : Finir le module d'intégrité scientifique
**** DONE PhD : Évaluer les formations suivies
**** DONE IUT : Rencontrer Nadia pour décider de mon service l'an prochain
**** DONE COO : Corriger les copies du partiel
**** DONE COO : Transmettre les notes des tests sur les diagrammes de classe et les diagrammes de classe
**** DONE COO : Encadrer les projets
**** DONE SATIS : Faire l'ordre de mission et réserver les billets
- Départ pour Oslo : <2018-08-03 Fri>
- Départ pour Tromso : <2018-08-09 Thu>
- Départ pour SATIS : <2018-08-14 Tue>
- Retour : <2018-08-18 Sat>
- Ordre de Mission : https://gfd.inria.fr/portail/Portail.jsp
- Ordre de Mission Sans Frais :
- Oreli : https://neo.mykds.com/home?instance=inria
** Semaine du <2018-07-02 Mon> au <2018-07-06 Fri>
*** Done
- IUT : Cours prog web client
  - Alternance
    - Y a des alternants dans les groupes
    - Il n'est possible de donner du travail aux alternants alors qu'ils sont en entreprises les semaines suivantes
    - La pause de 3 semaines début novembre est une période en entreprise
    - Peut pas donner un projet pour les vacances de la Toussaint
  - Groupes
    - LP1 (horizons variés)
    - LP2 (anciens DUT info)
      - Ont déjà du JS en DUT
      - Connaissent déjà les bases du langage et de la manipulation de DOM
  - Prog web/client
    - <2018-10-01 Mon> au <2018-11-30 Fri>
    - Compétences enseignées
      - LP1
	- Bases de JS
	- Manipulation de DOM
	- jQuery
	- Ajax?
      - LP2
	- DOM
	- jQuery
	- Héritage
	  - Prototype
	  - Classe
	- Fonctionnel
	  - Map
	  - Reduce
        - Modules
	- Webpack
	- TypeScript
	- Tests unitaires
	  - Ava : https://github.com/avajs/ava
	- Tests end-to-end
	  - Cypress : https://www.cypress.io/
  - App web/cient
    - <2018-12-01 Sat> au <2019-03-31 Sun>
    - Contenu
      - Démarre par une application panier en jQuery
      - Puis refait l'application en ViewJS
      - Projet : clone de Slack
    - Compétences enseignées
      - Appels asynchrones
      - Dev une web app avec un framework
      - Ergonomie
    - Pré-requis
      - Bases du langage
      - Connaissance en manipulation de DOM
      - Connaissance de jQuery
- PhD : Réunion suivi
  - Programmée le <2018-07-06 Fri> à 10:00
  - Préparation de la réunion
    - Mécanisme de log
      - Déterminé avec Le et Victorien les évènements et les données associées à logger
      - Travaillé avec Cédric pour la mise en place du système de log
	- Chaque navigateur log les informations en local
	- Puis envoie les logs à un serveur de collecte
      - Reste à mettre en place le mécanisme de groupe permettant d'activer/désactiver la publication des logs
    - Mécanisme de renommage
      - Motivations
	- Littérature
	  - Pas trouvé d'article justifiant que le phénomène des identifiants qui grossissent est un problème
	    - LSEQ par exemple se contente de dire que s'en est un
	  - À part l'article soumis à CSCW qui mentionne des expériences ayant montré une dégradation de l'expérience utilisateur en utilisant /teletype/
	  - Mais ne sait pas la crédibilité qu'on peut donner à ces propros
	    - Vu qu'il se contente de mentionner les expériences sans décrire le protocole expérimental ni publier les résultats obtenus
	- Renseigné sur Redis
	  - Redis propose des CRDTs dans le cadre de ses CRDBs
	  - Plusieurs types dont *String* et *List*
	  - Mais utilise un LWW pour *String*
	  - Et ne mentionne pas l'algorithme implémenté pour *List*
	    - Mais avec des méthodes limitées (ajout en tête, ajout en queue)
        - Renseigné sur Akka.DistributedData
	  - /Distributed key-value store/ construit sur Akka
	  - Propose des CRDTs
	  - Pas de *List*
	  - Mentionne l'utilisation de tombstones et d'un mécanisme de /pruning/
        - *Soulève l'interrogation sur le problème posé*
	  - Est-ce que les types proposés le sont car ils suffisent aux utilisateurs ?
	  - Ou est-ce que les systèmes se brident car les algorithmes proposés souffrent actuellement de problème de performance ?
        - Piste avec xRay
	  - Éditeur de code expérimental développé par l'équipe d'Atom
	  - Souhaitent intégrer des fonctionnalités d'édition collaborative
	  - Utilisaient leurs travaux sur /teletype/ qui implémente *RGASplit*
	  - Mais ont l'air de transitionner sur des identifiants denses
	  - Étudient des pistes d'optimisation de l'espace mémoire consommé par ces identifiants denses
	    - Notamment LSEQ
      - OT
	- Lu *Context-Based Operational Transformation in Distributed Collaborative Editing Systems*
	- Meilleure vision des contraintes et propriétés de cette approche
	- Le problème est maintenant de faire matcher mon mécanisme à cette approche
	  - Pour ce faire, soit je modifie le fonctionnement pour respecter les contraintes
	  - Soit je prouve que certaines contraintes peuvent être allégées
	  - Quelques difficultés à faire ça de façon simple
	- *Est-ce que ça ne vaudrait pas mieux de rattacher à OT que pour la partie validation ?*
      - Travaillé sur la gestion des opérations de renommage en concurrence
	- On peut réussir à gérer des opérations de renommage concurrentes facilement en décidant que l'une l'emporte sur l'autre
	- 2 approches pour cela
	  - Soit lorsqu'on transforme une opération de renommage par rapport à l'autre, celle-ci est transformé en /No Op/
	    - /IT(op1, op2) = op1'/ et /IT(op2, op1) = NoOp/
	  - Soit les opérations de renommage forment un /State-based CRDT/ et transformer une opération de renommage par rapport à l'autre consiste à les fusionner
	    - /IT(op1, op2) = IT(op2, op1) = op1'/
	    - Est-ce que ça permet de simplifier la preuve de correction du mécanisme ?
      - Pas avancé sur l'implémentation
    - Notes
      - Principalement regardé motivations pour les séquences
	- Mais qu'en conclusion on ne trouve pas d'autres cas d'utilisation que nos scénarios traditionnels d'édition collaborative
	- Voir si on peut se pencher sur d'autres CRDTs
      - OT
	- Pas forcément besoin d'appliquer tel quel
	- OT donne la forme générique que je dois respecter
      - Mécanisme de renommage CRDT
	- Finalement simplifie énormément le système de le voir comme un CRDT qui représente les epochs et qui est capable de faire passer les opérations d'une epoch à l'autre
	- Reste à l'écrire et à montrer sa correction
	- D'un point de vue défauts, peut argumenter que cette structure
	  - n'est pas nécessaire tout le temps donc peut être stocké dans une BDD
	  - peut être pruner au fur et à mesure que la causale stability avance
    - Suite
      - Implémentation du mécanisme d'arborescence des époques et de transformation des opérations
	- Test de ce dernier
	- Preuve
      - Mise en place de simulation
	- Déjà se comparer à LogootSplit sans mécanisme
	- Voir si on peut facilement plugger /teletype/ pour se comparer à RGASplit
      - Réfléchir à l'après
- CRDT : Regarder *RedisConf18: CRDTs and Redis—From Sequential to Concurrent Executions*
  - La vidéo présente les différents CRDTs implémentés et disponibles dans *Redis*
    - Mais ne parle pas de comment ces CRDTs sont ensuites utilisés
  - Remarque intéressante sur la latence
    - La latence existe et existera toujours, ainsi que les différences en fonction des distances
      - Déjà assez proche des limites théoriques
      - La latence ne pourra qu'augmenter avec l'expansion de l'espèce humaine (ex communication Terre-Mars)
    - Présente les différents temps de latence en fonction des mécanismes de réplication utilisés
      - Avec λ pour les latences locales (jusqu'à 50ms) et Λ pour les latences inter-continentales (entre 100 et 300ms)
      - Consensus/Paxos : [Λ, 2Λ]
      - Primary-backyp : [λ, Λ]
      - Multi-master : λ
  - Présente une implémentation d'une séquence répliquée dans une des dernières slides
    - Mais ne parle pas de quelle implémentation il s'agit
  - D'après les ressources consultées, les CRDTs sont utilisés dans le cadre des /Conflict-free Replicated Databases (CRDB)/
  - L'idée est de proposer une base de données géo-répliquée, utilisant des CRDTs à la place des types de données traditionnels
    - Adopte le modèle de réplication /Multi-master/
    - Permet d'assurer la disponibilité des données et leur convergence
    - Tout en évitant un consensus qui serait coûteux dans ce type d'application répliquée à travers le monde
  - Aucune information n'est disponible concernant l'algorithme utilisé pour implémenter la séquence répliquée
    - Mais cette séquence ne semble proposer que les opérations d'ajout en tête ou d'ajout en queue d'éléments
    - Les cas d'utilisation semble être spécifiques, comme récupérer les évènements les plus récents ou mettre en place des queues
  - On a donc aucune information sur les potentielles limites de cette implémentation d'un point de vue performance
  - La vidéo *Building Geo-distributed Applications with Redis CRDBs* offre quelques informations supplémentaires
    - /CRDBs/ ne sont pas disponibles dans la version open-source
      - Se base effectivement sur les fonctionnalités de géo-réplication offertes dans la version entreprise
    - Propose un type *String* mais il s'agit du seul /CRDT/ qui se base alors sur /LWW/
  - Ressources
    - Active-Active Redis Enterprise
      - https://redislabs.com/landing/active-active/
      - https://www.youtube.com/watch?v=WHcvc0I24wA
    - Whitepaper Redis CRDTs
      - https://redislabs.com/docs/active-active-whitepaper/
    - Geo-Distributed Active-Active Redis Applications with Conflict-free Replicated Databases (CRDB)
      - https://redislabs.com/redis-enterprise-documentation/concepts-architecture/intercluster-replication/
    - Developing Applications with Geo-replicated CRDBs on Redis Enterprise Software (RS)
      - https://redislabs.com/redis-enterprise-documentation/developing/crdbs/
    - Developing with Lists in a CRDB
      - https://redislabs.com/redis-enterprise-documentation/developing/crdbs/developing-lists-crdb/
    - Webinars
      - https://redislabs.com/resources/webinars/past/?post_id=34753
      - https://redislabs.com/resources/webinars/past/?post_id=34762
- CRDT : Se renseigner sur les CRDTs présents dans *Akka*
  - *Akka* propose un module *Akka.Cluster*
  - Ce module permet de connecter un ensemble d'instances d'application *Akka* via un réseau P2P
  - Le but est de permettre le développement de systèmes décentralisés
  - En se basant sur *Akka.Cluster*, *Akka* propose *Akka.DistributedData* permettant de mettre en place un /distributed key-value store/
  - Les types de données utilisées par *Akka.DistributedData* sont des CRDTs
  - Ces CRDTs sont différentes implémentations de *Counter*, *Flag*, *Map*, *Register*, *Set*
    - http://getakka.net/articles/clustering/distributed-data.html#available-replicated-data-types
    - https://github.com/akka/akka/tree/master/akka-distributed-data/src/main/scala/akka/cluster/ddata
  - Absence de CRDTs pour gérer les séquences
  - Semblerait en outre que les CRDTs implémentés utilisent des tombstones
    - http://getakka.net/articles/clustering/distributed-data.html#tombstones
  - Mais un mécanisme de /pruning/ permet d'en supprimer une partie
    - À voir comment ce mécanisme détermine quelles données supprimer (stabilité causale de la suppression ?)
- Meeting
  - Had a meeting with Olivier et Gérald this morning
  - So had to prepare for it
  - One of the main feedback from the last one was about the motivation of the problem i'm adressing
    - Yeah, identifiers are growing but is it really a problem ?
  - So i worked on this topic
    - Read LSEQ, a paper proposing a new strategy to allocate identifiers to prevent from growing too fast
      - But no justification
    - Took a look at xRay
      - Experimental code editor by Atom team
      - Want to implement collaborative editing features
*** Planned
**** DONE CRDT : Regarder *RedisConf18: CRDTs and Redis—From Sequential to Concurrent Executions*
- https://www.youtube.com/watch?v=ZoMIzBM0nf4
**** DONE CRDT : Se renseigner sur les CRDTs présents dans *Akka*
- Semblerait que *Akka* utilise des CRDTs
- Se renseigner sur le sujet
** Semaine du <2018-07-09 Mon> au <2018-07-13 Fri>
*** Done
- PhD : Retour réunion suivi avec Olivier
  - Olivier m'a fait des retours concernant notre réunion de suivi, notamment à propos des motivations
  - Trouve que la motivation est encore floue et ne s'appuie pas assez sur des scénarios de l'industrie
  - Pense pourtant qu'il y a matière à avec Redis, Cassandra et autres
- PhD : Motiver le problème adressé
  - Envoi d'un mail à Baquero à propos de l'utilisation des /Sequence CRDTs/ dans /Redis CRDB/
  - Ce dernier ayant rejoint le /Redis Labs' Technical Advisory Board/, il me paraît bien placé pour répondre à mes différentes questions à ce sujet
    - https://redislabs.com/press/crdt-expert-carlos-baquero-joins-redis-labs-technical-advisory-board/
- CRDT : Se renseigner sur ElmSEQ
  - Propose de représenter le texte en utilisant une liste de positions
  - Un caractère peut être attribué à une position pour indiquer son emplacement dans le texte
  - Mais plusieurs caractères peuvent être assignés en concurrence à une même position
  - À ce moment, l'utilisateur se voit afficher le conflit et doit le résoudre
  - Les opérations ont l'air d'être prévues pour une séquence /element-wise/
    - Il faudrait donc résoudre les conflits élément par élément
    - À voir à l'usage si c'est fonctionnel et pratique
  - Permettrait p-e de gagner sur la taille des identifiants sur plusieurs niveaux en ne conservant que le /siteId/ et le /seq/ de l'auteur
*** Planned
**** DONE CRDT : Se renseigner sur ElmSEQ
- Présentation disponible ici : https://brightdb.com/elmeurope18/#frame3897
** Semaine du <2018-07-16 Mon> au <2018-07-20 Fri>
*** Done
- Algo :
  - Base de l'algorithmie
    - Instructions
    - Conditions
    - Itérations
    - Structures
    - Tableaux
  - But : convaincre de réfléchir à l'algo avant de programmer
  - Cours en pseudo-code
    - Les langages de programmation sont abordés dans les modules suivants
  - Entremêle cours et exercices
    - Dès que possible faire les exercices histoire de ne pas perdre leur attention
  - Insiste sur les règles
    - Bon formalisme
    - Bon choix du nombre de variables
  - Pas de documents pour les DS
    - Leur indiquer de bien revoir le cours et de finir les exercices si besoin d'une séance à l'autre
  - 2 DS
    - <2018-09-28 Fri> : 45min (1/3 de la note)
      - Portera sur les 4 1ers chapitres
      - Au bout de 4 semaines de cours
      - Devrait être corrigé en 1 semaine
    - <2018-10-26 Fri> : 1h15 (2/3 de la note)
*** Planned
** Semaine du <2018-08-15 Wed> au <2018-08-17 Fri>
*** Done
- SATIS 2018
  - Session 1. Distributed Computing – External Simplicity, Internal Complexity
    - *Dag Johansen*
    - Systèmes distribués imposent des problèmes de
      - Security
      - Consistency
      - Complexity
      - Fault-tolerance
    - Un trade-off entre fault-tolerance et performance est nécessaire
    - Cherche de nouveau cas d'utilisation pour les systèmes distribués
    - Se penche sur le monde du sport et spécifiquement sur celui du foot
    - Beaucoup de staff théorique dans les clubs behind the scene (PhD en physique, médecine, info)
    - Développe un système pour mesurer précisement la charge physique d'un entraînement sur un athlète (voir si un athlète est dans la forme physique pour un entraînement et si l'entraînement se déroule correctement)
      - Scalable, non-invasif, high availability, real-time
    - A utilisé des émetteurs radios pour suivre la position, la vitesse et l'accélération des athlètes en temps réel (meilleure précision que des GPS)
    - Dorénavant travaille sur du processing vidéo pour obtenir ces données
      - Le système mettait initialement 1s pour processer une image
      - Incompatible avec du temps réel
      - Essaie de distribuer la charge de travail pour améliorer le pipeline
      - Un noeud dans le stade capture la vidéo en local et effectue des traitements sur l'image
      - Avant de déléguer les calculs au cloud (AWS, Azure)
      - Arrive à atteindre le temps réel dans ces conditions
    - Travaille maintenant sur utiliser des technologies similaire pour effectuer du 360-panorama streaming video
    - Remarques
      - Capable d'atteindre le temps réel en déléguant le processing sur le cloud
      - En fonction de la connexion et de la latence avec le cloud, peut impacter les performances
      - Peut voir pour déployer des centres de calcul aux edges
  - Session 2. If You’re Not Writing a Program, Don’t Use a Programming Language.
    - *Leslie Lamport*
    - Fait la distinction entre algorithme et programme
      - Algorithme est le coeur du "programme"
      - Le "programme" contient des informations supplémentaires (types, assertions, exceptions)
    - Décrit algorithmes avec un ensemble de comportements (suite d'états)
    - Décrit un ensemble de comportements avec un prédicat sur l'état intial /InitE/ et un /next-state predicate NextE/ sur chaque étape /(si, si+1)/
    - /Safety property/ asserts what is allowed to happen
    - /Liveness property/ asserts what must eventually happen
    - Safety and liveness can be precisely defined and /any property = safety ^ liveness/
    - Un algorithme maintient tout au long de ses étapes un /inductive invariant/
      - Exemple du GCD : /GCD(M, N) = GCD(x, y)/
    - Encourage fortement à décrire mathématiquement son algorithme avec un outil tel que TLA+ pour vérifier sa correction
  - Session 3. Lamport’s Turing Award talk
    - *Leslie Lamport*
    - The Beginning of Concurrent System : Dijkstra paper circa 1970 about mutual exclusion
    - Need for rigorous proof in concurrent systems
    - Two arrows system
      - -> (solid arrow) : Happen-before relation
      - --> (dotted arrow): ?
      - Properties
	- A -> B -> C => A -> C
	- A !-> A
	- A -> B => A --> B
      - Temporal Logic is to reason about Liveness
      - Pseudo-code is obsolete, should use PlusCal instead
  - Session 4. Consistency vs Performance 1
    - *Lorenzo Alvisi*
    - Parle du trade-off entre Simplicité du système et Performance dans le cadre des bases de données distribuées
    - Associe ACID à la Simplicité
    - ANSI SQL 92
      - Feu vert signifie qu'on peut effectuer cette opération
    - /Dirty Reads/ signifie qu'on peut lire une valeur au cours d'une transaction concurrente avant que la transaction concurrente n'ait COMMIT ou ABORT
    - /Fuzzy Reads/ signifie qu'au cours d'une même transaction, pour une même clé, on lit différentes valeurs
    - /Phantom Menace : Non-repeatable predicate-based reads/ : pareil que /Fuzzy Reads/, mais on ne lit pas une valeur mais un ensemble de valeurs respectant un prédicat
    - "ANSI isolation levels should be intended to proscribe phenoma, not anomalies"
    - /Dirty Write/ signifie que des write concurrents peuvent être entrelacés (T1 modifie X, T2 modifie X puis Y et T1 modifie Y)
    - Présente une approche pour caractériser la seriability d'un ensemble de transactions à l'aide d'états
  - Session 5. Consistency vs Performance 2
    - *Lorenzo Alvisi*
    - Performance vs Complexity
    - Propose d'autoriser plus d'entrelacements des transactions (en découpant des transactions) afin d'améliorer les performances, au coût d'une augmentation de la complexité
    - Argumente que seules certaines transactions introduisent des problèmes de performance
    - Suggère donc de découper sélectivement ces transactions uniquement afin d'améliorer les performances tout en introduisant un minimum de complexité dans le système
    - Parle de SALT Isolation
    - Distingue les transactions ACID et BASE
    - Seules les BASE peuvent être décomposées et entrelacées entre elles
    - Tandis qu'une transaction ACID ne peut pas l'être et ne peut avoir lieu qu'avant ou après une transaction BASE concurrente
    - La difficulté porte maintenant sur BASE-ifying les transactions lourdes
    - A présenté un autre SGBD pour améliorer les performances sans augmenter la complexité (pas besoin de modifier le code et de BASE-ifier les transactions comme avec SALT): Callas
  - Session 6. Fail-Stop Replication 1
    - *Robbert van Renesse*
    - Replication is drived by motivation in fault tolerance and scalability
    - Highly influential RFC : P.R. Johnson and R.H. Thomas. Maintenance of duplicate databases. RFC 677, Jan. 1975
      - Utilise la règle du LWW en utilisant des local timestamps pour annoter les modifications, utilisant le siteId pour résoudre les conflits
    - System/Threats Model
      - Peut faire des hypothèses sur
	- Les communications
	  - Fair Links
	    - Un message est éventuellement délivré après un nombre infini de tentatives
	  - Perfect Checksums
	    - Un message correct reçu a bien été envoyé par un autre site
	- Timing (Synchrone/Asynchrone)
	  - No bounds on timing
	    - Message latency
	    - Speed at which clocks increase
	    - The difference between clocks
	- Défaillance
	  - Byzantine > Crash > Fail-Stop
	    - Crash: Sites sont honnêtes mais peuvent crasher à un moment donné
	    - Fail-Stop: Dans cette sous-catégorie des Crashs, le mécanisme de détection des défaillances est fiable
	  - Nombre de sites requis pour tolérer /f/ défaillances
	    - Byzantine : 3f + 1
	    - Crash: 2f + 1
	    - Fail-Stop: f + 1
	  - Dans le cadre d'un datacenter, le modèle /Fail-Stop/ est réaliste
      - System Model dans le cadre des travaux présentés : Fail-Stop, Asynchrone, FIFO Communication
    - Protocole
      - Deux noeuds, la /head/ et la /tail/
      - Lors d'une update, le client contacte la /head/ qui ajoute l'entrée à son log, puis transmet l'update à la /tail/. La /tail/ ajoute l'entrée à son log à son tour puis répond au client
	- L'update peut être joué ensuite de façon asynchrone, mais doit être joué avant la prochaine query
      - Lors d'une query, le client contacte la /tail/ qui effectue la query et retourne le résultat
      - Le log de la /tail/ correspond à l'historique stable
      - Si un crash se produit, le site restant devient à la fois la /head/ et la /tail/
      - Peut ajouter un noeud au système après une panne histoire de revenir à 2 noeuds
      - Un mécanisme de catch-up permet au nouveau noeud de récupérer l'état et de prendre le rôle de /head/
	- Le nouveau noeud peut crasher au cours du mécanisme de catch-up sans que la pose (trop) de problèmes
      - Une fois que le mécanisme de catch-up a fini, on retombe sur le système initial avec 2 noeuds
  - Session 7. Fail-Stop Replication 2
    - *Robbert van Renesse*
    - Pour généraliser à plus de 2 noeuds : Chain Replication
    - Le problème dans ce scénario concerne la /Failure Recovery/
      - Les noeuds restants ont besoin de se coordonner puisque
	- Ils n'ont pas forcément détecter les défaillances dans le même ordre
	- Plusieurs noeuds peuvent essayer de joindre en concurrence
      - Ajoute des opérations spéciales pour la reconfiguration
	- /addReplica (replicaId)/
	  - Permet à un site de rejoindre et de devenir la /head/
	- /removeReplica (replicaId)/
	- /resumeAsHead (replicaId)/
      - La configuration peut donc être déterminée à partir de ces opérations dans le log lorsqu'elles sont stables (dans le log de la /tail/)
    - Remarque
      - La latence peut être élevée puisqu'une update doit parcourir toute la chaîne afin d'arriver à la /tail/ et être joué pour pouvoir être observé par les /queries/
    - Questions
      - Pourquoi la lecture ne se fait que la /tail/ et non pas de façon aléatoire entre la /head/ et la /tail/ ?
	- Car potentiellement certaines updates vues par la /head/ ne sont pas stables et peuvent mener à des résultats étranges si la /head/ vient à crasher
      - Lorsqu'on ajoute des noeuds, est-ce que seul le groupe de /tails/ augmente ou pouvons-nous avoir plusieurs /heads/ ?
	- Les deux : le 1er noeud joue le rôle de /head/, le dernier celui de /tail/ et ceux du milieu à la fois /head/ et /tail/
  - Session 8. Byzantine Consensus to Blockchains 1
    - *Christian Cachin*
  - Session 11. Security from Tags 1
    - *Fred Schneider*
    - Propose d'enforcer la sécurité et les safety properties d'un système non pas sur les opérations mais sur les valeurs
    - Approche existe depuis 1975
    - Pourquoi cette méthode d'enforcer la sécurité en attachant des tags sur les valeurs est sous-utilisée actuellement ?
    - Enforcement: Access Control
      - Utilise l'identité de l'utilisateur pour déterminer si iel a accès aux /containers/ ou aux /contents/
      - /Content-based Access Control/ semble préférable au /Container-based Access Control/ (E2E Access Control, moins error-prone)
    - Quel tag donner aux valeurs retournées par /op(x1, ..., xn)/ ?
      - Peut dépendre sur les tags associés à /x1, ..., xn/, la sémantique de /op/, les valeurs de /x1, ..., xn/
      - Prior work: Le résultat doit au moins être aussi confidentiel que l'union des tags associés aux valeurs
    - Mais cela est incompatible avec certaines applications
      - Vote électronique : chaque vote est secret, mais le résultat doit être public
      - Chiffrement : la valeur est secrète, mais la valeur chiffrée peut être public
    - Il est donc nécessaire d'associer à chaque opération une fonction permettant d'obtenir le tag du résultat
      - Fonctionnel, mais demande donc de définir une fonction pour chaque opération, ce qui peut s'avérer lourd
    - Peut définir une fonction universelle permettant d'obtenir un nouveau tag à partir de valeurs
      - Difficile d'établir une telle fonction
    - Ou peut enrichir les tags pour qu'ils définissent les contraintes sur les valeurs, mais aussi les contraintes sur les valeurs dérivées à partir d'opérations
      - Déplace donc la responsabilité non plus sur le développeur des opérations, mais sur le concepteur des tags
      - Semble être la solution la plus pratique
    - Tags forment alors un /join semi-lattice/
  - Session 13. Keidar: Reliable Distributed Storage 1
    - *Idit Keidar*
*** Planned
**** CANCELLED SATIS : Faire un poster
     DEADLINE: <2018-08-02 Thu>
- Peut proposer un poster pour SATIS
- Établir le plan
- Le faire valider par Gérald et Olivier
- Rédiger le poster
** Semaine du <2018-08-21 Tue> au <2018-08-24 Fri>
*** Done
- PhD : Motiver le problème adressé
  - Voir pour relancer M. Baquero
- MUTE : Implémenter le mécanisme de renommage
  - Pour faire le point, on a implémenté actuellement la version suivante
    - Un des pairs effectue un /renameLocal()/ et partage l'opération
    - Les autres pairs, en reçevant l'opération de renommage, applique le renommage sur leur copie
    - L'application d'opérations de renommage concurrentes n'est pas gérée
  - Ce qu'il reste à faire
    - [ ] À la réception d'une opération, déterminer le "chemin" entre les /epochs/ de l'opération et de l'état courant
    - [ ] Appliquer les transformations dans l'ordre
    - [ ] Écrire les tests correspondants
- Méthode formelle : Se remettre à niveau en méthode formelle
  - A lu https://learntla.com/
  - Exemple de code
    #+BEGIN_SRC
---- MODULE Transfer ----
EXTENDS Naturals, TLC // Imports de packages

(* --algorithm transfer  // Le programme est écrit en PlusCal, qui est ensuite transpilé en TLA+
variables alice_account = 10, bob_account = 10, money \in 1..20, // Définitions de variables, variables pouvant itérer sur un ensemble
  account_total = alice_account + bob_account;

begin
Transfer:
  if alice_account >= money then
  A: alice_account := alice_account - money;  // Définition d'une transaction
  bob_account := bob_account + money; \* Both now part of A
end if;
C: assert alice_account >= 0;

end algorithm *)


// Le code TLA+ généré est placé ici


MoneyNotNegative == money >= 0  // Définitions d'invariants du système
MoneyInvariant == alice_account + bob_account = account_total
====
  #+END_SRC
  - Représentation du modèle
    - Plusieurs instances d'un process
    - Chaque instance possède un état composé de
      - Un /siteId/
      - Une /clock/
      - Une séquence /list/ de tuples <id, elements>
      - Un vecteur d'état correspondant /stateVector/
      - Une /epoch/ courante
      - Une arborescence des /epoches/
      - Une liste des opérations distantes en attente /remoteOps/ (?)
    - À chaque étape, un site peut effectuer
      - Une insertion locale
	- Trouve la position correspondante dans la séquence
	- Génère un identifiant correspondant
	- Insère le tuple <id, elt> dans la séquence
      - Une suppression locale
	- Trouve la position correspondante dans la séquence
	- Récupère les identifiants correspondants
	- Supprime les tuples concernés de la séquence
      - Un renommage local
	- Répertorie les identifiants présents dans l'état actuel
	- Génère un nouvel interval d'identifiants
	- Crée une nouvelle séquence
	- Insère l'interval d'identifiants avec les éléments dans la nouvelle séquence
	- Remplace la séquence actuelle par la nouvelle
	- Incrémente l'epoch courante
      - Une opération distante
	- Insertion distante
	- Suppression distante
	- Renommage distant
  - Invariants
    - Convergence : avoir des vecteurs d'états équivalents implique d'avoir des états équivalents
    - Espace dense : je peux toujours générer un nouvel identifiant entre 2 autres
    - /rename/ ne modifie pas l'ordre : une séquence est toujours triée après que chacun de ses éléments ait été /rename/
      - Une séquence est triée si pour chaque élément, tous les éléments précédents ont un identifiant plus petit
    - /reverseRename/ ne modifie pas l'ordre : une séquence est toujours triée après que chacun de ses éléments ait été /reverseRename/
  - Questions
    - Comment modéliser un /message passing system/ en TLA+ ?
      - La livraison des messages aux process doit respecter un certain ordre
	- FIFO
	- Suppression d'un élément après son insertion
      - Mais en dehors de ces contraintes, chaque process peut observer les messages dans un ordre différent
      - Comment les instances communiquent entre-elles ?
    - [X] Est-ce que le caractère non-borné des états et des identifiants posent problème pour la modélisation ?
      - Si l'on met aucune limite à la taille de la séquence ou à celle des éléments, est-ce que ça pose problème à TLC ?
      - Si l'on met une limite, est-ce que l'on ne va pas provoquer des erreurs liées à cette dernière ?
	- À voir comment on définit les invariants et comment on instaure cette limite
	- Mais par exemple, empêcher d'effectuer une insertion locale grâce à un /await/ ne va pas à l'encontre de l'invariant *je peux toujours insérer un nouvel élément entre 2 autres*
    - [X] Est-ce qu'on peut limiter le nombre de pas ?
      - Genre 50 actions puis le système s'arrête
      - Oui, dans les /Advanced Options/ du modèle, on peut indiquer la longueur maximale de la trace
    - Est-ce que je dois spécifier et vérifier les invariants de *LogootSplit* ou seulement ceux du mécanisme de renommage ?
      - Par exemple celui concernant l'espace dense ne concerne que *LogootSplit*
  - Réunion avec Stephan Merz prévue le <2018-09-04 Tue 10:00>
  - Stephan m'a recommandé de lire *The TLA+ Hyperbook* (http://lamport.azurewebsites.net/tla/hyperbook.html) pour poursuivre mon apprentissage de TLA+
*** Planned
** Semaine du <2018-08-27 Mon> au <2018-08-31 Fri>
*** Done
- VTSA 2018 : Distributed Synthesis
  - Synthesis : construit un programme qui correspond à une/des propriétés
  - /Closed synthesis/ : pas d'environnement
  - /Open synthesis/ : le système est /réactif/, il évolue /contre/ l'environnement
  - Cas d'utilisation de /open synthesis/
    - Doit développer un module appartenant à un large système
    - Peut considérer le reste du système comme l'environnement
    - Le module doit se comporter correctement dans tous les scénarios d'interactions possible
  - /Model-checking linear-time properties (LTL, MSO) requires automata @ logic over infinite words/
- MUTE : Implémenter le mécanisme de renommage
  - Détection d'un problème avec la fonction /reverseRenameId()/ dans le cas suivant
    - On a inséré un identifiant /id/ en concurrence au renommage tel que /newFirstId < id < firstId/
      - Exemple avec /newFirstId = <1, -1, 1, 0>, id = <1, 1, 1, 0>, firstId = <1, 1, 1, 1>/
    - /id' = <1, -1, 1, -1><1, 1, 1, 0>/
    - Dans ce scénario, on a
      - /reverseRenameId(renameId(id)) /= id/
      - /<1, -1, 1, -1><1, 1, 1, 0> /= <1, 1, 1, 0>/
    - Ajout du test unitaire correspondant
    - Ré-écriture de ce cas
      - Considère qu'on ne peut avoir /closestPredecessorOfNewFirstId < id' < newFirstId/ seulement si /newFirstId < firstId/
      - /id'/ est donc forcément de la forme /closestPredecessorOfNewFirstId + tail/
      - Différents cas
	1. Si /random(tail) === random(firstId)/, renvoie /tail/
	   - Permet de couvrir l'ensemble des identifiants ayant pour position /random(firstId)/ jusqu'à /closestPredecessorOfFirstId/ (compris)
	   - Tout en préservant l'ordre existant entre ces identifiants
	2. Si /closestPredecessorOfFirstId < tail/, renvoie /closestPredecessorOfFirstId + tail/
	   - Permet de conserver l'ordre /closestPredecessorOfFirstId < tail/
	3. Renvoie /id/
	   - Dans ce cas, /id < closestPredecessorOfFirstId/
      - On a donc bien /ids3 < ids1 < closestPredecessorOfFirstId < ids2 < firstId/
- Algo : Préparer les slides pour la section /Instructions et Types Élementaires/
  - Durée estimée à 4h
  - Correspond aux pages 3 à 8 du poly de cours enseignant
  - Correspond à la page 2 du poly d'exercices enseignant
  - Plan
    - Se présenter
    - Présenter les règles
      - Appel obligatoire tous les cours
      - 5min de retard toléré
      - Encourage à travailler en binôme dans le cadre des exercices
    - Demander qui a déjà programmé
    - Présenter le module
      - Organisation
	- Volume horaire de 30h de début septembre à fin octobre
	- Enseignement intégré (CM + TD fusionnés en 1 seule séance)
	- 2 partiels pour l'évaluation (1/3 et 2/3 de la note)
      - Objectifs
	- Notion d'algorithme
	- Concevoir un algorithme
	- Dérouler un algorithme
	- Constituer une culture algorithmique
      - Lien avec le module de programmation
	- Dans le module d'algorithmie, on va réfléchir et concevoir des algorithmes
	- Que vous allez implémenter et faire exécuter par la machine dans le module de programmation
    - Notion d'algorithme
      - Définition: /Suite d'instructions à suivre pour réaliser une tâche/
      - Algorithmes dans la vie de tous les jours
	- Certains sont apparents : GIF d'un JV
	- D'autres sont plus subtils : Capture des résultats Google ou d'une timeline Twitter
	- Mais on peut rencontrer des algorithmes en dehors de l'informatique : Recette des crêpes
      - Algorithme /= Programme
	- Un algorithme correspond à la manière d'effectuer une tâche
	- Un programme est une traduction, une adaptation d'un algorithme pour qu'il soit compris par l'ordinateur
	- On parle d'/implémentation/
	- On peut avoir différentes implémentations d'un même algorithme
	  - Différents langages de programmation
	  - Différentes façons d'implémenter un algorithme pour un même langage de programmation
    - Exemple d'algorithme
- Méthode formelle : Se remettre à niveau en méthode formelle
  - Construction du graphe des états
    1. We start by setting G to the set of all possible initial states of behaviors, which we find by computing all possible assignments of values to variables that make the initial predicate true.
    2. For every state s in G
       1. We compute as follows all possible states t such that s → t can be a step in a behavior.
       2. We substitute the values assigned to variables by s for the unprimed variables in the next-state action
       3. Compute all possible assignments of values to the primed variables that make the next-state action true.
    3. For every state t found in step 2
       1. We add t to G if it is not already in G
       2. We draw an edge from s to t.
    4. We repeat steps 2 and 3 until no new states or edges can be added to G.
  - Lors de la vérification d'un modèle avec TLC, dans la partie statistiques
    - /Diameter/ The number of states in the longest path of G in which no state appears twice.
    - /States Found/ The total number of (not necessarily distinct) states it examined in step 1 or as successor states t in step 2.
    - /Distinct States/ The number of states that form the set of nodes of G.
    - /Queue Size/ The number of states s in G for which step 2 has not yet been performed.
  - LET ... IN ... permet de déclarer des variables temporaires
  - L'équivalent en *PlusCal* est /with (...) { ... }/
  - Questions
    - À quoi correspond une ligne dans les statistiques de TLC ?
    - Pourquoi /Queue Size/ serait différent de 0 dans le cas d'une spécification correcte ?
    - Déjà exécuter TLC en mode distribué ?
      - De quel manière ? Ad-hoc ou Cloud ?
      - Des retours à faire ?
*** Planned
** Semaine du <2018-09-03 Mon> au <2018-09-07 Fri>
*** Done
- Méthode formelle : Se remettre à niveau en méthode formelle
  - Récap questions
    - À quoi correspond une ligne dans les statistiques de TLC ?
    - Pourquoi /Queue Size/ serait différent de 0 dans le cas d'une spécification correcte ?
    - Est-ce que le caractère non-borné des états et des identifiants posent problème pour la modélisation ? Comment s'en sortir ?
    - Comment modéliser un /message passing system/ en TLA+ ?
    - Déjà exécuter TLC en mode distribué ?
    - Utiliser des invariants ou des asserts ?
  - Réunion
    - Model Checker != preuve
    - Permet de trouver des contres-exemples
    - Dois tout borner
      - Peut utiliser /Advanced Options > State Constraints/
    - Utiliser TLC le plus possible pour détecter les cas problématiques
    - Avant de se pencher sur une preuve si besoin
    - Possibilité d'être mis en lien avec quelqu'un qui a fait tourné TLC sur Grid5000
    - Possibilité d'implémenter l'ordre aléatoire des messages avec des ensembles
    - Statistics sert surtout à détecter une incohérence au niveau du système
      - Si j'ai pas trouvé d'erreurs mais que je n'ai pas exploré beaucoup d'états, y a p-e un problème sur Next
    - Ne pas trop jouer avec les TLC Options, plutôt définir des contraintes sur mes valeurs possibles du système
    - Simulation mode permet de demander à TLC d'essayer un chemin aléatoire (seulement un)
  - Poursuite de la lecture de l'hyperbook
    - Je ne comprends pas comment vérifier la terminaison d'un algo avec TLA+
    - Méthode à suivre
      - Avec /I/ un invariant de notre système et /Next/ l'action permettant de passer d'un état donné au suivant
      - Définit une /integer-valued state function W/ pour laquelle
	1. /I => (W ∈ Nat) ∨ (pc = "Done")/ pour tout état atteignable non-final
	2. /I ∧ Next => (W > W') ∨ (pc = "Done")/
    - Messages d'erreurs obtenus
      - The property L2 is not correctly defined.
      - The property Next is not correctly defined.
    - Définition de /weak fairness/
      - La /weak fairness/, notée /WF_var(A)/, stipule que pour une action arbitraire /A/ nous avons les 2 conditions suivantes
	1. The behavior does not end in a state s_n in which A is enabled.
	   - Le système ne s'arrête pas tant qu'il peut progresser
	   - Prévient le système de s'arrêter inopinément
	2. If the behavior is infinite, then there is no n such that the infinite behavior s_n -> s_n+1 -> ··· has no A steps but A is enabled in all of its states.
	   - Si le comportement est infini, il n'existe pas chaîne infinie d'états n'ayant pas effectué d'action /A/ alors que l'action /A/ est disponible pour chaque état
	   - Force le système à alterner entre l'ensemble des actions possibles
    - Définition de /strong fairness/
      - La /strong fairness/, notée /SF_var(A)/, remplace la condition 2. de la /weak fairness/ par la suivante
	- If the behavior is infinite, then there is no n such that the infinite behavior s_n -> s_n+1 -> ··· has no A steps but A is enabled in infinitely many of its states.
      - La différence se place sur l'action /A/ qui ne doit plus être disponible pour chaque état de la chaîne infinie mais /juste/ pour un nombre infini d'états
*** Planned
** Semaine du <2018-09-10 Mon> au <2018-09-14 Fri>
*** Done
- Méthode formelle : Se remettre à niveau en méthode formelle
  - Poursuite de la lecture de l'hyperbook
    - /Refinement mapping/
      #+BEGIN_QUOTE
A refinement mapping from a specification H to a specification A is an assignment of an expression v to each variable v of A, where v is defined in terms of the variables of H .
This refinement mapping defines, for each state s of H, the state s of A that assigns to each variable v of A the value of v in state s.
If σ is the behaviors 1 →s2 → ... of H, we define the behavior σ of A to be s1 → s2 → ... .
We say that H implements A under this refinement mapping iff, for each behavior σ satisfying the behavior specification of H , the behavior σ satisfies the behavior specification of A.
      #+END_QUOTE
- Réunion suivi
  - A contacté Baquero
    - Pas de réponse pour le moment
    - Vais voir pour faire un mail de relance
  - Faudrait que je rejette un oeil à xray
    - Voir comment le projet a évolué
  - Réinscription
    - Voir convention individuelle de formation
  - Remarques
    - Motivations
      - Recontacter Redis
    - Arborescence, choix de l'epoch gagnante
      - Voir si on peut réfléchir à un mécanisme plus intelligent que l'ordre lexicographique pour limiter le nombre d'/undo/
      - Voir comment ils ont fait dans les /blockchains/
    - Correction des fonctions de ré-écritures
      - Voir ce que Gérald a formalisé dans sa thèse et la façon dont c'est fait
      - Se renseigner sur les /theorem provers/
      - Mettre au clair les propriétés que l'on veut que les fonctions de ré-écritures respectent
    - Gain
      - Voir pour continuer sur la piste des simulations
      - Voir si on peut estimer le gain de façon analytique à l'aide de probabilités
    - Middleware
      - Voir pour contacter et obtenir un use case de Redis au plus tôt
      - Voir pour écrire une proposition pour le doctoral symposium de la conf
- PhD : Motiver le problème adressé
  - Redis
    - A envoyé un mail de relance à Baquero
    - Eu une réponse
      - Il a forwardé le mail à l'équipe technique de Redis
      - Mais Redis CRDB est un produit commercial, il faut s'attendre à ne pas avoir de réponse
    - Gérald a trouvé les slides suivantes
      - https://www.slideshare.net/RedisLabs/redisconf18-activeactive-geodistributed-apps-with-redis-crdts-conflict-free-replicated-datatypes
    - Leurs CRDTs utilisent des tombstones
    - Voir quels sont les mécanismes de garbage collection existants pour ce type de CRDT, leurs propriétés et leurs performances
  - IPFS
    - Suis retombé sur cette issue, *CRDT: Garbage Collection* : https://github.com/ipfs/dynamic-data-and-capabilities/issues/2
    - Étudier la discussion pour y trouver des points d'intérêts
- Biblio
  - Je suis retombé sur la page du projet *Lightkone* : https://www.lightkone.eu/index.php/publications/
  - Les travaux suivants ont l'air intéressants
    - *Aggregation Protocols in Light of Reliable Communication* : https://arxiv.org/pdf/1801.07005.pdf
    - *Borrowing an Identity for a Distributed Counter* : https://wiki.lightkone.eu/pmwiki/uploads/Main/borrow-counter.pdf
      - Slides : https://wiki.lightkone.eu/pmwiki/uploads/Main/borrow-counter-slides.pdf
*** Planned
** Semaine du <2018-09-17 Mon> au <2018-09-21 Fri>
*** Done
- PhD : Motiver le problème adressé
  - XRay
    - L'issue mentionnant la taille des identifiants comme problème n'a pas évoluée : https://github.com/atom/xray/issues/24
- MUTE : Implémenter le mécanisme de renommage
  - En présentant le mécanisme de renommage à Weihai, j'ai identifié un cas problèmatique
  - Si on doit annuler un renommage et que nous avons /id1/ et /id2/ de telle façon que
    - /id1/ dépendant causalement du renommage et /newLastId < id1 < lastId/
    - /id2/ inséré de façon concurrente au renommage et /id2 = lastId + tail/ avec /tail < id1/
  - Après avoir annulé le renommage, on va avoir
    - /id1' = lastId + id1/
    - /id2' = id2/
  - On a donc /id2' < id1'/ au lieu de /id1' < id2'/
  - Pour corriger ce scénario, il faut revoir comment on génère /id1'/
    - Une solution est d'introduire un nouveau tuple /min/
    - Et générer /id1'/ tel que /id1' = lastId + min + id1/
    - Et exclure /min/ de l'algorithme de génération des identifiants
    - Ceci permet d'empêcher les utilisateurs de générer des identifiants en utilisant cette valeur
    - Ainsi, tout identifiant /id2/ généré par une action utilisateur tel que /lastId < id2/ respectera la condition suivante
      - /id1' < id2/
- Enseignement : Réunion pédagogique LP CIASIE
  - 28 étudiants pour le groupe avec lequel j'interviens
  - Pas mal d'étudiants venant de DUT, mais d'autres IUT
  - Potentiellement des lacunes et des inégalités de niveaux à gérer
  - Voir avec Gilles pour se mettre d'accord sur le nombre de notes pour le module de programmation web sur le client
    - Faut voir pour l'intitulé des notes et leurs coefficients
    - Une note de contrôle continu (un TP noté ?), une note de projet ?
  - Participation de la matière dans le cadre de l'atelier 2
  - Opportunité d'encadrer des alternants (compte pour 5h par contre)
- PhD : Se réinscrire auprès de l'école doctorale IAEM
  - Complétion et mise en ligne de la convention individuelle de formation de cette année
  - Envoi de l'email à Vanessa BINET pour obtenir un lien pour compléter la réinscription
  - Complétion de l'inscription
  - Reste à récupérer les certificats de scolarité
*** Planned
** Semaine du <2018-09-24 Mon> au <2018-09-28 Fri>
*** Done
- PhD : Se réinscrire auprès de l'école doctorale IAEM
  - J'ai récupéré les certificats de scolarité
  - Envoi du certificat de scolarité à l'IUT
- PhD : Compléter et envoyer l'autorisation de cumul à l'IUT
  - Il faut bien que la composante signe l'autorisation de cumul préalablement pour que l'école doctorale accepte de signer l'autorisation de cumul
  - Renvoi de l'autorisation de cumul à l'IUT pour signature
- IUT : Se concerter avec Gilles pour déterminer le nombre de notes dans le module de prog web client
  - Envoi d'un mail à Gilles avec une proposition de 2 notes
    - Note de contrôle continu
      - Peut être obtenue différement en fonction des groupes
      - À l'aide d'un ou plusieurs TPs notés, de tests écrits
    - Note de projet
*** Planned
**** DONE PhD : Se réinscrire auprès de l'école doctorale IAEM
** Semaine du <2018-10-01 Mon> au <2018-10-05 Fri>
*** Done
- IUT : Préparer le cours de prog web
  - Planning des séances
    - <2018-10-02 Tue>
      - Cours
	- Bases du langage
	- Bases des objets
      - Exos
	- Spécification du langage
    - <2018-10-09 Tue>
      - Cours
	- Closures, IIFE, Namespace, Module
	- Prototype
	- Classe
      - Exos
	- Bases des objets
	- Closure
	- Prototype
	- Classe
    - <2018-10-16 Tue>
      - Cours
	- Les évènements
	- Le DOM
	- jQuery
      - Exos
	- TP Noté ?
	  - Application jQuery respectant le pattern Module et devant forcer l'usage de classes ?
    - <2018-10-23 Tue>
      - Cours
	- Node
	- Modules (CommonJS -> ESM)
	- Webpack
      - Exos
	- TBD
    - <2018-11-20 Tue>
      - Cours
	- TBD
      - Exos
	- Projet
    - <2018-11-27 Tue>
      - Cours
	- TBD
      - Exos
	- TBD
      - Soutenance projets
  - Notions restantes à aborder
    - Code asynchrone
    - Typescript
    - Progressive Web App
  - Séance du <2018-10-02 Tue>
    - Exos
      - Spécification du langage
	- [X] Ajouter des tests liés à la déclarations de variables
	  - /let/
	  - /const/
	- [X] Ajouter des tests liés au parcours de tableaux
	  - Simple boucle /for/
	  - Boucle /for ... in/
	  - Méthode /forEach()/
	- Ajouter des tests avancés
	  - [X] /filter()/
	  - [X] /map()/
	  - [ ] /reduce()/
    - Notes
      - Certains tests échouent de façon aléatoire avec la dernière version de Jasmine si l'ordre aléatoire d'exécution des tests est activé
	- Notamment ceux ayant trait au /scope global/
      - Le test qui montre comment on capture une exception est incorrect
      - Le test sur l'exécution d'une RegExp échoue pour une raison inconnue
- IUT : Correction du partiel d'algo
  - Question 1
    - Ajout d'un paramètre inutile à la fonction
    - coef en tant que paramètre /InOut/ au lieu de juste le retourner
    - Utilise /lire()/ pour obtenir la valeur des paramètres
    - Plusieurs /retourne/
    - Pas de sortie (pas de /retourne/, pas de paramètre modifiable)
    - Des /ecrire(...)/ au lieu de /retourne/
    - Au lieu de juste calculer le /coef/, multiplie le /score/ passé en paramètre par le /coef/ et retourne le résultat
  - Question 2
    - Lit le nom, prénom ou temps à chaque itération imbriquée et non globalement
    - Ne conserve pas et n'affiche pas les informations liées au meilleur résultat
    - Ne remet pas à 0 le score final d'un participant à chaque itération
    - Ne boucle pas pour lire les différentes notes
  - Divers
    - Pour affecter la valeur 1 à une variable, utilise /toto <- lire(1)/
    - Noms de variables pas clairs
    - Évalue l'expression booléenne /v <= d <= q/
    - Utilise && ou || au lieu de /et/ et /ou/
    - Utilisation du signe = au lieu de <-
    - Soin de la copie
- IUT : Se concerter avec Gilles pour déterminer le nombre de notes dans le module de prog web client
  - Mail envoyé à Amine
- PhD : Compléter et envoyer l'autorisation de cumul à l'IUT
  - Autorisation de cumul signée par l'IUT
  - Je l'ai transféré à Sabrine Ferry pour signature de l'école doctorale et du président de l'université
- MUTE : Corriger l'inversion du renommage dans le scénario
  - Définition de nouvelles valeurs, *INT32_BOTTOM_USER* et *INT32_TOP_USER* de façon à réserver *INT32_BOTTOM* et *INT32_TOP* pour le mécanisme de renommage
  - Modification de /createBetweenPosition()/ pour utiliser les valeurs *USER* à la place des valeurs systèmes
  - Modification de /reverseRenamingId()/ pour gérer le cas problématique
*** Planned
**** DONE IUT : Se concerter avec Gilles pour déterminer le nombre de notes dans le module de prog web client
**** DONE MUTE : Corriger l'inversion du renommage dans le scénario
** Semaine du <2018-10-08 Mon> au <2018-10-12 Fri>
*** Done
- IUT : Préparer le cours de prog web de <2018-10-09 Tue>
  - Cours
    - Refaire un rappel sur les objets
    - Les fonctions
    - Les arguments des fonctions
    - /this/ dans les fonctions
    - Closures, IIFE, Namespace, Module
  - Exercices
    - Égalités, fonctions et closures
  - Mettre en place une /GitHub Classroom/
    - Disponible ici : https://classroom.github.com/classrooms/43989904-teaching-mnicolas/assignments/lp-ciasie-prog-web-td2
    - Pour chaque étudiant, une copie du repo d'origine est réalisée
    - Mais le repo d'origine n'est pas par défaut dans les repos distants de leur copie
    - Il faut donc ajouter "à la main" le repo pour qu'ils puissent récupérer les modifications suivantes
    - Veiller à mettre à disposition les URLs et les commandes nécessaires à l'ajout du repo distant et la récupération des derniers commits
- MUTE : Réflexion sur la simulation
  - Si on évalue le mécanisme de renommage en déployant un ensemble de bots
  - Ça veut dire qu'on déploie plusieurs instances de /node/ qui vont collaborer ensemble
  - Car le bot tourne sous /node/, il n'a pas accès à /WebRTC/
  - Vérifier si les bots vont créer des connexions /WebSocket/ entre eux
  - Ou s'ils vont tous communiquer via le /TURN Server/
    - Auquel cas le /TURN Server/ peut s'avérer incarner le /bottleneck/ de notre simulation
  - Ne faudrait-il pas remplacer la couche réseau dans le cadre de l'expérience ?
*** Planned
** Semaine du <2018-10-15 Mon> au <2018-10-19 Fri>
*** Done
- MUTE : Implémenter le mécanisme de renommage
  - Ajout d'une méthode /getEpochPath()/ pour récupérer le chemin complet d'une /epoch/
  - Ajout d'une méthode /getPathBetweenEpochs()/ pour récupérer la liste des /epochs/ à traverser (à annuler ou à appliquer) pour passer d'une /epoch/ à une autre
  - Modification de /renameRemote()/ pour
    - Récupère le chemin entre l'ancienne /epoch/ et la nouvelle
    - Récupère la liste des identifiants actuellement présents dans la structure de données
    - Applique les inversions de renommage puis les renommages successifs sur la liste des identifiants
    - Remplace l'état par ce nouvel état renommé
*** Planned
**** DONE MUTE : Implémenter le mécanisme de renommage
** Semaine du <2018-11-12 Mon> au <2018-11-16 Fri>
*** Done
- DISTRIBUTED SYSTEMS : Regarder *Liberating Distributed Consensus*
  - Modèle du système
    - Réseau asynchrone, non-fiable
    - Noeuds non-byzantins
    - Aucune mention du groupe, s'il est statique ou dynamique
  - Propose une généralisation de l'algorithme de consensus
  - Dans le cadre d'un consensus, les clients vont proposer leurs valeurs aux différents serveurs au cours d'epoch successives jusqu'à atteindre une décision
  - Une fois une valeur choisie par un serveur pour une epoch donnée, celle-ci est immutable
  - Les valeurs choisies sont conservées par les serveurs dans une table des états
  - Les clients conservent eux une table globale des états des serveurs d'après les informations qu'ils collectent au fur et à mesure
  - Ceci va permettre aux clients de prendre des décisions de manière informé et d'accélérer le mécanisme de consensus dans certains cas de figures
  - Définit des règles pour s'assurer qu'on aboutit à une décision
  - *Epoch Allocation Rule*
    - Exclusive value
      - Peut limiter le nombre de valeurs différentes proposées par epoch à 1
      - Par exemple en attribuant le droit d'écrire à un seul client par epoch via un round robin
    - Non-exclusive values
      - Dans le cas contraire où on autorise plusieurs valeurs différentes à être proposées en concurrence par les différents clients
      - Il est nécessaire que les quorums d'une epoch s'entrecroisent pour qu'une décision soit validée de manière sûre
  - *Client Write Rule*
    - Afin d'éviter qu'on obtienne des valeurs différentes pour une décision d'une epoch à l'autre, on ajoute des contraintes sur les valeurs proposées par les clients
    - Avant de proposer une valeur /v/ à une epoch /e/, un client doit s'assurer que
      - Aucune décision n'a été atteinte de l'epoch 0 à l'epoch /e-1/
	- Auquel cas le client est libre de soumettre n'importe quelle valeur
      - Toute décision qui a été de l'epoch 0 à l'epoch /e-1/ a pour valeur /v/
  - Propose ensuite de relâcher certains requirements notamment de Paxos pour atteindre une décision de manière plus efficace
  - *Value Selection*
    - Relâche la contrainte sur la valeur proposée par un client en phase 2
    - Dans Paxos, le client doit proposer la valeur associée à la plus grande epoch retournée par un serveur au cours de la phase 1
    - Ici, un client a pour obligation de proposer une valeur retournée par un des serveurs si cette valeur peut avoir été choisie par un quorum d'après les informations de la table des états
  - Finalement, propose des compromis/variantes d'algo de consens pour obtenir de meilleures performances dans des cas donnés
  - Une variante intéressante est le *binary consensus*
    - L'idée est d'associer exclusivement des valeurs aux epochs
      - Les clients doivent alors attendre une epoch donnée pour pouvoir proposer une valeur spécifique
    - Comme l'epoch 0 n'a pas d'epoch précédente, on peut optimiser le consensus en skippant la phase 1 de préparation
      - Aucune valeur n'a pu être encore choisi, donc on peut directement proposer une valeur
    - En associant une valeur commune/prioritaire de notre système à cette epoch, le consensus peut être effectuer plus rapidement (1 RTT)
      - Exemple de valeur pour l'epoch 0 : COMMIT une transaction
    - Pour les epochs suivantes, on peut retomber sur du Basic Paxos
  - Remarques
    - Terminaison de l'algo: quand est-ce que le mécanisme s'arrête ?
    - Comment la /configuration table/ est-elle établie et communiquée aux noeuds ?
    - Est-ce que la /configuration table/ peut être modifiée en cours d'exécution ?
  - *Slide 65*: Pas compris pourquoi l'optimisation donnée peut être effectuée
    - Si j'ai bien compris, explique que dans Paxos on attend une majorité de réponses des serveurs à la phase 1 pour déterminer la valeur /v/ à proposer à la phase 2
    - Ici, explique qu'on est pas obligé d'attendre autant de réponses
    - À partir du moment où une réponse comporte une valeur /v/ telle que /v ≠ nil/, peut proposer /v/
    - Affirme que seule la valeur retournée aurait pu être décidée dans les epochs précédentes
    - Je ne comprends pas pourquoi la réponse reçue ne pourrait pas être une valeur différente que la valeur choisie à l'epoch précédente
      - À voir le quorum requis pour cette optimisation
- DISTRIBUTED SYSTEMS : Regarder *Is there anybody out there?*
  - Key issues for building clusters
    - *Discovery*: who's there ?
    - *Fault Detection*: who's is in trouble ?
    - *Load Balancing*: who can take up work ?
  - *Group Membership* permet d'adresser l'ensemble de ces problèmes
  - Pour implémenter *Group Membership*, a besoin de 3 composants
    - *Failure Detection*: détecter qu'un noeud est en panne
    - *Dissemination*: transmettre l'information aux autres noeuds du système
    - *Consensus*: se mettre d'accord sur l'état du système malgré des informations potentiellement contraires
  - *Failure Detection*
    - 2 propriétés clés sont définies pour un tel composant
      - *Completeness*
	- crash-failure of any group member is detected by all non-faulty members
      - *Accuracy*
	- no non-faulty group member is declared as failed by any other non-faulty group member
	- no-false positives in short
    - D'autres propriétés sont néanmoins désirées pour un système réel
      - Vitesse
      - Consommation en bande passante
    - Il est /impossible/ pour un *Failure Detector Algorithm* de respecter à la fois la *Completeness* et l'*Accuracy* si le réseau est *asynchrone* et *non-fiable*
    - Les algorithmes doivent donc effectuer un compromis entre ces deux propriétés
      - *Strong - Weak Completeness*: *All / Some* non-faulty members detect a crash
      - *Strong - Weak Accuracy*: There are *No / Some* false-positives
      - Généralement les applications privilégient une *Strong Completeness* et se retrouvent donc avec une *Weak Accuracy*
    - Stratégies de détection
      - *Heartbeat*: "Je suis en vie"
      - *Ping*: "Es-tu en vie ? - Oui"
    - Présente ensuite un *Failure Detector Algorithm* intéressant: *Phi Adaptive Accrual Failure Detector*
      - A été rendu populaire par Cassandra
      - Introduit la notion de /accrual failure detection/
      - La valeur φ représente la suspicion qu'un noeud ait un problème plutôt qu'être un simple booléen
      - Peut donc ajuster le comportement du système par rapport à cette valeur
      - Par exemple
	- φ(w) > 8 ⇒ On n'arrête de donner de nouvelles tâches à ce noeud
	- φ(w) > 10 ⇒ On redistribue les tâches actuellement en cours du noeud aux autres
	- φ(w) > 12 ⇒ On retire le noeud du système
      - Par contre, la valeur de φ augmente rapidement jusqu'à atteindre ~12 mais la valeur augmente bien plus lentement ensuite
    - Présente ensuite un nouveau *Failure Detector Algorithm* et son évolution: *SWIM Failure Detector* et *Lifeguard Failure Detector*
      - *Scalable Membership Protocol*
      - Comme avec *Phi*, les noeuds sont tout d'abord suspectés avant d'être notés comme étant en panne
      - Détecte les pannes en pingant de façon aléatoire les autres noeuds
      - Si un noeud ne répond pas, demande à d'autres noeuds d'essayer de le ping et de communiquer le résultat
      - *Lifeguard* améliore *SWIM* en diminuant le nombre de faux-positifs
  - *Dissemination*
    - Repose sur des algorithmes de gossiping
    - Présente quelques optimisations possibles
      - Contacte en priorité les noeuds qui n'ont pas été contacté depuis longtemps
      - Accélère la fréquence de gossiping si beaucoup de noeuds n'ont pas observé le dernier gossip
  - *Consensus*
    - Parle de /State Machine Replication/, de /CRDTs/ et de /Protocols/
    - Mentionne aussi le consensus par *Convention*
    - Son exemple concerne l'élection de leader
    - Réplique la liste des membres du groupe sur chaque noeud
    - À partir de là, considère que le leader est le noeud de la liste qui respecte une certaine contrainte
      - Identifiant le plus "petit"
    - Pas besoin de communication supplémentaire pour ce type de consensus
- PhD : Lire les updates de *xray* depuis celle du <2018-07-10 Tue> incluse
  - <2018-07-10 Tue>
    - Le focus du projet a changé de *xray* à *Eon*
    - *Eon* est un système de gestion de version distribué basé sur des CRDTs
    - La collaboration se ferait en temps réel en propageant les modifications effectués par un noeud aux autres
    - Propose pour le moment un CRDT pour représenter le système de fichiers et gérer ses modifications
  - <2018-07-16 Mon>
    - Porte principalement sur la gestion d'un type de conflit : les cycles
    - Un cycle se produit lorsqu'un noeud déplace un dossier *a/* dans un dossier *b/* tandis qu'en concurrence un autre noeud déplace le dossier *b/* dans *a/*
    - En appliquant ces deux opérations successivement, on détache les dossiers *a/* et *b/* de l'arborescence du système de fichier et on créé une boucle entre *a/* et *b/*
    - Afin d'éviter ce genre de situation, il est nécessaire d'empêcher la génération de cycle
    - Propose 2 approches pour résoudre ce problème
    - *Undo: Ordonner totalement les opérations et en annuler une*
      - Lorsque deux opérations en conflit sont détectées, l'une prend la précédence sur l'autre
      - Les opérations suivantes sont ré-évaluées en fonction de l'état résultant de la résolution du conflit
      - Met de côté cette solution pour les raisons suivantes
      - Des cycles pourraient être résolus pour que finalement une autre opération concurrente que ce n'était pas nécessaire
	- Pas sûr de comprendre le cas de figure mentionné ici
	- Est-ce que l'autre opération concurrente serait un autre déplacement générant un autre cycle mais ayant une plus forte priorité?
	- Ou s'agit-il d'opérations genre suppression d'un des deux répertoires?
      - La résolution de conflits même ferait apparaître des dossiers de manière non-intuitive pour les utilisateurs, notamment si la résolution du conflit est longue à calculer
	- Ceci pourrait rendre confus les utilisateurs
	- Dans le cas du renommage, comme il s'agit d'une opération interne, on n'a pas ce genre de problèmes
    - *Fixup: Génère une opération de réparation*
      - Lorsqu'un déplacement génère un cycle, identifie les opérations qui ont participé à la création de ce cycle
      - Génère une opération qui annule l'effet d'une d'entre elles (celle ayant l'horloge la plus élevée)
      - Propage cette opération de *réparation* aux autres noeuds
      - Peut entraîner la génération d'opérations de réparation *redondantes* ou même portant sur des déplacements différents formant le même cycle
      - Implique un état entre l'opération générant le cycle et celle de réparation *incohérent*
    - Dans leur cas, ils ont privilégié la 2nde approche pour des raisons d'usabilité
  - <2018-07-23 Mon>
    - Implémentation de la *Fixup strategy* pour les cycles
      - Lors de la détection d'un cycle, génère une opération de réparation annulant le dernier déplacement qui a généré le cycle
      - Potentiellement cette opération mène à un état où un cycle existe toujours
      - Continue de générer des opérations de réparation jusqu'à ne plus avoir de cycles
    - Implémentation de la *Fixup strategy* pour les renommages
      - Ils se sont rendus compte qu'on pouvait avoir un conflit sur le nom des répertoires
      - À ce moment, compare les deux opérations ayant généré les répertoires en conflit
      - Une des opérations prend précédence sur l'autre et le répertoire généré conserve le nom
      - Génère une opération de renommage ajoutant un ~ au nom de l'autre répertoire
      - Potentiellement ce renommage génère un nouveau conflit
      - Continue de générer des opérations de renommage jusqu'à ne plus avoir de conflits
    - Synchronisation avec le système de fichiers
      - Instancie une structure de données représentant l'état du système de fichiers à l'initialisation
      - Lorsque des opérations distantes arrivent, les applique au système de fichiers
      - Mais potentiellement, une modification locale peut avoir en concurrence modifié l'état du système de fichiers mais n'étant pas encore reflété dans le modèle
      - Et il n'est semblerait pas possible d'effectuer des opérations sur le système de fichiers de façon atomique
      - On peut donc vérifier avant d'appliquer l'opération distante si l'état du système de fichiers correspond à l'état du modèle
      - Mais une modification locale, potentiellement conflictuelle, pourra toujours se glisser entre la vérification et l'exécution de l'opération distante
      - Accepte cette possibilité d'erreur car juge son risque extrêmement faible
  - <2018-07-31 Tue>
    - Application des opérations par batch
      - En effectuant des tests, ils se sont rendus compte qu'appliquer une par une les opérations peut poser des problèmes
      - Notamment dans le cas où on détecte un changement en local correspondant à plusieurs opérations
      - Prend pour exemple deux dossiers avec le même, mais avec un niveau dans l'arborescence différent, dont on échange les positions
	- En résolvant les modifications une par une, on va se retrouver à mettre les deux dossiers au même endroit
	- Une opération de réparation sera donc générée pour résoudre le conflit de nom
	- Puis on va finir l'échange en déplaçant le second dossier
	- On aura donc bien échanger la position des deux dossiers
	- Mais au passage on en aura renommé un
      - Ils ont donc modifié leur modèle pour appliquer les opérations par batch, et seulement après voir s'il y a besoin de générer des opérations de réparation
      - Leur système converge dorénavant
	- Test où 5 différents noeuds appliquent 20 opérations aléatoires
	- Test répété 1 million de fois
    - Modification du système de fichier par batch
      - De la même façon, on ne peut pas mettre à jour l'état du système de fichier au fur et à mesure que l'on applique les opérations sur le CRDT
      - En effet, de par l'approche choisie, on peut se retrouver avec un état du modèle qui est inconsistent
	- Présence de cycles par exemple
      - Reproduire cette état sur le système de fichiers génèrerait des erreurs
      - Il est donc nécessaire d'appliquer toutes les opérations nécessaires pour que le CRDT soit dans un état cohérent avant de générer et d'appliquer les opérations locales correspondantes aux modifications
      - Pour que l'exécution des opérations locales se déroulent correctement, se base sur l'intuition suivante
	- Les insertions et déplacements doivent être exécutés en premiers, en s'occupant des opérations les plus proches de la racine en priorité
	- Les suppressions sont quant à elles traitées en dernières
    - Synchronisation avec le système de fichiers
      - Comme dit précédemment, le modèle qu'on manipule peut en fait représenter une version antérieure de l'état actuel du système de fichier
      - Pour adresser ce problème, créé une copie du modèle avant d'appliquer un batch d'opérations distantes: /old_tree/
      - Une fois les opérations distantes appliquées au modèle, essaie d'appliquer les opérations locales correspondantes sur le système de fichier
      - Utilise /old_tree/ pour déterminer l'emplacement de chacun des fichiers manipulé par les opérations distantes
      - Si une incohérence est détectée, interrompt le mécanisme pour mettre à jour le modèle de /old_tree/
      - Pour chaque mise à jour du modèle, les opérations correspondantes sont appliqués sur l'arbre courant
      - Reprend ensuite l'application des opérations sur le système de fichier
  - <2018-08-21 Tue>
    - Synchronisation avec le système de fichiers
      - La synchroniation s'est avérée être plus difficile que prévue
      - Notamment à cause du fait que le système de fichier peut être mis à jour au cours même du scan
      - Mais ont réussi à faire converger leur CRDT
    - Ajout du support des fichiers textes et de leur structure
      - Jusque là, le CRDT ne pouvait que gérer des dossiers
      - Travaille sur l'ajout de la gestion de fichiers
	- Devrait s'avérer proche de ce qu'ils ont déjà fait pour les dossiers
	- Les fichiers étant des dossiers simplifiés
      - Reste le problème de la synchronisation du contenu des fichiers présents dans l'arborescence
      - Travaille sur l'architecture du logiciel, notamment la partie *Composition*
  - <2018-08-28 Tue>
    - Ajout du support des fichiers
    - CRDT pour le contenu des fichiers
      - Maintenant il faut répliquer le contenu des fichiers
      - Pour cela, ils reprennent leur module *buffer*, qui a l'air d'être leur CRDT pour représenter un document
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Regarder *Liberating Distributed Consensus*
- Lien : https://www.reactivesummit.org/2018/schedule/liberating-distributed-consensus
**** DONE DISTRIBUTED SYSTEMS : Regarder *Is there anybody out there?*
- Lien : https://www.reactivesummit.org/2018/schedule/is-there-anybody-out-there
** Semaine du <2018-11-19 Mon> au <2018-11-23 Fri>
*** Done
- PhD : Lire les updates de xray depuis celle du <2018-07-10 Tue> incluse
  - <2018-09-14 Fri>
    - Intégration avec Git
      - Constate qu'un CRDT pour la gestion de versions n'offrirait pas assez d'avantages par rapport à Git, notamment pour les collaborations asynchrones, pour inciter au changement
      - Constate qu'il faudrait probablement pouvoir interagir avec Git de toute manière
      - Décide donc de changer l'orientation du projet
      - Propose de construire un CRDT pour la collaboration en temps entre les commits
      - Permet de réduire la taille des métadonnées à conserver et l'empreinte du CRDT sur le système
	- Puisqu'on peut reposer sur Git dans une certaine mesure
	- Uniquement besoin de conserver les opérations correspondantes aux modifications depuis le dernier commit
	- A l'air de reset l'état du CRDT à chaque commit
    - Undo
      - Rencontre avec un problème avec l'undo
      - Jusqu'à maintenant, maintenait la liste des opérations et associait un compteur pour chacune d'entre elles
      - Undo une opération consistait à incrémenter son compteur
      - Si son compteur était pair, elle était undo, sinon elle était prise en compte
      - Avait un mécanisme pour gérer les undos/redos concurrents
      - Dorénavant le CRDT est reset après un commit
      - Un undo en concurrence d'un commit pose donc quelques problèmes pour être appliqué, l'opération initiale ayant été perdue lors du commit
      - Propose de maintenir 2 CRDTs pour chaque fichier
	- Le 1er correspond à l'historique complet du fichier
	  - Ça a l'air d'être un DAG des opérations distantes
	- Le 2nd correspond à son historique depuis le dernier commit
      - Lorsqu'un undo est reçu, il est déjà appliqué au CRDT représentant l'historique complet
      - Ceci génère une opération distante qui correspond à l'effet du undo
      - Cette opération distante est ensuite appliquée au second CRDT et transmise aux autres sites
    - Remarques
      - Voir plus en détail comment ils gèrent les opérations concurrentes à un commit
      - Même, voir comment ils gèrent deux commits concurrents
      - Comment ça se passe si je pull un commit qui comprend certaines opérations que j'ai déjà observé grâce au CRDT ?
	- Duplication d'intention ?
      - De plus, un commit peut potentiellement contenir qu'une partie d'une opération observé via le CRDT
	- On copie/colle plusieurs lignes de codes mais on ne commit dans un 1er temps uniquement les premières lignes
  - <2018-10-02 Tue>
    - Release d'une 1ère version de Memo
      - Pour le moment, le système de fichiers est virtuel pour des raisons de simplicité
      - Aucune interaction avec un système de fichiers externe
    - Work Tree
      - *Work Tree* est le CRDT représentant l'état du système de fichiers
      - Assume qu'un *Work Tree* est généré sur l'état qui correspond au dernier commit
      - Assume que l'application se charge d'instancier un nouveau *Work Tree* à chaque commit, reset ou checkout
    - Epochs
      - Décompose l'évolution du *Work Tree* en epochs
      - Chaque epoch correspond à un commit
      - Les opérations sont liées à une epoch et correspondent aux modifications non-committées
      - Une seule epoch active à un moment donné chez un noeud
      - Les epochs possèdent un identifiant composé d'une horloge de Lamport
      - Lorsqu'un site effectue un commit (ou un reset), génère une nouvelle epoch et la broadcast aux autres sites
      - Lorsque des epochs concurrentes sont générées, privilégient arbitrairement l'epoch ayant la plus grande horloge de Lamport
    - Livraison causale des epochs
      - Attache un version vector aux epochs pour indiquer les opérations qui ont été committées
      - Suppose que dans la majorité des cas, un site va recevoir un commit ayant été effectué sur son epoch courante
      - Auquel cas, l'état du commit est inclus dans son état courante
	- Implique une livraison causale des opérations (du moins celle de commit)
      - On peut donc récupérer une partie des opérations actuelles et les conserver pour le CRDT de l'epoch nouvelle créée
      - Mais des commits peuvent être construits sur d'autres epochs que l'actuelle
      - Semble pas clair pour le moment comment ils comptent gérer cela
    - Remarques
      - S'ils utilisent un mécanisme d'epochs similaire au mien avec une décision arbitraire
      - Ils doivent rencontrer le même problème de la gestion des opérations générées sur une epoch "perdante"
      - Voir comment ils gèrent ces opérations
- Meeting
  - Reading about Memo
    - New project from the GitHub team working on teletype
    - Original idea was to propose a Version Control System based on CRDTs to allow real-time collaboration
    - Built on top of Git
    - Epoch-based system
    - Each commit corresponds to a new epoch
    - CRDT keep track of changes between two epochs
    - Need to deal with concurrent epochs and thus operations build on concurrent epochs
  - Refactoring of Mute
    - Want to deploy a version of Mute not using LogootSplit but RenamableLogootSplit or DottedLogootSplit
    - Unfortunately, have some code deeply tied to LogootSplit currently (the object which represents the text document, the anti-entropy mechanism)
    - Need to refactor these components to make them more generic
    - Work on the conception of this refactoring with Victorien and Cedric
*** Planned
**** DONE PhD : Lire les updates de xray depuis celle du <2018-07-10 Tue> incluse
- Leur projet de Git enrichi avec des fonctionnalités d'édition collaborative temps réel à l'aide de CRDTs à l'air d'avoir bien avancé
- Voir le travail réalisé
- Les bulletins d'information sont disponibles ici : https://github.com/atom/xray/tree/master/docs/updates
**** CANCELLED MUTE : Ajouter la gestion du titre du document
- Le titre du document peut être l'objet de modifications concurrentes
- Il est donc nécessaire de s'assurer qu'il converge chez tous les utilisateurs
- Utiliser *LogootSplit* pour gérer le titre semble un peu /overkill/
- Un simple *LWW-Register* suffirait
- Voir pour lier le document et son titre
  - Le titre peut être amener à évoluer suite à des modifications du document
  - Lier l'état du document et l'état du titre?
**** CANCELLED MUTE : Améliorer le pipe /slice/ d'Angular pour les *Strings*
- Angular propose un pipe /slice/ permettant de limiter le nombre de caractères que nous souhaitons afficher d'une *String*
- Cependant, rien n'indique dans le résultat si la chaîne de caractères a effectivement été tronquée ou non
- Créer un pipe /sliceString/ ajoutant '...' au résultat si la chaîne a été coupée
** Semaine du <2018-11-26 Mon> au <2018-11-30 Fri>
*** Done
*** Planned
**** DONE Middleware 2018 : Préparer la présentation pour le Doctoral Symposium
- 10min allouées pour ma présentation (questions incluses)
- Prévois 8min de pres, 2min de questions
** Semaine du <2018-12-03 Mon> au <2018-12-07 Fri>
*** Done
- Formation
  - Outil de quizz : https://socrative.com
  - Mur de post-it : https://padlet.com/
  - Slideshare pour trouver des supports de cours réutilisables
  - 3 types d'évaluations
    - Diagnostic : Quizz pour déterminer le niveau initial
    - Formative : Quizz pour apprendre de ses erreurs
    - Sommative : Interro finale
*** Planned
**** DONE Middleware 2018 : Préparer la *One-Minute Madness*
     DEADLINE: <2018-12-07 Fri>
- Dans le cadre du Doctoral Symposium, doit faire la pub de ma présentation en 1min avec 1 slide
**** DONE Middleware 2018 : Faire un poster présentant le mécanisme de renommage
** Semaine du <2018-12-10 Mon> au <2018-12-14 Fri>
*** Done
- Doctoral Symposium
  - Towards an optimized PhD experience
  - Cloudiator
    - Unified multi-cloud, cross-cloud orchestration
  - Feedback
    - On a l'impression que ce que l'on gagne en espace pour la séquence va être bouffé par ailleurs pour les maps de renommage
      - Maps de renommage seulement nécessaire pour gérer les opérations concurrentes à un renommage, peut les stocker sur le disque au bout d'un moment
      - Stockage est plus cheap que la mémoire
    - À quelle point le document grossit ? À quelle point est-ce un problème ?
      - Nous faudrait des traces pour pouvoir répondre à cette question
- Bird of feather
  - System model
    - Low latency environment, in single data center
    - Crash Fault Tolerant
  - How to accelerate consensus using hardware
    - Utilise du hardware spécifique pour mettre en place un réseau low-latency et reliable entre différents noeuds
      - NICS, RDMA
    - Peut utiliser des techniques de routage particulière pour ordonner les messages de façon à ce que l'application soit toujours dans son /best-case scenario/
  - Se pose la question sur comment du /specific hardware/ peut être utilisé pour améliorer les performances dans différents scénarios
  - What about Geo-Distribution?
    - Majorité du temps d'attente est dû à la latence entre sites, ça peut difficilement être amélioré avec du matériel dédié
    - Par contre, du matériel dédié permettrait p-e d'offrir des garanties sur l'état du système, l'ordre des messages qui permettraient d'optimiser l'algorithme
  - Throughput (TPUT) vs Latency
    - La majorité des travaux avec de l'accélération matériel souhaitent effectuer un grand nombre de consensus par seconde (1M rounds/s) tout en gardant la latence basse (μs)
      - Applications concernées : KVS, Multi-Tenancy
    - Batcher les opérations logiques permet de réduire le nombre de rounds nécessaires tout en conservant la latence basse
  - BFT
    - Dans le cas d'une base de données distribuée dans un système BFT
    - Reçoit un batch de transaction de la part du leader
    - Doit décoder le message
    - Puis doit vérifier l'authenticité des transactions du batch
    - La vérification de l'authenticité des transactions peut être parallélisé
      - Peu de choses à gagner là-dessus
    - Par contre, sur la partie réseau, pourrait utiliser de l'accélération matérielle pour chiffrer/déchiffrer les données
- Réflexions pour le renommage
  - Pourrait adapter la stratégie de renommage selon les informations que l'on a
  - Si on détecte beaucoup de concurrence ou une partition, délaie le renommage ou durcit les conditions de déclenchement
- Recap Middleware
  - *METRIC: A Middleware for Entry Transactional Database Clustering at the Edge* (E. Saurez, B. Balasubramanian, R. Schlichting, B. Tschaen, Z. Huang, S. Puzhavakath Narayanan, U. Ramachandran)
    - https://dl.acm.org/citation.cfm?id=3286686
    - Propose une base de données composée
      - De noeuds à l'edge pour réduire la latence des clients
      - De noeuds globaux pour gérer la synchro entre les noeuds à l'edge
    - Pose un trade-off entre la disponibilité des noeuds à un edge donné et la disponibilité des dernières données d'un point de vue globale
    - Fonctionne sur un système d'ownership
      - Un client revendique l'ownership de plusieurs clés
      - Il est ensuite capable d'effectuer des transactions efficaces sur ces dernières grâce aux noeuds à l'edge
      - Par contre, revendiquer l'ownership requiert une communication entre les différents noeuds pour s'assurer qu'il n'y a pas 2 owners
    - On-going work
  - *ShareLatex on the Edge: Evaluation of the Hybrid Core/Edge Deployment of a Microservices-based Application* (G. Tato, M. Bertier, E. Rivière, C. Tedeschi)
    - https://hal.inria.fr/hal-01942807
    - Le but est de prendre une application construite à l'aide de micro-services et d'essayer de répartir ces services entre le cloud et l'edge
    - Prend *ShareLatex* comme cas d'usage
    - Classifie les micro-services en fonction de différentes catégories (stateless, splittable, critique, temps réel, fréquent...)
    - À partir de cette classification, détermine une répartition entre le cloud et l'edge
    - Permet ainsi d'obtenir de meilleures perfs sur certaines opérations sensibles
    - Mais augmente aussi le coût de certaines opérations qui nécessitent dorénavant une interaction edge/cloud
    - A pour but d'automatiser cette classification et ce déploiement hybride
    - A utilisé Locust (https://locust.io/) pour simuler le comportement de l'utilisateur dans leur expérience
    - La configuration utilisée est disponible ici https://github.com/gtato/sharelatex-loadgenerator
    - D'après notre discussion, les pairs écrivent de manière aléatoire dans le document
  - *QueCC: A Queue-oriented, Control-free Concurrency Architecture* (Thamir M. Qadah, Mohammad Sadoghi)
    - https://dl.acm.org/citation.cfm?id=3274810
    - Se place dans le cadre de la gestion de transactions par batch dans une base de données hautement parallèle (nombreux cores, mémoire partagée)
    - Cherche à réduire le coût de synchronisation entre threads
      - Dû des conflits entre transactions concurrentes
    - L'idée est de séparer la gestion des transactions en 2 phases
      1. Phase de planification
      2. Phase d'exécution
    - La phase de planification est la plus importante
      - Elle détermine les conflits entre transactions
      - Elle les résout en ordonnant les transactions de façon déterministe
      - Les transactions sont placées dans des queues
	- Queues parallèles si pas de conflits
	- Queues de différentes priorités sinon
    - Les queues sont ensuite affectées à des threads
    - Chaque thread peut alors exécuter chaque transaction "sans se poser de questions"
    - Implémenté, obtient de meilleurs résultats (throughput) que l'existant
  - *SPADE: Tuning scale-out OLTP on modern RDMA clusters* (G. Chatzopoulos, A. Dragojevic, R. Guerraoui)
    - https://dl.acm.org/citation.cfm?id=3274815
  - *Speculative Read-write Locks* (S. Issa, P. Romano, T. Lopes)
    - https://dl.acm.org/citation.cfm?id=3274825
    - Le but est d'améliorer les performances en écriture et lecture dans les systèmes utilisant du /Hardware Transactional Memory (HTM)/
    - /HTM/ permet d'offrir le mécanisme de transaction au niveau de l'accès à la mémoire par différents threads
    - Les transactions sont potentiellement aborted/restarted si une transaction concurrente a été commit et provoque un conflit
    - Même si ça permet d'améliorer les performances d'applications hautement parallèle par rapport à des mécanismes de synchronisation logiciels, il y a toujours moyen de faire mieux
    - Propose d'effectuer les lectures seules en dehors du mécanisme de transaction proposé par /HTM/
      - Ceci est rendu possible sans briser l'isolation des lectures grâce à 2 propriétés de /HTM/
      - Les écritures effectuées dans le cadre d'une transaction ne sont externalisées qu'après avoir été /commit/
	- Prévenant ainsi tout /dirty read/
      - /HTM/ détecte les conflits entre les accès mémoire transactionnels et non-transactionnels et annule la transaction courante le cas échéant
	- Prévenant ainsi tout /stale read/
    - Pour éviter d'/abort/ fréquemment les écritures, met en place plusieurs mécanismes de synchronisation
      - Les /writers/ vérifient qu'il n'y a pas de /readers/ actifs avant de /commit/ leur transaction
      - Les /readers/  vérifient qu'il n'y a pas de /writers/ actifs avant d'effectuer leur lecture
	- Le cas échéant, ils attendent que les /writers/ aient finis pour effectuer leur lecture
	- Par contre, une fois que tous les /writers/ ont fini, tous les /readers/ en attente peuvent effectuer leur lecture en concurrence
      - Les /writers/ prédisent le temps d'exécution des lectures en cours pour pouvoir exécuter leur transaction en amont et /commit/ juste après que les lectures soient finies
  - *SpecRPC: A General Framework for Performing Speculative Remote Procedure Calls* (X. Yan, A. Pie Joa, B. Wong, B. Cassell, T. Szepesi, M. Naouach, D. Lam)
    - https://dl.acm.org/citation.cfm?id=3274829
    - Dans le cadre des systèmes composés de plusieurs services
    - Il est souvent nécessaire d'enchaîner plusieurs requêtes séquentiellement
      - La réponse à la 1ère requête sert d'entrée pour la 2nd et ainsi de suite
    - De ce fait, pour former la réponse globale, on accumule les latences nécessaires de chacune de ces requêtes
    - Dans ce papier, les auteurs proposent d'utiliser un outil de prédiction pour prédire les différents résultats successifs
    - Ceci permet
      - D'effectuer les différentes requêtes en parallèle
      - De préparer la réponse pour l'utilisateur
    - Si l'ensemble des prédictions s'est avéré correct, on peut répondre bien plus rapidement qu'avec l'approche standard
    - Si une prédiction s'est avérée incorrecte, on peut tout simplement reprendre le fil d'exécution standard
    - Ce mécanisme a pour défaut d'augmenter la charge reposant sur les différents services en fonction de la précision du mécanisme de prédiction
      - Potentiellement, on va effectuer des requêtes donc les résultats seront inutiles car la prédiction des paramètres était fausse
    - Le mécanisme de prédiction ne fait pas partie du scope de définition
      - Reste à déterminer le coût d'utilisation d'un tel mécanisme
*** Planned
** Semaine du <2018-12-17 Mon> au <2018-12-21 Fri>
*** Done
- Réflexions sur la fusion de traces d'utilisation
  - Obtenir des traces d'utilisation réelles est difficile
  - Par contre, on possède plusieurs traces d'utilisation sur des courtes sessions d'édition collaborative
  - On peut essayer de les combiner afin de générer des exemples de document à grande échelle
  - Plusieurs pistes possibles
  - Combiner séquentiellement, spatialement et temporellement, des traces de plusieurs documents de façon *statique*
    - À partir des traces de deux documents, /doc1/ et /doc2/, on peut générer un nouveau document /doc1+2/ assez simplement
    - Il suffit d'obtenir la /length/ et le /context/ de l'état final de /doc1/
    - Puis de décaler les traces de /doc2/ en ajoutant /length/ à la position de ses opérations et /context/ au contexte de génération de ses opérations
    - Suffit ensuite de concaténer les logs de /doc1/ et de /doc2'/
    - Peut facilement étendre ce mécanisme de combinaison à /n/ documents en utilisant /doc1+2/ comme de nouvelles traces à utiliser
    - Jouer sur l'/offset/ utilisé pour décaler les opérations permet de jouer sur la position de la section
    - Jouer sur le /context/ utilisé pour décaler les opérations permet de jouer sur la causalité de la section
    - Permet d'effectuer plusieurs types de combinaison et de permutation d'un même ensemble de traces
  - Par contre, combiner différentes traces pour les jouer de façon concurrente est un peu plus tricky
    - Pose soit des problèmes de synchronisation, soit des problèmes de cohérence
    - Décompose le groupe en /n/ sous-groupes, chacun jouant une trace
    - Si un membre du sous-groupe 1 reçoit une opération du sous-groupe 2, il ne peut pas la délivrer
      - Cela provoquerait un décalage dans le doc qui briserait l'intention des opérations locales restantes
    - Pour prévenir cela, il faudrait donc partitionner les sous-groupes le temps que chacun exécute sa trace
      - Soit de façon logique, en ne délivrant pas les opérations concernées
      - Soit de façon physique en isolant effectivement les différents sous-groupes
  - Pourrait résoudre ce problème de cohérence à l'aide d'un décalage *dynamique*
    - Peut au fur et à mesure qu'un noeud reçoit des opérations provenant d'autres sous-groupes calculer un nouvel /offset/ et /context/ à ajouter aux prochaines opérations locales
    - Implique un overhead lors de l'exécution, autant d'un point de vue computationnel que d'un point de vue RAM, ce que je cherche à éviter
  - Mais quel serait l'intérêt de combiner des traces pour qu'elles s'exécutent de manière concurrente tout en ne partitionnant pas le système ?
    - Est-ce que ça aurait un effet sur la taille des identifiants ?
      - Si les modifications ne se font pas à la même position, il n'y a pas de raison
    - Est-ce qu'on a envie d'entrelacer deux traces ?
      - Ça peut fournir des nouvelles traces plus "intéressantes" puisque les modifications seraient plus localisées dans une section du document
      - Mais on ne tiendrait plus compte de la cohérence du résultat dans ce cas là
  - Effets de la combinaison des traces sur l'overhead observé
    - Si on se contente de linéariser, spatialement et temporellement, les traces dans une seule et unique trace
      - J'aurai tendance à dire que l'overhead résultant correspondra à la somme des overheads de chaque trace
    - En fait, jouer sur le /context/ de décalage des opérations ne va pas influer sur leur overhead
      - Si j'ai deux traces /doc1/ et /doc2/ et que je veux obtenir le résultat /doc1 + doc2/
      - Jouer /doc1/ puis /doc2/ ou /doc2/ puis /doc1/ devrait me mener à des réprésentations équivalentes
    - C'est si on fait se chevaucher différentes traces à l'aide de l'/offset/ que l'on va augmenter l'overhead
  - Après discussion avec Victorien, il y a moyen de générer un document à partir de plusieurs traces en ajoutant de la concurrence tout en conservant la cohérence
    - Se fait en 2 étapes
    - Génère au préalable un log composé de plusieurs traces avec une partie concurrente
    - Définit une position de référence à partir de laquelle les opérations de la partie concurrente vont être relativement exécutées
    - Fait rejouer ce log par des bots
    - Lorsque les bots rentrent dans la partie concurrente, ils utilisent la position de référence pour calculer l'index des opérations locales à effectuer
    - Ceci permet de générer une nouvelle trace d'exécution ensuite réutilisable par l'outil de simulation standard
- CRDT : *CRDTs in Production*
  - https://www.infoq.com/presentations/crdt-production
  - Système géo-répliqué
  - Auparavant, utilisé des locks pour gérer la concurrence
  - Mais avec le passage à un système géo-répliqué, la latence entre les noeuds impactent négativement les performances
  - Afin d'offrir de meilleures performances, décide de relâcher la cohérence pour la disponibilité
  - Liste plusieurs approches pour maintenir la cohérence dans un système /eventually consistent/
  - *Affinity Based Approaches*
    - Consiste à définir une affinité entre un utilisateur et un noeud de façon à ce que l'utilisateur n'interagisse qu'avec ce dernier
    - Permet de facilement assurer /Read Your Writes/
    - Par exemple, lie à son commencement la session d'un utilisateur à un noeud
    - Ou en routant les requêtes d'une zone géographique sur un noeud
    - Cette approche fonctionne correctement si les données modifiées par un utilisateur lui sont propres
      - Son profil par exemple
    - Mais rencontre des conflits si plusieurs utilisateurs modifient les mêmes données depuis des noeuds différents
    - Nécessite aussi des mécanismes pour recouvrir des pannes
  - *Coordinator Based Approaches*
    - Ajoute au système un coordinateur en charge de gérer les conflits entre les différents noeuds
    - Pas nécessaire qu'il prenne part à chaque modification
    - Est jugé très complexe comme approche
      - Notamment pour supporter les pannes et /recoveries/ dans des états intermédaires
  - *Consensus Based Approaches*
    - Explique l'approche du consensus marche très bien, mais qu'elle était difficilement applicable dans leur contexte
    - Décompose l'application en plusieurs couches
      - Business Logic (Type of documents, business rules)
      - Domain Platform (Entity objects, DAO)
      - Service Infrastructure (Service discovery, routing & balancing)
      - Domain Data (Data model, records)
      - Data Infrastructure (Deployment configuration, namespaces & schemas, replication params)
    - Chacune de ses couches est gérée par une équipe dédiée
    - En tant que dev, a généralement la responsabilité des couches suivantes
      - Business Logic
      - Domain Platform
      - Domain Data
    - Les autres couches sont fournies tels des services par les autres équipes
    - Pour implémenter un algorithme de consensus, indique que les couches concernées sont les suivantes
      - Domain Platform
      - Service Infrastructure
      - Data Infrastructure
    - Nécessite donc d'interférer avec les autres équipes, celles-ci ayant déjà leurs préoccupations respectives
  - *CRDTs*
    - Connaît les /CRDTs/ via Riak, Akka et Jepsen
    - Utilise un /State-based CRDT/
    - Semble avoir implémenté un /Multi-Value Register/
  - Aborde ensuite les problèmes rencontrés
  - *Siblings Explosion*
    - Appelle /Siblings/ des valeurs concurrentes
    - Doit conserver ces différentes valeurs pour potentiellement résoudre l'incertitude par la suite
    - Cet ensemble de valeurs peut grossir indéfiniment en fonction du scénario
    - Définit une stratégie tronquant l'ensemble si sa taille commence à impacter négativement les performances
    - Très arbitraire comme solution
  - *Wait, Siblings?*
    - Souhaite initialement possèder une seule valeur
    - Mais à cause de modifications concurrentes, peut se retrouver avec plusieurs /siblings/
    - Comment gérer cette situation ?
    - Peut demander à l'utilisateur de résoudre le conflit manuellement
    - Ceci peut s'avérer perturbant et/ou rébarbatif pour l'utilisateur
    - Peut masquer ce phénomène à l'utilisateur en prenant la décision pour lui
    - Quelle décision prendre dans ce cas de figure dépend de l'application concernée et est +/- arbitraire
    - Ici, ajoute une couche intermédiaire entre l'application et le /MV-Register/
    - Ce composant intermédiaire définit une stratégie déterministe pour choisir quelle valeur retourner à l'application si le /MV-Register/ retourne plusieurs /siblings/
  - Remarques
    - Ne présente pas leur cas concret, leur type, qu'il a fallu transformer en CRDT
    - D'un côté, s'ils ont fait un /MV-Register/, c'est pas non plus super important
    - Mais potentiellement ça ramène au sujet de la composition de CRDTs
    - La composition permettrait de définir une granularité plus fine des modifications
    - Permettrait ainsi de définir des stratégies différentes de résolution du cas des /siblings/
*** Planned
** Semaine du <2018-12-24 Mon> au <2018-12-28 Fri>
*** Done
- CRDT : Regarder *CRDTs: From sequential to concurrent executions*
  - *Principle of permutation equivalence*
    - Si des opérations, dans le cadre d'une séquence, sont commutatives
    - Alors on doit obtenir le même résultat, dans le cadre d'un système distribué où elles sont sont concurrentes
  - *Principle of sequential semantics*
    - L'exécution concurrente d'opérations doit respecter la même sémantique que leur exécution séquentielle
    - Permet aussi d'assurer la correction d'une exécution séquentielle distribuée
  - Cas d'usage : /Registers in Redis CRDB/
    - Implémente des /MV-Registers/ dans Redis
    - Peut donc se retrouver avec plusieurs valeurs suite à l'exécution d'opérations concurrentes
    - Mais les utilisateurs sont déjà habitués à l'API de Redis
    - Proposer plusieurs valeurs en réponse à une requête qui renvoyait habituellement une valeur peut être déroutant pour l'utilisateur
    - A donc décidé de trancher pour l'utilisateur
    - Conserve des informations de causalité ainsi qu'un timestamp pour chaque opération
    - Lors d'une lecture, utilise le timestamp pour départager les valeurs et n'en retourner qu'une, si plusieurs sont actuellement contenues dans le registre
    - Quel est l'intérêt d'un /MV-Register/ dans ce cas là par rapport à un /LWW-Register/ ?
      - Peut-être lié au point qu'il aborde plus tard dans la présentation concernant le fait de retarder le choix de la sémantique du type de données jusqu'à la requête
  - *Equivalence to a sequential execution?*
    - Les exécutions concurrentes ne peuvent pas toujours être ramenées à des exécutions séquentielles équivalents
    - Prend l'exemple du /Add-Wins Sets/
    - On ne peut donc pas toujours /seulement/ raisonner sur la sémantique séquentielle d'un type de données
    - Il faut prendre des décisions pour ces scénarios
  - *Choice of semantics*
    - Peut retarder le choix de la sémantique du type de données (/Add-Wins, Remove-Wins, .../) jusqu'à la requête
    - Mais ceci implique un coût pour conserver assez de meta-données pour assurer n'importe quelle sémantique à n'importe quel moment
  - Indique qu'un /Operation-based CRDT/ nécessite un mécanisme de membership
    - Je ne vois pas la nécessité
    - À moins que ça ne soit pour obtenir un /global unique ID/
    - Par contre nécessaire pour le mécanisme de /garbage collection/
- DISTRIBUTED SYSTEMS : Regarder *Towards language support for distributed systems*
  - Explique qu'il y a 3 grandes zones d'intérêt dans le domaine à l'heure actuelle
  - *Consistency & Programming Models*
    - Parle de /programming models/ qui offrent des garanties de cohérence
    - Généralement, assume qu'on possède la /strong consistency/
    - Mais parfois, de la /weak consistency/ est suffisante
    - Peut-on permettre aux développeurs de définir le comportement attendu via leurs outils de programmation ?
    - Présente *Lasp*, le langage de programmation proposant des /CRDTs/ comme types de données
    - Présente *Spry* (Christopher Meiklejohn)
      - Cite *On the Design of Distributed  Programming Models* de ce dernier
	- https://arxiv.org/pdf/1701.07615.pdf
	- /the morning paper/ lui a consacré un article
	  - https://blog.acolyer.org/2017/08/17/on-the-design-of-distributed-programming-models/
      - Autant *Lasp* et les /CRDTs/ se focalisent sur le côté /AP/ du /CAP Theorem/
      - Autant *Spry* propose de faire évoluer le trade-off au cours de l'exécution
	#+BEGIN_QUOTE
	Spry is a programming model for building applications that want to tradeoff availability and consistency at varying points in application code to support application requirements
	#+END_QUOTE
      - *Spry* permet de définir des contraintes et des limites sur la /staleness/ et /freshness/ des données via des annotations
    - Présente *Correctables*
      - Introduit dans le papier *Incremental Consistency Guarantees for Replicated Objects* (Rachid Guerraoui, Matej Pavlovic, Dragos-Adrian Seredinschi)
	- https://www.usenix.org/system/files/conference/osdi16/osdi16-guerraoui.pdf
      - Permet à l'utilisateur de faire varier les contraintes de cohérence au fur et à mesure de l'exécution
      - Peut vouloir de la /weak consistency/ jusqu'à un point puis passer en /strong consistency/ ensuite par exemple
      - Fournit à l'utilisateur différentes APIs en fonction des contraintes à assurer
    - Présente *MixT*
      - Introduit dans le papier *MixT: A Language for Mixing Consistency in Geodistributed Transactions* (Matthew Milano, Andrew C. Myers)
	- https://www.cs.cornell.edu/andru/papers/mixt/mixt.pdf
      - Un langage de programmation permettant d'associer différents modèles de cohérence avec des /remote storages sites/
      - Permet d'effectuer des transactions sur un ensemble de données, potentiellement présentes sur des /remote storages sites/ différents et donc ayant différent modèles de cohérence
      - Vérifie à la compilation que ces différents modèles de cohérence sont mixés correctement et avertit l'utilisateur si une erreur peut survenir
  - *Session Types*
    - Correspond à des types pour des protocoles de communication
    - Si le type d'un programme est vérifié, alors cela garantit qu'il suit le protocole de communication souhaité
    - Permet donc de vérifier statiquement la correction de l'implémentation d'un protocole de communication
    - Plusieurs variantes existent
      - *Binary & Multiparty*
	- Protocoles de communication entre deux noeuds
	- Puis étendu aux protocoles de communication impliquant plus que 2 noeuds
      - *Static & Dynamic*
	- Étend certains langages de programmation avec un système de typage statique pour ajouter ces vérifications
	- Mais se fait aussi pour d'autres langages
    - Recommande comme introduction *Session types in programming languages---a collection of implementations.* (Simon Fowler)
      - http://simonjf.com/2016/05/28/session-type-implementations.html
    - Présente un formalisme pour décrire un protocole de communication
    - Un type est ensuite généré et peut être utilisé pour permettre la vérification de la correction de l'implémentation
  - *Static Analysis & Verification*
    - Pas abordé dans le cadre de ce talk
  - Présente *Whip*
    - Introduit dans *Whip: Higher-Order Contracts for Modern Services* (Lucas Waye, Stephen Chong, Christos Dimoulas)
      - https://people.seas.harvard.edu/~chong/pubs/icfp17-whip.pdf
    - Définit des /runtime/ contrats que les services doivent respecter pour assurer leur compatibilité
    - Contraintes du système
      - Doit fonctionner dans le cadre d'un déploiement partiel
      - Ne pas modifier les communications
      - Être /language agnostic/
      - Être compatible avec du /loose coupling/ entre services
      - Être compatible avec des /wire protocols/
	- Cite *Thrift* en exemple
  - Présente *Statically ensuring that functions on replicated data are monotone* (Kevin Clancy)
    - Travail en cours d'un de ses étudiants
    - Permettrait de vérifier la validité d'un /CRDT/ ?
*** Planned
**** DONE CRDT : Regarder *CRDTs: From sequential to concurrent executions*
- Talk présenté par Carlos Baquero à Code Mesh LDN 2018
- https://www.youtube.com/watch?v=hw4agjz4240
**** DONE DISTRIBUTED SYSTEMS : Regarder *Towards language support for distributed systems*
- Talk présenté par Heather Miller à Code Mesh LDN 2018
- https://www.youtube.com/watch?v=-SyvnfBeWJk
** Semaine du <2019-01-07 Mon> au <2019-01-11 Fri>
*** Done
- DISTRIBUTED SYSTEMS : Regarder *Designing Distributed Systems with TLA+*
  - Propose la définition suivante d'un système distribué
    - Multiple agents
    - Global properties
    - Localized information
    - Partial failure
  - *State space*
    - La représentation de l'ensemble des états qu'un système distribué peut atteindre
  - *Behavior*
    - Une chaîne d'états forme un *Behavior*
    - Peut être entre deux états intermédiaires, pas forcément entre l'état initial et un état final
  - Nombre maximum d'états
    - Avec /n/ le nombre d'agents et /m/ le nombre d'étapes des agents
    - On a, au max, /(m*n)(m*n)! / m!^n/ états possibles
    - À titre d'exemple, si on a 2 agents avec 4 étapes chacun, on se retrouve avec environ 550 états au max
    - Par contre, on peut avoir des états qui sont factorisables
  - *Number of states grows fast*
  - *Number of behavior grows very very fast*
  - *Safety properties*
    - Habituellement vérifiée sur chaque état
  - *Systems may have invalid behaviors and states*
    - Certains états peuvent violer des /safety properties/
    - Mais même si tous les états du système vérifient les /safety properties/
    - On peut avoir des /behaviors/ qui représentent une exécution incorrecte
  - *Over a long enough time, a system will do everything*
  - *Invariants*
    - *State Invariants*
      - "At least one server is online"
    - *Action Invariants*
      - "We do not remove servers if below capacity"
    - *Behavior Invariants*
      - "Eventually we have enough server"
  - Références
    - Cite plusieurs pistes pour apprendre à spécifier des systèmes et TLA+
    - *Pratical TLA+* (Hillel Wayne)
      - Pour débutants
      - Pas mal d'exemples pratiques
    - *Specifying Systems* (Leslie Lamport)
      - Niveau intermédiaire
      - Couvre aussi des aspects théoriques pour bien assimiler la rigeur de la preuve
- Réflexion sur les propriétés du mécanisme de renommage que l'on souhaite vérifier
  - On dispose d'un ensemble ordonné correctement formé d'identifiants /Ids/
    - Pas de duplication d'un dot
  - À partir d'un sous-ensemble de /Ids/, /IdsToRename/ (l'état local), on génère une fonction /rename/ et une fonction /reverseRename/
  - En appliquant /rename/ sur chaque élément de /Ids/, on obtient un second ensemble /RenamedIds/
  - Pour la fonction /rename/
  - On cherche à vérifier
    - *Unicité des identifiants*
      - Pour tout identifiant /id/ appartenant à /Ids/, /rename(id)/ n'apparaît qu'une fois dans /RenamedIds/
      - Plus précisément, le dot de /rename(id)/ doit être unique
    - *Préservation de l'ordre*
      - Pour tout /id1, id2/ appartenant à /Ids/ tels que /id1 < id2/
      - On doit avoir /rename(id1) < rename(id2)/
  - Ceci suffit pour assurer le bon fonctionnement de /rename/
  - Un système prévenant l'exécution en concurrence de plusieurs renommages peut se contenter de ces propriétés
  - Pour pouvoir résoudre le cas de renommages en concurrence, d'autres propriétés sont nécessaires
  - On se place dans le cas où on fixe un ordre total sur les opérations de renommage
  - Grâce à cet ordre, on résout de manière déterministe le cas d'opérations de renommage concurrentes en donnant à l'une la priorité sur l'autre
  - Pour la fonction d'inversion de renommage /reverseRename/
    - Alors pas exactement une fonction d'inversion de renommage puisque on peut utiliser /reverseRename/ sur des identifiants n'étant pas issu de l'application de /rename/ sur un identifiant de /Ids/
  - On cherche à vérifier
    - *Inversibilité pour les identifiants appartenant à Ids*
      - Pour tout identifiant /id/ appartenant à /Ids/, on doit avoir /reverseRename(rename(id)) = id/
    - *Préservation de l'ordre*
      - Pour tout /id1, id2/ appartenant à /RenamedIds/ tels que /id1 < id2/
      - On doit avoir /reverseRename(id1) < reverseRename(id2)/
  - Le fait de toujours se déplacer vers l'epoch la plus à gauche (ou la plus à droite) de l'arbre des epochs permet de se passer de la propriété
    - *reverseRename réciproque de rename*
      - Permettrait d'avoir /rename(reverseRename(id) = id/
- DISTRIBUTED SYSTEMS : Regarder *AntidoteDB: a planet scale, highly available, transactional database*
  - Présente un bug intéressant de Discord lié à Cassandra
    - Certains messages avaient pour auteur *NULL*
    - Ce bug se produisait lorsqu'un utilisateur éditait un message alors qu'en concurrence un autre supprimait le même message
    - Dans Cassandra
      - L'update d'une entrée consiste à appliquer une logique de LWW pour chaque colonne de l'entrée
      - La suppression d'une entrée conserve une tombstone, se contente donc de passer chaque colonne de l'entrée à *NULL*
    - Ainsi, la suppression passait tous les champs du message à *NULL*
    - Puis l'update réassignait une valeur aux champs concernés (contenu notamment) mais pas à tous
  - Présente l'évolution des base de données distribuées
    - Au début, AP-Oriented
      - Cite Cassandra, Riak et DynamoDB en exemples
      - Habituellement proposent de manipuler une map clé-valeur
      - Ne supportent généralement pas les transactions
      - Possèdent des sémantiques "flexibles"
      - Besoin de se plonger dans les méandres de la base de données pour comprendre son comportement
    - Puis a évolué vers CP-Oriented
      - Cite Spanner, Cosmos DB
      - Supportent les transactions et SQL
      - Assure /strong consistency/ et un ordre total des opérations
      - Mais au prix d'une plus grande latence
  - Indique qu'il y a un juste milieu entre ces 2 orientations
  - Présente AntidoteDB
    - Base de données distribuée offrant un compromis entre AP-Oriented et CP-Oriented
    - Propose un modèle de cohérence /Transactional Causal+ Consistency/
    - Utilise des transactions pour grouper les mises à jour ensemble
    - Utilise des Operation-Based CRDTs comme types
    - Propose différents types de données
      - Counter
      - Set (Grow-Only, 2-Phase, Add-Wins, Remove-Wins)
      - Sequence
      - Map, JSON
      - Register (LWW, MV)
      - Flags
      - Graph (directed, monotone DAG)
    - Semble inclure de la composition de /CRDTs/ avec le type *JSON*
  - Propose un outil pour vérifier les /safety properties/ de /CRDTs/ : *Repliss*
    - Ont publié 2 papiers utilisant/sur cet outil
      - *Formal Specification and Verification of CRDTs* (P. Zeller, A. Bieniusa, A. Poetzsch-Heffter)
	- Publié à FORTE 2014
      - *Verifying properties of weakly consisten programs with Repliss* (P. Zeller, A. Bieniusa, A. Poetzsch-Heffter)
	- En cours de soumission
    - Permet de la vérification automatique et manuelle avec *Why3*
  - Propose *Just-Right Consistency*
    - Indique que toutes les opérations de notre système n'ont pas les mêmes contraintes de cohérence
    - Que choisir un seul modèle de cohérence serait préjudiciable
      - Soit autoriserait des anomalies
      - Soit serait trop strict et coûteux
    - Propose d'utiliser un modèle de cohérence différent en fonction des invariants de chaque opération
  - Illustre ce principe avec quelques exemples
    - *Causal Order*
      - Peut facilement garantir une /causal delivery/ avec un système AP
    - *Joint updates*
      - Peut facilement grouper des mises à jour en transaction atomique dans un système AP
    - *Limited resources*
      - Doit par contre instaurer une synchronisation entre noeuds pour éviter d'utiliser plusieurs la même ressource en concurrence dans un système où les ressources sont limitées
	- Exemple des tickets
      - La synchronisation peut par exemple être nécessaire qu'à partir d'un certain palier de ressources restantes
  - Présente un mécanisme d'*Access Control for Highly Available Datastore*
    - Définit le comportement que doit proposer ce mécanisme
      - Restrict operations of a system based on a set of rules
      - Rules should be dynamically adaptable (e.g. new employees, new roles)
      - System component Access Monitor enforces rules
      - Operations not adhering to the rules must not be executed
    - Définit la sémantique souhaité d'un système de contrôle d'accès
      - Revoking the right of subject /s/ to execute operation /op/ on /o/ means that /op/ may not be executed by /s/ on /o/ until the right is granted again
    - Cite leur publication sur le sujet
      - *Access Control for Weakly Consistent Replicated Information Systems* (M. Weber, A. Bieniusa, A. Poetzsch-Heffter)
      - Publié dans STM 2016
    - Couple les /access control policies/ avec les données
      - Stockées dans le même store
      - Partage la même couche de livraison
      - Dépendances causales entre les modifications des /access control polices/ et de la structure de données
  - Présente finalement l'écosystème d'Antidote
    - Propose un /CRDT Visualizer/
      - À voir à quoi ça ressemble, comment ça s'utilise et si on peut l'étendre?
    - Propose des outils de vérifications
      - *Repliss*
      - *CISE / CEC*
    - Propose un benchmarker
      - À voir qu'est-ce qu'il benchmark et comment ça fonctionne
  - Notes
    - Étudier le type *JSON* proposé
    - Quelle différence entre *Flag* et *Register* ?
    - Étudier *Repliss*
- Réunion suivi thèse
  - Retours de /Middleware/
    - "Tu rajoutes des metadonnées pour réduire l'overhead, est-ce qu'on s'y retrouve ?"
      - On peut garbage collecter ces métadonnées additionnelles du renommage ou ne serait-ce que les sortir de la mémoire
    - "L'overhead augmente, mais il n'y a pas de changement d'ordre de grandeur"
      - À voir l'effet sur une plus longue session de collaboration
      - Mais l'augmentation de la taille des métadonnées influent aussi sur la taille des messages du réseau
      - On peut aussi pruner du log toutes les opérations dont dépend causalement la dernière opération de renommage stable pour réduire le coût de synchroniser un nouveau pair
    - "Coûteux de transformer les opérations d'une epoch à l'autre"
      - Parallèlisable et pipelinable, si ça peut servir à qqch
    - "Besoin de stabilité pour garbage collecter les nouvelles méta-données"
      - Pas énormément plus couteux que ce que l'on peut avoir actuellement
      - Mais requiert un mécanisme de membership
      - À moins de juste stocker sur le disque quand tous ceux que je connais ont observé le renommage
    - "Y a eu une preuve qu'une liste répliquée nécessite un overhead en meta-donnée minimal et que RGA est proche de celle-ci"
      - Cette preuve semble porté que sur certaines spéficifications de listes répliquées
      - À voir si le fait d'aggréger les éléments en bloc nous fait sortir de ces spécifications
  - Discussion
    - Preuve du mécanisme
      - Prouver la correction du mécanisme, c'est bien
      - Mais potentiellement la partie "preuve" du papier va consister en 1 paragraphe
      - Il faut donc limiter le périmètre de ce que l'on veut prouver pour ne pas perdre trop de temps dessus
    - Garbage collection
      - Pour garbage collecter définitivement les métadonnées d'une epoch, il faut qu'une opération de renommage suivante soit stable
      - Pour déterminer la stabilité d'une opération, un mécanisme de membership est nécessaire
      - Un tel mécanisme est cependant couteux
      - On a donc un choix à faire entre conserver indéfinitivement les données ou mettre en place un mécanisme pour permettre cette garbage collection
      - Peut proposer différents trade-offs à partir de différentes hypothèses ou contraintes
	- Peut stocker sur le disque les métadonnées à partir du moment que les membres que je connais ont observé le renommage
	- Peut supprimer les métadonnées à partir d'un moment sur les noeuds participant à la collaboration mais introduire un noeud spécial qui lui les conserve
    - Perspectives du mécanisme de renommage
      - À priori, tenter de généraliser l'approche à d'autres CRDTs serait limité
	- Peu de CRDTs ont les mêmes contraintes sur leurs métadonnées et donc n'ont pas un overhead qui croît de la même manière
	- Un tel mécanisme permettrait toutefois de définir des états comme "checkpoints" et permettrait de réinitialiser les métadonnées
	- Mais seul le framework de l'approche est généralisable IMO
      - Il existe plusieurs pistes d'améliorations du mécanisme
	- Fonctions de renommage plus performantes
	- Façon de déterminer le renommage gagnant ou de résoudre les renommages concurrents
      - C'est des pistes intéressantes mais qui reviennent à perséverer sur une contribution qui est déjà de niche
      - Dans le cadre de la thèse, ce n'est pas le plus conseillé
  - Plan
    - Rédiger le papier sur la contribution du mécanisme de renommage
      - Choisir la conf visée
	- Permettra de fixer une deadline
	- Mais aussi de fixer le nombre de pages et le public auquel on s'adresse
      - Permettra de mettre en lumière les différentes pièces manquantes actuellement
    - Travailler sur la preuve des /safety properties/ des fonctions de renommage
    - Travailler sur l'évaluation du mécanisme
- DISTRIBUTED SYSTEMS : Regarder *7 Reasons why your microservices should use Event Sourcing & CQRS*
  - Le but est de construire un système basé sur des microservices
    - Independently deployable
    - Owns its schema
    - API only access to data
  - *What is Event Sourcing & CQRS*
    - Command Query Responsibility Segregation (CQRS)
    - Propose de décomposer son système en 2 parties
      - Command, qui correspond aux écritures
      - Query, qui correspond aux lectures
    - Une Command génère une sortie, généralement un Event, qui va ensuite être utilisé en tant qu'entrée pour alimenter le modèle ou les modèles de données utilisés pour les Query
    - Chacune des parties va être responsable de son bon fonctionnement efficace
    - Comme les Commands et Query sont découplées, cela permet à chacun d'adopter un modèle de donnée spécifique à ses besoins
      - Potentiellement, on peut utiliser plusieurs services pour les Queries
	- Chacun ayant son propre modèle de données
	- Afin de gérer des requêtes différentes de la manière la plus optimisée possible
      - Mais avec tous les services générant et mettant à jour leur base de données à partir des évènements générés par la partie Command
  - *Smooth transition from DDD & Event Storming to implementation*
    - Domain Driven Design (DDD)
      - Le but est de décomposer une application large et complexe en différents services ayant chacun leur domaine d'expertise et responsabilité
    - Event Storming
      - Le but est de répertorier les différents évènements pouvant se produire dans le système
      - Puis de les regrouper par domaine
    - Coupler ces deux procédés permet d'obtenir
      - Les différentes Commands de notre système et les Events correspondants
      - Les Microservices concernés
    - Indique qu'il est assez naturel de transitionner de cette spécification vers une implémentation si on architecture notre application sur de l'Event Sourcing et CQRS
  - *Reduce service coupling*
    - Traditionnellement, les services communiquent entre via le biais de requêtes HTTP
      - Que ça soit pour les Queries
      - Ou pour les Commands
    - Si le service appelé plante durant la requête, le service appelant se retrouve bloqué
    - La difficulté consiste à recouvrir de ce type crash
      - Dans le cadre d'une Query, il suffit juste de la refaire
      - Mais dans le cadre d'une Command, il y a plusieurs scénarios possibles
	- La Command peut avoir été perdue à cause du réseau à l'aller
	- Ou le service exécutant la Command peut avoir planté avant de l'exécuter
	- Ou après l'avoir exécuté mais avant d'avoir répondu
	- Ou la réponse peut avoir été perdue à cause du réseau
	- Ou le service appelant peut avoir planté avant de délivrer la réponse
      - Il faut déterminer le cas concerné et être capable de réparer l'état du système
    - L'Event Sourcing permet de transitionner d'un modèle basé sur des pushs synchrones à un modèle basé sur des pulls asynchrones
    - Dans le cadre de Queries, les services peuvent répondre à partir des évènements des autres services dont ils ont connaissance
    - Dans le cadre de Commands
      - J'ai un doute sur comment ça se passe
      - Le service "appelant" génère un évènement
      - Le service "appelé" consomme l'évènement et génère un autre évènement en réponse
      - Le service "appelant" consomme la réponse
  - *Break the read versus write performance bottleneck*
    - Habituellement, on designe notre modèle de données de manière à mettre en place un compromis entre les performances des lectures et celles des écritures
    - Comme on découple la partie Commands, qui génère les écritures, de la partie Queries, qui génère les lectures
    - On peut optimiser le modèle de données de chaque composant pour sa responsabilité
  - *Elevate the concurrency barrier*
    - Comme on a découplé les écritures des lectures
    - Bien plus simple de juste scaler les lecteurs pour supporter le nombre de Queries
  - *Simplify and harden messaging*
    - Raisonner sur les évènements permet de raisonner à nouveau sur les garanties que l'on doit offrir sur la diffusion de chacun d'entre eux
    - at-most-once : /maybe-once/
    - at-least-once : /once-or-more/
    - exactly-once : /essentially-once/
    - Propose différentes solutions pour implémenter ces garanties de delivery en fonction du modèle utilisé (push ou pull)
  - *Eliminate service coupling*
    - Chaque service peut, au fur et à mesure qu'il observe les évènements des autres, se créer une vue locale de l'état du système
    - Cette vue lui permet de fonctionner de manière indépendante, sans avoir à dépendre de la disponibilité d'autres services
    - Les interactions entre services sont seulement nécessaires pour faire évoluer l'état en fonction des évènements du reste du système
  - *Graduate from the IT Nursery*
    - Exploser le système en microservices découplés permet à chacun d'entre eux d'évoluer de façon indépendante
    - Ceci permet de réduire le nombre des personnes concernées dans la prise de décision pour un microservice
    - Et donc limite la coordination requise
    - Ce qui permet d'évoluer plus rapidement et fréquemment
  - Remarque
    - Construire un système en se basant sur un mécanisme de pull asynchrone implique d'autoriser les composants à diverger temporairement
    - Ceci n'est pas compatible avec toutes les applications, Commands ou Queries
*** Planned
**** DONE DISTRIBUTED SYSTEMS : Regarder *Designing Distributed Systems with TLA+*
- Talk présenté par Hillel Wayne à Code Mesh LDN 2018
- https://www.youtube.com/watch?v=tfnldxWlOhM
**** DONE DISTRIBUTED SYSTEMS : Regarder *AntidoteDB: a planet scale, highly available, transactional database*
- Talk présenté par Annette Bieniusa à Code BEAM Lite Munich 2018
- https://www.youtube.com/watch?v=ol1D9X2_nJc
**** DONE DISTRIBUTED SYSTEMS : Regarder *7 Reasons why your microservices should use Event Sourcing & CQRS*
- Lien : https://www.reactivesummit.org/2018/schedule/7-reasons-why-your-microservices-should-use-event-sourcing--cqrs
**** DONE DISTRIBUTED SYSTEMS : Surveiller l'apparition des vidéos des talks de *Code Mesh 2018*
- Plusieurs talks ont l'air intéressant
- Lien YT : https://www.youtube.com/channel/UC47eUBNO8KBH_V8AfowOWOw
- Lien Twitter : https://twitter.com/CodeBEAMio
** Semaine du <2019-01-14 Mon> au <2019-01-18 Fri>
*** Done
- CRDT : Lire *Extending Eventually Consistent Cloud Databases for Enforcing Numeric Invariants*
  - Répartit la différence la valeur actuelle du compteur et la limite entre les différents noeuds
  - Ne se base pas sur un système centralisé et des communications synchrones
  - Mais uniquement sur des infos locales et des communications asynchrones
  - Évite les cas où le système centralisé est indisponible et empêche le système de progresser
    - Il est précisé dans le /system model/ que les processus peuvent crash de manière définitive
    - Quid du cas où un noeud avec des autorisations restantes rencontre une panne définitive ?
    - Peut-on récupérer ses autorisations ?
    - Peut-on gérer des opérations concurrentes du noeud (car finalement il a réussi à recover) du mécanisme de redistribution de ses autorisations ?
  - L'API permet de spécifier si l'on souhaite exécuter une opération seulement en local ou en global
    - Dans les 2 cas, essaie déjà d'exécuter l'op en local
    - Si le noeud n'a pas l'autorisation d'exécuter l'opération demandée
      - Si l'op était strictement locale, elle échoue et indique de ré-essayer en global
      - En mode global, tente de contacter d'autres noeuds pour exécuter l'opération de façon safe
    - Indique qu'en mode global, renvoie une erreur si le compteur s'avère avoir atteint sa limite
      - Que se passe-t-il si je suis isolé du reste du groupe ?
	- Dans le cas d'une partition réseau par exemple
      - Renvoie /retry/ dans ce cas ?
  - Doit préserver l'invariant à tout moment, que ce soit localement et globalement
    - Nécessité de respecter la condition suivante
    - /lower bound <= initial value + Σ inc - Σ dec <= upper bound/
  - Considère la diff entre la valeur du compteur et son invariant comme un nombre de droits d'exécution d'opérations
    - Si un compteur démarre à /n = 40/ et possède comme invariant /n >= 10/
    - Cela résulte en 30 droits de décrémenter /n/ (40 - 10)
    - Incrémenter /n/, dans cet exemple, crée des droits supplémentaires
    - Ces droits peuvent être répartis entre replicas
    - Si un replica possède assez de droits, il peut exécuter une op sans coordination sans crainte de violer l'invariant globalement
    - Si pas assez de droits, l'opération échoue ou nécessite une synchronisation avec un autre replica pour obtenir des droits supplémentaires
  - État se décompose en 3 parties
    - /K/, la valeur initiale du compteur
    - /R/, une matrice /n * n/ où /n/ est le nb de replicas
      - /R[i][i]/ correspond au nombre de droits générés par le replica /i/
	- Ici, cela correspond au nombre d'/inc/ effectués
      - /R[i][j]/ correspond au nombre de droits transférés par le replica /i/ au replica /j/
    - /U/, un vecteur de dimension /n/
      - /U[i]/ correspond au nombre de droits consommés par le replica /i/
	- Ici, cela correspond au nombre de /dec/ executés par /i/
  - Les opérations sur le compteur sont définies en utilisant ces 3 composants
    [[file:img/bounded-counter-pseudo-code.png]]
  - Peut modifier la sémantique du *Bounded Counter* en inversant le sens de /increment/ et de /decrement/
    - Possible alors de définir un compteur tel que /v ≤ K/
    - Pour ça, modifie la sémantique pour que ça soit les droits d'utiliser /increment/ qui soient limités et représentés dans /R/
  - Peut alors combiner les 2 logiques pour créer un *Bounded Counter* avec une borne minimale et maximale
    - Il faut alors modifier les 2 /R/ et 2 /U/ de manière synchronisée (par le biais de transaction)
  - Propose 2 implémentations en se basant sur *Riak* comme /data store/
    - Client-Based Middleware
    - Server-Based Middleware
  - Client-Based Middleware
    - Chaque /Data Center (DC)/ est un replica
    - L'état du *Bounded Counter* est stocké en tant que boîte noire dans *Riak*
    - La MàJ se décompose en 3 étapes
      - Lit la valeur actuelle du compteur
      - Exécute l'opération
      - Soumet la nouvelle valeur au /DC/
    - Marqué comme /strongly consistent/
      - Pas de concurrence autorisée sur un *Bounded Counter* dans un /DC/
      - Si des opérations concurrentes sont générées, seule la 1ère est acceptée
      - Les autres écritures sont rejetées
    - Seul cas où des opérations concurrentes peuvent apparaître est lorsqu'on synchronise des /DCs/
      - Les versions concurrentes du compteur seront alors fusionnées à la prochaine MàJ par un client
    - Souffre de la disponibilité de la ressource critique, la BDD
      - Augmenter le nombre de clients n'augmente pas le /throughput/, juste le nombre d'écritures rejetées
  - Server-Based Middleware
    - Corrige ce problème en déplaçant la logique du *Bounded Counter* sur un serveur qui
      - Est chargé d'exécuter toutes les opérations pour un compteur dans un /DC/
	- Il y a donc sérialisation des opérations
      - Puis de sauvegarder la nouvelle version du *Bounded Counter* dans *Riak*
    - Les clients sont redirigés vers le serveur gérant le *Bounded Counter* concerné à l'aide d'une /DHT/
    - Implémente des optimisations tels que un mécanisme de cache de l'état du compteur et de batch des opérations pour accroître les performances
  - Transferts de droits
    - Met en place un mécanisme pour transférer des droits d'un noeud à l'autre
    - Peut être utilisé en tâche de fond pour rebalancer les droits disponibles sur l'ensemble des noeuds
    - Ou ponctuellement pour permettre d'effectuer une opération
    - Principe
      - Le replica /i/ arrive sous un certain seuil d'autorisations restantes
      - Il utilise sa vue locale pour déterminer quel autre replica /j/ possède le plus d'autorisations
      - Il envoie une requête à ce dernier afin de demander que celui-ci lui transfère une quantité de droits
      - À la réception d'un tel message, le noeud /j/ vérifie s'il peut satisfaire la requête
      - Si oui, il effectue une opération locale pour mettre à jour le nombre de droits qu'il a transféré à /i/
	- Correspond à incrémenter /R[j][i]/
      - En fonction du mode de requête choisi par /i/, /sync/ ou /async/, /j/ répond directement à /i/ ou laisse le processus de synchronisation s'en charger
    - Il s'agit donc d'un mécanisme asynchrone par son design
  - Remarques
    - L'implémentation Client-Based Middleware me semble apporter que peu de valeur
    - Une approche entièrement distribuée aurait été plus pertinent IMHO
    - L'évaluation me semble injuste sur certains points
      - L'implémentation Server-Based Middleware profite d'optimisations supplémentaires que sont le mécanisme de caching et de batching d'opérations
    - Récupération des droits d'un noeud ayant subit une panne définitive
      - D'après le /system model/, les noeuds peuvent subir une panne définitive
      - L'article indique qu'il est possible de récupérer les droits de ce type de noeuds
      - Mais aucune proposition n'étaye ces propos
      - Ne serait-ce, s'agit-il d'un processus manuel ou automatique ?
  - Adaptation de l'approche à d'autres CRDTs
    - Y aurait-il d'autres CRDTs qui bénéficieraient de cette possibilité que les noeuds possèdent et puissent se transmettre le droit d'effectuer certaines opérations ?
    - Il ne faudrait pas que cela soit pour des fonctionnalités critiques, puisque dans ce cas il faudrait supporter la récupération des droits d'un noeud crashé définitivement
    - Un mécanisme d'attribution d'identifiants pour les différents noeuds dans un système distribué reposant sur un mécanisme d'invitation
      - Le noeud initial possèderait l'ensemble des identifiants possibles
      - À l'invitation d'un nouveau noeud, il attribuerait un identifiant au nouveau noeud et lui confierait la moitié de l'ensemble des identifiants restants
      - Avec ce design, un noeud peut se retrouver dans l'incapacité d'inviter un nouveau noeud, faute d'identifiants
      - Un tel mécanisme permettrait de récupérer d'autres identifiants à allouer d'un autre noeud
      - Dans ce cas, nécessiterait un mécanisme pour gérer les pannes définitives
    - Dans tous les CRDTs utilisant des dots, pourrait être utilisé pour effectuer une opération en le nom d'un autre noeud
      - Quelle utilité ?
      - Dans /LogootSplit/, permettrait d'append/prepend à un bloc plutôt que d'en créer un nouveau
  - Mais là où leur solution est élégante, c'est que la représentation des droits est la base même du CRDT
    - On n'a pas d'un côté le compteur, de l'autre les droits d'effectuer les opérations
    - Mais on a une structure unique permettant de représenter les deux à la fois, selon comment on la lit
  - Essayer d'incorporer ce mécanisme à un autre CRDT ressemblerait plus à du hack qu'autre chose
  - Il faudrait plutôt reconcevoir les CRDTs concernés avec ce mécanisme en tête
- Netflux : Implémenter un scénario où 2 bots se connectent, sans aucun browser dans la collaboration
  - Correspond au scénario qu'on souhaite mettre en place dans le cadre des simulations
    - Ça nous paraît effectivement plus simple de mettre de côté toute la partie interaction avec le navigateur
    - Permet de se concentrer sur la manipulation de la structure de données
  - Aucun test de *Netflux* cependant implémente un tel scénario
  - Certains tests par contre font se connecter un bot puis un client à une session de collaboration
  - Ré-utilise ce code pour écrire un test où un bot se connecte puis un second
- Répétition défense de thèse de Guillaume
  - Général
    - Police du texte de la barre du bas trop petite ?
  - Slide 3
    - Mettre en couleur les zones de sous-capacité et sur-capacité ?
  - Slide 8
    - Ajouter une image pour illustrer qu'on peut regrouper les différents tenants sur une seule machine ?
  - Slide 8 à 10
    - Redondant ?
  - Slide 13
    - Chargée
    - Faire apparaître les problèmes dans une autre couleur ?
  - Slide 16
    - Mettre "Hypothesis" au pluriel ?
  - Slide 24
    - Faire apparaître les chiffres importants d'une manière ?
  - Slide 36
    - Ajouter du texte pour les équations comme un titre ?
    - Ce que tu fais slide 41
  - Slide 37
    - Tu donnes les conclusions de l'analyse des résultats à l'oral
    - Les highlighter avec du texte ne serait-il pas utile ?
  - Questions
    - Slide 26
      - Tu parlais de migration à chaud
      - Finalement tu stoppes sur la machine initiale
      - Transfert les données
      - Et redémarre le tenant sur la machine destination
      - Quelle différence avec de la migration à froid ?
    - Slide 46
      - Donc finalement, on migre plus des tenants mais des machines finalement ?
      - Tu parles du coût
      - Quid de la qualité de service ?
*** Planned
**** DONE CRDT : Lire *Extending Eventually Consistent Cloud Databases for Enforcing Numeric Invariants*
- Disponible ici : https://arxiv.org/abs/1503.09052
- Par Valter Balegas, Diogo Serra, Sérgio Duarte, Carla Ferreira, Rodrigo Rodrigues, Nuno Preguiça, Marc Shapiro, Mahsa Najafzadeh
- Publié dans /2015 IEEE 34th Symposium on Reliable Distributed Systems (SRDS)/
- Propose de limiter les opérations que peut effectuer un noeud sans coordination afin de respecter des invariants globaux
- Certains opérations peuvent alors échouer si le noeud a déjà atteint la limite qui lui est imposée
- Un mécanisme permet à un noeud de confier une partie de ses droits à un autre
- Une synchronisation entre les 2 noeuds est alors requise
** Semaine du <2019-01-21 Mon> au <2019-01-25 Fri>
*** Done
- Meeting
  - Read *Extending Eventually Consistent Cloud Databases for Enforcing Numeric Invariants*
    - https://arxiv.org/abs/1503.09052
    - Propose a Counter CRDT which ensures to preserve some global numeric invariants
      - Like always positive or always less than 42
    - According to the invariant, it limits the number of times a replica can perform an update
      - For example, for an always positive counter, i can not decrement more than i increased
    - The state allows to keep track of the value of the counter but also of the number of rights of each replica
      - Very smart
    - Add a mechanism to ask another replica to transfer some of its rights to us
    - Issues
      - The state is a matrix of dimension /n x n/, with /n/ being the number of replicas
	- Should not scale so well with the number of replicas
      - Claim that they can retrieve rights from replicas which failed definitively
	- But do not describe the way to do it
      - Very specific, would be kind of hacky to add it to other data structures
    - Try to see if we could somehow adapt such ideas to other distributed data structures
      - Like FileSystem, without success
  - Struggling with JS for the integration of the renaming mechanism in /mute-core/
*** Planned
** Semaine du <2019-01-28 Mon> au <2019-02-01 Fri>
- AMQP/JMS message brokers
  - *Advanced Message Queuing Protocol (AMQP)*
  - *Java Message Service (JMS)*
  - Aussi appelés /message queues/
  - Permet de connecter des producteurs et des consommateurs
  - Centralise les données dans le /message broker/
  - Permet au système de tolérer du churn des côté des clients (producteurs et consommateurs)
  - Mais déplace le problème de tolérance aux pannes au niveau du /message broker/
  - Notamment, certains /message brokers/ ne conserve les messages à transmettre qu'en RAM
    - Une interruption suite à une panne entraînera donc la perte des messages courants
    - Le système doit pouvoir supporter ce scénario
  - Introduit de l'asynchronicité au sein du système
    - Le producteur se contente de délivrer le message au /message broker/ et d'obtenir son ACK
    - Mais rien ne borne le temps requis pour qu'un consommateur traite ce message
      - Généralement court (<1s)
      - Mais en cas de surcharge ou de panne, peut prendre bien plus longtemps
  - Ne conserve pas les messages
    - Généralement, dès qu'un message a été traité par un consommateur, il est supprimé de la /message queue/
    - Les /message queues/ supposent donc qu'elles ont généralement un nombre "limité" de messages
    - Si, pour une raison donnée, cette hypothèse n'est plus vérifiée
      - Plus de consommateurs de disponibles par exemple
    - Les performances des /message queues/ se dégradent
      - Besoin de stocker les messages sur le disque
      - Ceci impacte les performances globales du /message broker/
  - Offre des moyens pour ne suivre qu'un sous-ensemble des messages
    - Permet aux consommateurs de ne traiter qu'uniquement les messages qui les intéressent
  - Notifie les consommateurs concernés lorsqu'un nouveau message est disponible
  - Permet la mise en place de différents patterns de messaging
    - *Load balancing*
      - Plusieurs consommateurs peuvent s'abonner à un même ensemble de messages et effectuer le même traitement
      - Lorsqu'un message arrive, seul un d'entre eux va s'occuper de le traiter
	- Le consommateur peut être désigné à l'aide d'un système de /round robin/
	- Ou tout simplement un des consommateurs /idle/ à ce moment peut s'en charger
      - Permet d'améliorer les performances lorsque le traitement d'un message est lourd
      - Requirements
	- Le traitement des messages doit être commutatif
	  - Des messages vont être traités en parallèle en fonction du nombre de consommateur
	  - Mais aussi, des messages peuvent être réordonnés
	    - Si un noeud crash alors qu'il était en train de traiter un message
	    - Ce dernier sera retransmis à un autre consommateur par la suite
	    - Cet autre consommateur aura potentiellement traité entre-temps des messages dépendant causalement de ce message
	  #+BEGIN_CAPTION
	  [[file:img/ddia-message-queue-reordering.png]]
	  #+END_CAPTION
    - *Fan-out*
      - Plusieurs consommateurs peuvent s'abonner à un même ensemble de messages mais effectuer des traitements différents
      - Lorsqu'un message arrive, tous les consommateurs vont le traiter
      - Mais ils vont générer chacun leur sortie
      - Permet d'effectuer plusieurs tâches de manière totalement découplée
    #+BEGIN_CAPTION
    [[file:img/ddia-message-queue-messaging-patterns.png]]
    #+END_CAPTION
    - Ces deux patterns peuvent être combinés pour profiter des avantages de chacun
  - Permet de configurer la nécessité d'un ACK
    - Si le système ne peut tolérer la perte d'un message, un mécanisme de ACK est nécessaire
      - Un message pourrait délivrer à un consommateur
      - Mais ce dernier pourrait crash avant même de ne l'avoir traité
    - Afin d'éviter ce scénario, peut exiger des consommateurs qu'ils ACKent explicitement le traitement d'un message
    - Si le /message broker/ ne reçoit pas de ACK d'un consommateur pour un message, il peut le redélivrer à un autre au bout d'un certain temps
    - Mais ceci mène à un scénario où l'on peut traiter plusieurs fois le même message
      - Un consommateur en charge d'un message peut le traiter mais planter juste avant d'émettre le ACK
	- Ou le ACK peut être perdu par le réseau
      - Dans ce cas, le message est redélivré à un autre consommateur et traité de nouveau
    - Il faut donc
      - Soit tolérer la perte de messages si on n'active pas le mécanisme de ACK
      - Soit assurer l'idempotence du traitement des messages si on l'active
- Log-based message brokers
  - L'idée est de proposer une approche hybride entre une base de données et un AMQP/JSM /message broker/
  - Le principal point sur lequel repose cette fusion est la durabilité des données
    - Dans une /message queue/, les messages sont effacés dès lors qu'ils ont été délivrés
    - Dans une base de données, toutes les données sont conservées jusqu'à une suppression explicite
    - Cette seconde approche ouvre des possibilités, notamment sur la création de /derived data/
      - Un nouveau client peut être démarré à tout moment
      - Il est alors capable d'interroger la base de données et de voir les écritures passées
      - Ceci lui permet de générer son état initial
    - En comparaison, démarrer un client dans un système reposant sur une /message queue/
      - Il va juste pouvoir suivre les nouveaux messages
      - Mais il ne pourra pas initialiser son état à partir des précédents, puisqu'ils auront été supprimés
  - Une approche hybride permet de bénéficier des avantages de stocker durablement les messages tout en profitant du système de notifications rapides des /message brokers/
  - Le /message broker/ conserve un log, une séquence /append-only/
  - Les messages générés par les producteurs sont ajoutés à ce log
  - Les consommateurs lisent le log et traitent son contenu de manière séquentielle
  - Si un consommateur atteint la fin courante du log, il attend une notification d'un nouveau message pour reprendre la lecture
  - Un log peut être partitionné afin d'accroître les performances
    - Permet de répartir le log sur plusieurs machines et donc de ne plus être limité par les performances d'une seule et unique machine
  - Pour chaque partition
    - Chaque message se voit assigné un /sequence number/ monotonique
    - Les messages étant ordonnés séquentiellement
  - Par contre, on n'a aucune relation d'ordre entre les messages de partitions différentes
  - Log-based message brokers et patterns de messagings
    - /fan-out messaging/
      - L'approche log-based supporte par défaut ce pattern
      - Il suffit que plusieurs consommateurs s'abonnent à un topic pour le mettre en place
      - Comme aucun message n'est supprimé du /message broker/, chaque consommateur pourra lire et traiter chaque message à son rythme
    - /load balancing/
      - Ce pattern devient par contre plus complexe à mettre en place
      - Si plusieurs consommateurs s'abonnent au même topic, ils vont tous traiter chaque message au lieu de se les répartir
      - Il faut donc diviser le topic en partitions et répartir les partitions entre les noeuds
      - Il faut remarquer que dans ce cas on perd l'ordre total sur les messages
  - Inconvénients
    - Le nombre de noeuds se répartissant la charge travail peut n'être au plus qu'égal au nombre de partitions
      - Puisqu'affecter plusieurs noeuds à une même partition ne fait que de dupliquer le travail effectué
    - Mais partitionner un topic pour augmenter le parallèlisme peut s'avérer gênant pour d'autres consommateurs
      - Certains peuvent ne pas souffrir de problèmes de performances et surtout nécessiter l'ordre total existant avec la partition actuelle
    - Si un message s'avère long à traiter, il délaie le traitement de l'intégralité du reste de la partition car les messages sont traités séquentiellement
  - Stratégies pour le load balancing
    - Une stratégie est proposée dans *Designing Data-Intensive Applications*
      - Elle consiste à abonner plusieurs consommateurs au même topic
      - Tout en ajoutant une logique sur les messages à traiter dans ces derniers
	- Par exemple, un consommateur ne traite que les messages pairs tandis que l'autre ne traite que les impairs
	- Mais comment on fait pour gérer ça dynamiquement ?
	- Notamment, comment on fait pour faire varier le nombre de consommateurs en live ?
      - Ceci permet d'accroître le parallèlisme sur le traitement des messages
      - Tout en ne modifiant pas la partition existante
      - Mais on perd l'ordre total sur les messages
      - Comment augmenter ou diminuer dynamiquement le nombre de consommateurs dans ce scénario ?
      - Et comment gérer les pannes d'un des consommateurs dans ce type de système ?
    - Finalement, Kleppmann recommande de multiplier les partitions
      - De la même manière, comment fait-on varier le nombre de partitions dynamiquement dans des /log-based message brokers/ ?
    - Une autre façon de faire serait d'ajouter un niveau d'indirection
      - Le consommateur, au lieu de traiter directement le message, va les insérer dans un nouveau log
      - Ce log étant spécifique à ce sous-système, il peut être partitionné en fonction de ses besoins
      - Cela correspond à augmenter le nombre de partitions, mais cette fois-ci en aval
      - Cette approche a le mérite de ne pas modifier la partition initiale et donc de ne pas générer de conflits avec les autres consommateurs
      - Mais elle ajoute une indirection et donc une latence supplémentaire
  - /Consumer offsets/
    - De par la nature séquentielle du log, il est simple de déterminer quels messages a traité un consommateur et lesquels il n'a pas encore observé
    - Il suffit pour le consommateur de conserver l'offset du dernier message lu
    - Cet offset est transmis au /message broker/ de façon à ne pas tout rejouer en cas de panne du consommateur
    - Mais cet offset, côté /message broker/, peut être inexact
      - Un consommateur peut avoir récupéré et traité un message
      - Mais avoir crash avant d'avoir pu envoyer la mise à jour de l'offset au /message broker/
    - Dans ce cas, redémarrer un consommateur et lui faire reprendre le travail à partir de cet offset va entraîner le re-traitement de certains messages
    - Il faut donc que le système soit en mesure de supporter ce cas de figure
- Choisir entre les types de /message brokers/
  - Le choix entre ces deux systèmes de messages est motivé d'après les critères suivantes
    #+BEGIN_QUOTE
    In situations where messages may be expensive to process and you want to parallelize processing
    on message-by-message basis, and where message ordering is not so important, the JMS/AMQP style
    of message broker is preferable. On the other hand, in situations with high message throughput,
    where each message is fast to process and where message ordering is important, the log-based
    approach works very well.
    #+END_QUOTE
  - J'ai toutefois l'impression que le facteur motivant est le fait de conserver ou non les messages
    - Peut être utile pour démarrer un nouveau /derivated data system/ ou pour débugger
  - L'autre aspect important IMO est l'élasticité du système
    - J'ai l'impression que les /log-based message brokers/ permettent un passage à l'échelle mais planifié
      - Il faut en amont de l'exécution déterminer comment on va partitionner les différents messages de façon à pouvoir tenir la charge
    - Alors que les /AMQP/JMS message brokers/ m'ont l'air plus flexibles et permettent de s'adapter à la charge courante en démarrant de nouveaux consommateurs
- Change Data Capture (CDC)
  - Consiste à ce qu'une base de données streame les modifications qui sont effectuées sur son contenu
  - Ce stream peut être rendu persistant sous la forme d'un log
  - Permet de créer un système à base
    - d'une base de données /leader/
    - de /derivated data systems/
  #+BEGIN_CAPTION
  [[file:img/ddia-change-data-capture.png]]
  #+END_CAPTION
  - Permet d'éviter les problèmes de cohérence que l'on pourrait rencontrer si on effectuait des écritures parallèles sur les bases de données des différents sous-systèmes
    #+BEGIN_CAPTION
    [[file:img/ddia-dual-writes-inconsistency.png]]
    #+END_CAPTION
  - Tout en ne nécessitant pas l'implémentation d'un mécanisme de transaction distribuée
  - Mais introduit les problèmes relatifs au /replication lag/ et à l'/eventual consistency/
    - Certaines opérations ne seront effectives sur les /derivated data systems/ qu'après un certain délai
  - Propose un mécanisme pour tronquer le log
    - Conserver tous les changements n'est pas faisable
    - Ainsi que tous les rejouer
    - Génère une snapshot correspondant à l'état de la base de données en un point du log
    - Supprime les entrées précédentes du log
  - Propose aussi un mécanisme de compaction du log alternatif à l'approche via snapshot
    - En associant une clé primaire à chaque entrée modifiée
    - On peut reconstruire l'état courant de la base de données en ne rejouant que la dernière entrée concernant chaque clé
    - On peut donc supprimer au fur et à mesure les entrées concernant une clé donnée
** Semaine du <2019-02-04 Mon> au <2019-02-08 Fri>
- Event Sourcing
  - Comparaison avec /Change Data Capture/
    - Dans /Change Data Capture/, on utilise une base de données mutable
    - Le log des changements correspond aux effets des requêtes sur la base de données
    - L'application utilisant la base de données peut ne pas savoir que du /CDC/ est utilisé
    - Dans /Event Sourcing/, on construit la logique de l'application autour d'un log d'évènements immutables
    - Plutôt que de retenir les effets, ici on stocke les causes (les évènements)
    - Exemple
      - Avec /CDC/, on peut conserver qu'à un moment "on a supprimé une entrée de la table des inscriptions" et qu'"on a ajouté une entrée dans la table des retours des étudiants"
      - Avec /Event Sourcing/, on peut plutôt conserver "un étudiant a annulé son inscription"
  - Le log des évènements gardent représente donc l'historique des modifications
  - Afin de recréer l'état, il est nécessaire de reprendre l'intégralité du log et de le rejouer
  - Log compaction
    - Avec /CDC/, nous pouvions compacter le log en ne conservant que la dernière modification pour chaque clé primaire
    - Avec /Event Sourcing/, comme les évènements conservés sont de "haut niveau", on peut difficilement inférer leur résultat sur l'état à 1ère vue
    - Les évènements n'écrasent pas les précédents
    - On ne peut donc pas appliquer le même mécanisme de /log compaction/
  - Snapshot
    - D'un point de vue applicatif, afin d'éviter de reparcourir l'intégralité du log pour regénérer l'état
    - Les applications utilisent généralement un mécanisme de snapshots pour sauvegarder l'état de l'application en 1 point du log
    - Mais ce mécanisme est utilisé en complément du log que l'on souhaite conserver entièrement, afin d'accélerer l'instanciation d'une réplique de l'application
  - Différences entre /commands/ et /events/
    - Une requête d'un utilisateur est originellement une commande
    - À ce stade, elle peut toujours échouer, pour des raisons d'intégrité par exemple
    - L'application doit déjà valider la commande puis ensuite l'exécuter
    - Une fois que la commande est exécutée, elle devient un évènement
    - Un évènement est un fait durable et immutable
    - Même si le client change ensuite d'avis et annule sa commande par la suite
    - L'évènement précédent restera toujours valide
    - Juste, un nouvel évènement indiquera l'annulation
  - Validation d'une /command/
    - Une commande doit donc être validée avant d'être appliquée et de générer un évènement
    - La validation peut se faire via une synchronisation entre noeuds
    - Si l'on souhaite/a besoin de faire une validation asynchrone, un évènement peut être splitté en plusieurs
    - Par exemple, une réservation peut être décomposée en 2
      - Une tentative de réservation
      - Une confirmation de réservation
  - Avantages des évènements immutables
    - Permet d'auditer le système
      - La liste des évènements permet de retracer pas-à-pas l'évolution du système
      - On peut donc auditer le log pour trouver l'origine d'un dysfonctionnement
      - Cette tâche peut être rendue plus complexe dans un système avec une simple base de données mutables puisque des informations peuvent avoir été supprimées
    - Simplifie la /recovery/
      - Puisque tous les évènements sont toujours présents, peut tout à fait rejouer les évènements pour regénérer l'état
      - Alors qu'avec une base de données mutables, si un code incorrect a été exécuté, on peut avoir perdu des données
      - Et à moins d'avoir un backup sous la main, on ne peut plus les récupérer
    - Permet de capturer plus d'informations que l'état du système courant
      - Par exemple, un utilisateur peut ajouter un item à son panier puis le retirer avant de valider sa commande
      - Dans un système avec une base de données mutable, on perd cette information
      - Alors que d'un point de vue analytics, elle peut être intéressante à conserver
  - Dériver de multiples vues d'un /event log/
    - Peut lire le même /event log/ pour générer plusieurs vues du système
    - Permet de faciliter l'évolution de l'application au fil du temps
      - Ajouter une nouvelle feature peut correspondre à déployer un nouveau service qui lit le log pour établir un état et répondre à des requêtes
      - Modifier le schéma d'un service peut être fait en déployant une nouvelle version en parallèle de l'ancienne
	- Une fois que la nouvelle a rattrapée son retard, on peut arrêter l'ancienne version
	- Ce procédé peut s'avérer plus simple à mettre en place que de mettre en place un mécanisme de migration
    - Permet de découpler l'écriture de la lecture des données
      - Permet donc d'optimiser les vues pour répondre aux requêtes spécifiquement
  - L'inconvénient de cette approche (ainsi que de /CDC/) est d'ajouter de l'asynchrone entre la génération d'un évènement et l'application de ses effets dans les vues
    - Les problèmes habituelles /eventual consistency/ s'applique donc
- <<stage-log-based-message-broker>> Remplacer le *BotStorage* par un /log-based message broker/
  - Sujet de stage ?
  - L'idée est de mettre en place un /log-based message broker/
  - Les pairs lui enverraient les opérations distantes qu'ils génèrent
  - Et ils s'abonneraient à ce même topic pour recevoir les opérations distantes des autres pairs
  - Les opérations seraient donc reçues potentiellement à la fois du /webGroup/ et du /message broker/
  - Mais le *SyncService* permet de filtrer ces opérations dupliquées
  - Présente plusieurs avantages
    - Permet de communiquer à travers les partitions provoquées par webRTC
      - À partir du moment où des noeuds arrivent à communiquer avec le /message broker/
      - Ils peuvent se communiquer leurs opérations via ce dernier
      - Le /message broker/ jouerait alors le rôle de TURN
    - Permet initialement à un nouveau pair de récupérer l'ensemble des opérations à sa connexion
    - Permet de retirer toute logique applicative
      - Le /message broker/ n'a pas à connaître la logique de l'application
      - Il lui suffit de stocker les messages
      - Il n'a pas à appliquer les opérations et construire une vue
      - C'est les autres pairs qui se chargeront de traiter les messages en les recevant
    - Permet de ne plus dépendre de la version des clients
      - À partir du moment que les clients envoient des messages
      - C'est leur responsabilité de pouvoir les décoder
      - Le /message broker/ est donc /version agnostic/
    - Permet de n'avoir aucune connaissance des messages
      - Vu qu'on n'a pas besoin de jouer les opérations
      - Pourrait les recevoir chiffrés
      - Par contre, chiffrer les messages lève quelques problèmes d'utilisabilité
    - Permet de jouer la carte de la transparence
      - Si le /message broker/ possède une interface
      - On pourrait montrer directement aux utilisateurs ce que l'on possède du document
    - Permet de logger durablement les évènements
      - Pourrait nous être utile à des fins de débuggage
    - Permet de réduire la reponsabilité du *BotStorage*
      - Le *BotStorage* n'aurait plus besoin de remplir le rôle de TURN de back-up
      - Peut se concentrer sur "juste" être un pair
    - Permet de réduire la surface de développement
      - Pourrait complètement remplacer le *BotStorage*
      - Ce qui nous permettrait de concentrer nos efforts de développement sur d'autres aspects
      - Pourrait aussi remplacer le *LogCollector*
	- Finalement, on a réimplémenté un /log-based message broker/ à l'époque
  - Présente néanmoins aussi des inconvénients
    - Ces inconvénients sont liés au protocole de chiffrement utilisé
    - En fonction des garantis fournies par ce dernier, peut entraver l'utilisabilité
    - Par exemple, dans le cas du /group key agreement/
      - Un noeud ne pourrait déchiffrer que les opérations envoyées lorsqu'il était dans le groupe
      - À partir du moment où il se déconnecte, la clé change
      - Il n'est donc pas en mesure de lire les messages précédents et suivants
      - La seule utilité du /message broker/ dans ce cas est de potentiellement communiquer à travers une partition webRTC
      - Il devient nécessaire de se connecter à un autre noeud pour se synchroniser
  - Étapes
    - État de l'art
      - Quels /log-based message brokers/ existent ?
      - Quelles sont leurs spécificités ?
      - Lequel utiliser ?
    - Prototype
      - Concevoir une application basique à part
      - Potentielles idées
	- Chat
    - Intégration dans MUTE basique
      - Un topic d'opérations distantes est créé par document
      - Les noeuds en sont à la fois producteurs et consommateurs
      - Ils y ajoutent leurs opérations distantes
      - Et récupèrent les opérations du log et les appliquent
      - Il n'y a pas de chiffrement
      - Un utilisateur peut choisir d'utiliser le /message broker/ ou pas pour un document
    - Intégration dans MUTE avec chiffrement
      - Cette fois-ci, les opérations sont chiffrées avant d'être envoyées au log
      - Si on part sur le /group key agreement/, ajout d'un topic pour permettre l'établissement de la clé de groupe à travers les partitions webRTC
    - Remplacement du *LogCollector* par le /log-based message broker/
      - Modifie *MUTE* pour envoyer les logs non plus au *LogCollector* mais au /message broker/
- Réflexions sur le mécanisme de renommage
  - Un mécanisme de consensus est nécessaire
    - Dans /Core & Nebula/, il est nécessaire pour déclencher le renommage
    - Dans mon cas, il est nécessaire pour GC les métadonnées
      - C'est même plus fort que cela, il faut l'unanimité dans mon cas
      - L'opération doit être stable
  - La différence par rapport à /Core & Nebula/ est la position du protocole lourd dans le mécanisme
    - Dans /Core & Nebula/, il se trouve sur le chemin critique
    - Dans mon cas, la stabilité est déplacée hors du chemin critique
  - On profite déjà des effets du renommage pendant la récupération des informations de stabilité
    - Réduction de la bande passante et des computations
  - Peu importe les droits attribués pour effectuer un renommage, on demandera toujours la stabilité pour pouvoir GC
    - On peut arriver dans un état où l'on ne peut plus recevoit d'opérations de renommage concurrentes
    - Mais où un noeud peut toujours générer une opération concurrente qu'il faudra transformée
  - Il faut donc chercher une autre piste d'amélioration
  - Un axe d'amélioration possible est la stratégie pour définir l'opération de renommage prioritaire entre plusieurs concurrentes
  - Actuellement, il suffit qu'un seul noeud isolé génère une opération de renommage prioritaire pour potentiellement faire inverser un grand nombre de renommage au reste du groupe
  - Une stratégie limitant le nombre de migration d'/epochs/ d'un point de vue global du système améliorerait les performances du mécanisme
  - Pour prendre la décision, peut reposer sur différents types de données
    - Les données fixes
    - Les données dynamiques
  - Les données fixes
    - Nb d'éléments renommés
    - Nb d'opérations observées
    - Nb de noeuds ayant observés l'/epoch/ parente
    - Nb de destinataires
  - Les données dynamiques
    - Nb d'opérations dépendantes
    - Nb d'éléments courant
    - Nb de noeuds convertis
  - Les données fixes permettent d'assurer la convergence de façon assez triviale
    - On considère que les données fixes sont incluses dans l'opération de renommage
    - Lorsque les noeuds comparent leur /epoch/ courante avec l'/epoch/ d'une nouvelle opération de renommage, ils utilisent donc les mêmes valeurs
    - Ils prendront donc tous la même décision
  - Mais cette approche ne permet pas de capturer les changements depuis la dernière opération de renommage
  - La décision peut donc être prise sur des données /outdated/
  - Les données dynamiques permettent de palier à ce cas de figure
  - Mais il devient bien plus difficile de montrer que le système converge
    - Des noeuds peuvent prendre des décisions différentes en fonction de leur vue locale
    - Comment garantir que le processus de décision termine à terme ?
  - Ces différentes données permettent de définir une fonction /poids()/ pour une /epoch/
  - En se basant sur cette fonction /poids()/, peut définir une stratégie pour choisir laquelle de 2 /epochs/ concurrentes choisir
  - Là encore, 2 types de stratégies possibles
    - Comparaison par palier
    - Comparaison par branche
  - Comparaison par palier
    - Prévient le yo-yo entre plusieurs branches
    - Les fonctions de transformations doivent offrir moins de garanties
      - Juste préserver l'ordre et /reverseRename(rename(id)) = id/
    - Un noeud isolé peut déclencher une inversion de plusieurs renommages chez le reste des noeuds
  - Comparaison par branche
    - Plus de confiance dans ma branche au fur et à mesure
    - Peut me retrouver à faire yo-yo entre plusieurs branches
      - Notamment si on a plusieurs assez équilibrés qui travaillent en concurrence et qui se resynchronisent brièvement par intermittence
    - Les fonctions de transformation sont plus complexes à écrire
      - Elles doivent respecter plus de propriétés
      - Notamment, on doit avoir /rename(reverseRename(id)) = id/
  - Point intéressant, on n'a pas besoin d'être à la même /epoch/ pour converger
    - Des noeuds peuvent être sur des /epochs/ différentes
    - Tant que chaque noeud sait comment traduire les opérations de l'autre
    - Ils sont capables de les appliquer sur leur état local
    - C'est juste qu'il sera nécessaire de transformer chacune de ces opérations
      - Ce qui sera probablement pas terrible pour les performances
  - L'indécision sur l'/epoch/ n'influence donc pas la /liveness/ du système
** Semaine du <2019-02-19 Tue> au <2019-02-22 Fri>
- Réflexions sur l'impact de la stratégie utilisée pour déterminer la priorité entre plusieurs /epochs/ sur le mécanisme de /garbage collection/ des /epochs/
  - Je parlais précédemment de deux stratégies différentes pour déterminer laquelle de 2 /epochs/ concurrentes choisir
    - Comparaison par palier
    - Comparaison par branche
  - Contrairement à ce que je pensais, ces 2 stratégies différentes partagent le même mécanisme de /garbage collection/ des /epochs/
  - Part du constat qu'on ne peut pas GC une opération de renommage dès que celle-ci est stable
    - Par exemple si j'ai 2 opérations de renommage concurrentes, /e1A/ et /e1B/
    - Avec /e1B/ prioritaire par rapport à /e1A/
    - Lorsque /e1A/ devient causalement stable, on ne peut pas la GC
    - En effet, que /e1A/ soit stable n'assure en rien que tous les noeuds ont observé /e1B/ et transitionné vers cette /epoch/
    - Ils peuvent donc toujours générer des opérations sur l'/epoch/ /e1A/
    - On a donc besoin de conserver les règles de ré-écritures des identifiants de /e1A/ pour être capable de traduire l'effet de ces opérations vers /e1B/
  - Pose donc la question de quand peut-on GC les /epochs/ ?
    - Si on reprend l'exemple courant mais que cette fois-ci, c'est l'/epoch/ /e1B/ qui devient stable
      - La stabilité de /e1B/ offre bien plus de garanties
      - Elle indique plus aucun noeud n'est ou ne sera sur l'epoch /e1A/
      - En effet, tous les noeuds ont observé /e1B/
      - S'ils étaient sur /e1A/, ils ont dû transitionné sur /e1B/ en conséquence
      - S'ils n'avaient pas encore observé /e1A/, ils l'ignoreront lorsqu'ils la recevront
      - Plus aucune opération reposant directement sur l'/epoch/ /e1A/ ne peut encore être reçue ou même émise
      - Pour le moment on considère qu'il n'y a pas eu d'opérations de renommage concurrentes à /e1B/
      - À ce moment là, on peut GC /e1A/
      - En effet, on a déjà reçu et transformé toutes les opérations basé sur /e1A/
      - Si maintenant on considère qu'il y a une opération de renommage concurrente à /e1A/ et /e1B/, /e1C/ telle qu'elle est prioritaire par rapport à /e1B/
      - On peut toujours GC /e1A/
      - Donc à partir du moment qu'une /epoch/ prioritaire devient stable, on peut GC les /epochs/ plus petites
    - Si on reprend l'exemple courant, avec /e1B/ qui devient stable, mais qu'on a en plus /e2A/ qui dépend de /e1A/ et /e2B/ qui dépend de /e1B/
      - On considère la stratégie de priorité par branche
      - Et qu'on a /e1A/ < /e1B/ < /e2A/ < /e2B/
      - On ne peut de nouveau plus GC /e1A/ quand /e1B/ devient stable
      - Des noeuds peuvent être sur l'/epoch/ /e2A/
      - On a donc encore besoin des règles de ré-écritures de /e1A/ pour transformer ces opérations vers /e2B/
      - La stabilité d'une /epoch/ prioritaire ne suffit donc pas pour GC une /epoch/
      - Il faut aussi que l'/epoch/ à GC soit une feuille
  - On peut GC une /epoch/ quand elle est une feuille et qu'une /epoch/ prioritaire est devenue stable
  - Pour GC les /epochs/ de façon efficace, il est donc nécessaire de faire évoluer la structure de données *EpochStore*
    - Conserver la liste des feuilles courantes
      - Permet de facilement accéder aux /epochs/ potentiellement GC -able
      - Principe
	- Quand on ajoute une /epoch/ à l'*EpochStore*
	- Supprimer l'/epoch/ parente de la liste des feuilles
	- Ajouter l'/epoch/ à la liste des feuilles
    - Compter le nombre d'/epochs/ filles de chaque /epoch/
      - On ne peut GC une /epoch/ que si elle est une feuille
      - Permet de facilement identifier qu'une /epoch/ est une feuille si son compteur est à 0
      - Principe
	- Quand on ajoute une /epoch/ à l'*EpochStore*
	- Initialise son compteur à 0
	- Incrémente le compteur de l'/epoch/ parente de 1
  - Algorithme de GC
    - Lorsqu'une /epoch/ devient stable
    - S'il s'agit de l'/epoch/ courante
      - Peut vider l'*EpochStore* dans son intégralité
    - Sinon
      - Calcule la priorité /p/ de l'/epoch/ devenue stable
      - Pour toutes les feuilles, tente de la GC
	- Si la priorité de la feuille est inférieure à /p/
	  1. Récupère l'/epoch/ parente
	  2. Supprime l'/epoch/
	  3. Retire l'/epoch/ de la liste des feuilles
	  4. Diminue de 1 le compteur d'/epochs/ filles de la parente
	  5. Si le nombre d'/epochs/ fille de la parente vaut 0
	     - On suit les mêmes étapes pour GC l'/epoch/ parente
	     - Peut juste skip la 3.
      - Si toutes les feuilles restantes sont des descendantes de l'/epoch/ devenue stable
	- La GC ainsi que ses ancêtres
  - Cet algorithme de GC peut encore être raffiné
    - On a pas nécessairement besoin d'attendre qu'une /epoch/ devienne stable
    - On a juste besoin de tracker la priorité de l'/epoch/ courante de chaque noeud
    - Si on a l'information que tous les noeuds sont sur une /epoch/ avec une priorité plus forte qu'une feuille existante
      - Même s'il s'agit d'/epochs/ différentes et qu'ils n'ont pas connaissance d'/epochs/ concurrentes
    - On peut GC cette feuille
    - Aucun noeud ne peut encore générer une opération dépendant de cette feuille
  - Il s'agit donc de trouver la priorité minimum globale
    - Pour chaque noeud
      - Récupèrer la dernière opération observée de ce noeud à partir du log
      - Récupérer l'/epoch/ de génération de cette opération
      - Calculer sa priorité
    - Récupérer la priorité minimum
  - On peut donc lancer un mécanisme de GC de temps en temps
    - À partir du log, calcule la priorité minimum globale
    - Tente de GC les feuilles en utilisant cette priorité
  - Pose toujours le problème qu'un seul noeud peut bloquer le système de GC en n'émettant pas de nouvelles opérations
- Différences induites par un changement de stratégie utilisée pour déterminer la priorité entre plusieurs /epochs/
  - La représentation de la priorité
    - Peut s'agir d'une valeur
      - Dans le cas d'un poids calculé à partir de plusieurs informations contenues dans l'opération
    - Peut s'agir d'une liste de valeurs
      - Dans le cas de l'ordre total défini à priori sur les /epochs/ par exemple
  - La fonction de comparaison entre priorités
    - Une simple comparaison <
      - Dans le cas où les priorités sont représentées par des valeurs
    - Un ordre lexicographique
      - Dans le cas où les priorités sont représentées par une liste de valeurs
  - Les garanties que doivent fournir les fonctions de renommage /renameId()/ et /reverseRenameId()/
    - De base, ces 2 fonctions doivent préserver l'ordre entre identifiants
      - Pour /id1/ et /id2/ tels que /id1 < id2/
      - On doit avoir
	- /renameId(id1) < renameId(id2/
	- /reverseRenameId(id1) < reverseRenameId(id2)/
    - Les opérations de renommage concurrentes doivent être commutatives
    - Donc pour deux opérations de renommage /op/ et /op'/, on doit avoir /op · op' === op' · op/
    - Ceci peut facilement être garanti si on utilise la priorité pour
      - Annuler l'effet du renommage perdant si on l'a appliqué auparavant
      - Décider d'ignorer le renommage perdant si on le reçoit par la suite
    - On doit avoir /reverseRenameId(renameId(id)) = id/
      - Permet de garantir que dans le cas d'opérations de renommage concurrentes
      - Pour tous les identifiants de l'/epoch/ ancêtre commune
      - On arrivera à récupérer l'identifiant initial
    - Cette propriété est suffisante si la représentation de la priorité utilisée prévient les allers-retours entre /epochs/
    - Si ce n'est pas le cas
    - On doit avoir /renameId(reverseRenameId(id)) = id/
      - Permet de garantir que dans le cas d'opérations de renommage concurrentes
      - Pour tous les identifiants dépendant d'une opération de renommage autrefois perdante, dorénavant sur la branche gagnante
      - On arrivera à récupérer l'identifiant initial
- Lire *IPA: Invariant-Preserving Applications for Weakly Consistent Replicated Databases*
  - Disponible http://www.vldb.org/pvldb/vol12/p404-balegas.pdf
    - Par V. Balegas, S. Duarte, C. Ferreira, R. Rodrigues, N. Preguiça
  - Idée
    - La réplication optimiste permet généralement d'atteindre de meilleurs performances
    - Notamment car elle permet la génération d'opérations concurrentes sans aucune coordination
    - Ce qui, dans certains cas, peut résulter en des états incohérents en fonction de l'ordre d'exécution de ces opérations par sites
    - Afin de prévenir ce problème, certains systèmes utilisent des mécanismes de synchronisation pour ces opérations qui pourraient provoquer un conflit
    - Propose ici une nouvelle approche qui consiste à modifier les opérations conflictuelles pour qu'elles ne génèrent plus de conflits
      - Bon, jusque là, ça ne me paraît pas nouveau pour un CRDT
    - La modification des opérations n'influent pas sur leur comportement habituel
    - Mais elle permet de résoudre les conflits lorsque des opérations concurrentes existent
    - Propose un outil d'analyse statique pour déceler les conflits et proposer des modifications permettant de les résoudre
  - Rétablissement d'invariants
    - Chaque opération possède un ou plusieurs invariants requis pour sa bonne exécution
    - Son exécution peut modifier la valeur d'autres invariants
    - Le problème d'opérations concurrentes est qu'elles peuvent être générées sur un état commun qui est valide en regard de leurs invariants respectifs
    - Mais l'exécution d'une de ces opérations invalide un invariant de la seconde, et inversement
    - Propose donc d'étendre ces opérations pour, si besoin est, rétablir leurs invariants
    - Revient donc à établir une priorité entre ces opérations
    - Et à annuler l'effet de certaines d'entre elles afin de préserver l'invariant
  -
  - Remarques
    - Coût en métadonnées
      - Ne s'attarde que sur le temps d'exécution des opérations
      - Mais la modification des opérations introduit un overhead d'un point de vue méta-données
      - Par exemple, dans le cas du tournoi, s'inscrire à un tournoi est modifié pour regénérer le tournoi si besoin
      - Ceci implique que
	- Soit le tournoi n'est pas supprimé lors de sa suppression, juste caché
	  - Auquel cas un mécanisme de GC reposant sur la stabilité causale est nécessaire
	- Soit l'opération d'inscription embarque toutes les informations du tournoi pour pouvoir le recréer
	  - Auquel cas je serai curieux de voir l'augmentation de la taille des opérations modifiées par rapport aux originales
    - Duplication de la compensation
      - Pour résoudre certains conflits, met en place un système externe de compensation
      - Par contre, chaque replica en détectant un conflit peut déclencher un traitement de la part de ce système
      - Il est donc possible qu'un même conflit soit détecté plusieurs fois par différents sites et que chacun déclenche une procédure de compensation
      - Suppose que c'est le système externe qui gère la compensation qui est responsable de dé-dupliquer les compensations
      - Mais ce n'est pas adressé ici
** Semaine du <2019-02-25 Mon> au <2019-03-01 Fri>
- Réunion suivi de thèse
  - /Safety properties/ des fonctions de renommage
    - /renameId()/ préserve l'ordre
      - i.e. pour tout /id1/ et /id2/ tels que /id1 < id2/, on a /renameId(id1) < renameId(id2)/
    - /reverseRenameId()/ préserve l'ordre
      - Même chose qu'au dessus
    - /reverseRenameId()/ inverse /renameId()/
      - i.e. pour tout /id/, on a /reverseRenameId(renameId(id)) = id/
      - Permet de garantir qu'on peut annuler l'effet d'un renommage perdant et revenir à l'/epoch/ ancêtre
    - En fonction de la stratégie de priorité des epochs, peut nécessiter que /renameId()/ et /reverseRenameId() soient réciproques
  - GC les opérations de renommage
    - Pensais que je pouvais GC une /epoch/ dès qu'elle devenait stable
    - Mais c'était incorrect
    - Besoin de
      - Attendre que tous les noeuds soient sur epoch prioritaire
      - Que l'epoch soit une feuille
  - Comparaison pire cas avec /Core & Nebula/
    - /Core & Nebula/, renommage pessimiste
      - Pire cas, n'arrive pas à faire le consensus
      - N'arrive pas à réduire l'overhead de la structure (en RAM, CPU, bandwidth)
    - Dans notre cas, renommage optimiste
      - Pire cas ?
	- N'arrive pas à GC les epochs
	  - Mais profite déjà d'un bénéfice en RAM, CPU et bandwidth
	- Ou n'arrive pas à se stabiliser sur une epoch
	  - Là par contre, les perfs doivent en prendre un coup
  - Réflexion sur le mécanisme de renommage
    - Tradeoff sur la stratégie de priorité des epochs
      - Pire cas différent en fonction de la stratégie choisie
    - Pas besoin d'être sur la même epoch pour collaborer
  - Plan
    - Intro
    - Séquences répliquées
      - CRDTs
      - Séquence
      - Logoot?
      - RGA?
      - LogootSplit
      - Problème des identifiants qui grossissent, de l'arbre qui grandit
    - Renommage avec coordination
      - Idée du renommage
	- Identifiants plus petits
	- Un seul bloc
      - Exécution locale
      - Gestion des opérations d'insertion/suppression concurrentes
      - Exécution du renommage en distant
      - *EpochStore*
    - Renommage sans coordination
      - Stratégie de priorité entre les différentes epochs
      - Inversion du renommage perdant
      - GC des epochs
    - Évaluation
    - Discussion
      - Différences entre stratégies de priorité d'epochs
      - Peut diverger sur l'epoch courante
  - Questions plan
    - Présenter dans un 1er temps l'approche décentralisée à la /Core & Nebula/ puis ensuite présenter les mécanismes pour supporter un système entièrement distribué
      - Permet de proposer une approche initiale simplifiée dans un 1er temps
      - Avant d'introduire les problèmes liés aux renommages concurrents
      - Permettrait de bien présenter les 2 approches et leurs compromis respectifs
  - Point sur les expérimentations
    - Mis en place une simulation
      - 5 bots qui écrivent de manière aléatoire, 10k opérations chacun
    - Log les opérations jouées
    - Conserve des snapshots au fur et à mesure de l'avancement
    - Souhaite récupérer les différentes snapshots
    - Puis jouer des opérations locales et distantes aléatoires sur les différentes snapshots pour estimer le temps d'exécution
    - A révelé différents bugs
      - Dans /mute-structs/ et dans le bot d'expérimentation
  - Event sourcing
  - Time series database
    - Volume de lecture ?
** Semaine du <2019-03-04 Mon> au <2019-03-08 Fri>
*** DONE IUT : Corriger les projets étudiants réalisés dans le cadre du module JS
- Lien de la classroom : https://classroom.github.com/classrooms/43989904-teaching-mnicolas/group-assignments/2018-lp-ciasie-prog-web-project
*** CANCELLED PhD : Compléter et envoyer l'autorisation de cumul à l'IUT
** Semaine du <2019-03-11 Mon> au <2019-03-15 Fri>
*** DONE DISTRIBUTED SYSTEMS : Lire *Keeping CALM: When Distributed Consistency is Easy*
- Par Joseph M. Hellerstein et Peter Alvaro
- Disponible ici : https://arxiv.org/abs/1901.01930
- Un post de /the morning paper/ y est consacré
  - Disponible ici : https://blog.acolyer.org/2019/03/06/keeping-calm-when-distributed-consistency-is-easy/
- Je me suis principalement basé sur ce commentaire
- Part du constat qu'un système distribué offre de meilleures performances s'il est coordination-free
- Pose la question de quand peut-on éviter la coordination et cherche à y apporter une réponse
- Introduit la notion de /essential coordination/ et /accidental coordination/
  - /essential coordination/
    - Une coordination qui est nécessaire pour offrir une garantie, pour respecter une contrainte
  - /accidental coordination/
    - Une coordination qui aurait pu être évitée en proposant un meilleur design du système
- Indique qu'une cause de /accidental coordinations/ est qu'on tâche de résoudre les problèmes de cohérence au plus bas niveau (le stockage)
- Alors qu'on pourrait déléguer cela aux couches supérieures (l'application)
- Formule la problématique de la manière suivante
  #+BEGIN_QUOTE
  What is the family of problems that can be consistely computed in a distributed fashion without coordination and what problems lie outside that family?
  #+END_QUOTE
- Afin de déterminer si un système peut être coordination-free, s'intéresse aux questions que ce système se pose et à la monotonicité des propriétés qu'il cherche à observer
  - S'il s'agit d'un "Existe-t-il?", ∃, alors dès qu'une occurrence est détectée, on a la réponse
    - Des occurrences supplémentaires ne changeraient pas ce fait
  - S'il s'agit d'un "Est-ce qu'il n'existe pas?", ∄, alors on ne peut obtenir de réponse qu'après avoir pris connaissance de l'état de tout le système
    - On ne peut pas prendre de décision dès qu'une occurrence est détectée
    - Puisqu'il suffit d'un seul autre noeud pour lequel ce prédicat n'est pas respecté pour que le prédicat soit faux pour l'ensemble du système
    - Il est donc nécessaire de vérifier ce prédicat pour chaque noeud avant de prendre une décision
  - De même pour une question "Pour tout", ∀
    - Il est intéressant de noter que ∄x · P(x) <=> ∀x · !P(x)
- Par contre, si on autorise la suppression d'informations, la réponse à la question ∃ peut évoluer au cours du temps
- On ne peut donc plus répondre à la question sans connaître l'état de l'intégralité du système s'il n'est pas monotone
- Énonce le CALM Theorem de la manière suivante
  #+BEGIN_QUOTE
  Consistency as Logical Monotonicity (CALM)
  A program has a consistent, coordination-free distributed implementation if and only if it is monotonic
  #+END_QUOTE
- Introduit la notion de commutativité et de confluence
  - Commutativité
    - Une opération binaire est commutative si l'ordre de ses opérandes n'a pas d'influence sur son résultat
  - Confluence
    - Une généralisation de la commutativité
    - Une opération est confluente si elle produit le même set de résultats pour n'importe quel ordonnancement pour un même set d'inputs
- Composition d'opérations confluentes
  #+BEGIN_QUOTE
  If the outputs of one confluent operation are consumed by another, the resulting composite operation is confluent.
  Hence confluence can be applied to individual operations, components in a dataflow, or even entire distributed programs.
  If we restrict ourselves to building programs by composing confluent operations, our program are confluent by construction, despite orderings of messages or execution races within and accross components.
  #+END_QUOTE
- Explique que les opérations confluentes sont les briques pour construire un système monotone
  - Dans quel sens ?
  - Je le comprends dans le sens "Pour construire un système monotone, doit utiliser des opérations monotones"
  - À confirmer
- Afin d'avoir un système sans coordination, explique qu'on doit toujours éviter les négations
  - Suppression de données
  - ∄ et ∀
  - D'un point de vue état mutable, cela signifie qu'on peut ajouter des données, mais pas les modifier ou supprimer
- Présente les CRDTs comme un framework pour construire un système monotone
- Explique que l'immutabilité est un pattern monotone
- Tandis que la mutabilité est non-monotone
  - Me paraît pas évident
  - À creuser
- Présente le langage *Bloom* comme un langage spécifiquement conçu pour aider au développement d'applications CALM
  - Rend les patterns monotones (et donc confluents) les plus simples à utiliser pour le développeur
  - Effectue de l'analyse statique reposant sur les principes de CALM pour garantir la convergence du programme
- Si on n'arrive pas à fournir une implémentation monotone d'une feature, propose de déplacer la coordination nécessaire en dehors du chemin critique
- Propose une alternative consistant à procéder sans coordination et en mettant en place un mécanisme pour détecter les incohérences et de compensation
- Termine sur la citation suivante
  #+BEGIN_QUOTE
  The CALM Theorem presents a positive result that delineates the frontier of the possible;
  CALM shows that monotonicity, a property of a program, implies consistency, a property of the output of any execution of that program.
  The inverse is also established: non-monotonic programs require runtime enforcement (coordination) to ensure consistent execution.
  As a program property, CALM enables reasoning via static program analysis, and limits or eliminates the use of runtime checks.
  #+END_QUOTE
*** DONE PhD : Lire *Iodide: an experimental tool for scientific communication and exploration on the web*
- S'agit d'un blogpost sur un outil de notebooks développé par Mozilla
- Disponible ici : https://hacks.mozilla.org/2019/03/iodide-an-experimental-tool-for-scientific-communicatiodide-for-scientific-communication-exploration-on-the-web/
- François nous posait la question récemment de l'effort que représenterait d'ajouter des fonctionnalités Jupyter-like dans MUTE
- Serait intéressant de comparer les approches
- Réflexions sur l'intégration de fonctionnalités Jupyter-like dans MUTE
  - Document structuré
    - Les documents dans Jupyter sont des documents structurés
    - Ils sont composés de différentes cellules de différents types
    - On pourrait considérer chaque cellule comme un document
    - De façon à ce que chaque cellule puisse être éditée sans nécessiter de coordination, peut utiliser des CRDTs
      - Doit proposer un CRDT spécifique à chaque type de cellule disponible
    - À ce moment là, le document global serait une composition de CRDTs
    - Pose des questions sur les opérations spécifiques à une telle composition
      - Suppression d'une cellule
	- On devrait pouvoir supprimer une cellule précédemment ajoutée
	- Comment se comporte le système dans le cas d'une modification d'une cellule en parallèle de sa suppression?
      - Déplacement d'une cellule
	- On devrait pouvoir réordonner les cellules ajoutées
	- Comment se comporte le système dans le cas de déplacements concurrents d'une même cellule?
  - Exécution du code
    - Dans un notebook, des cellules correspondent à des fragments de code à exécuter
    - Pose des questions sur comment gérer ces exécutions de code
    - Peut introduire dans la collaboration des bots en charge d'exécuter le code
    - Ce qui pose différentes questions sur le fonctionnement de ses bots
    - Rôle dans la collaboration?
      - Est-ce que c'est vraiment un collaborateur avec qui on partage un document?
      - Ou est-ce qu'il s'agit juste d'un serveur auquel on soumet des demandes d'exécution?
      - Est-ce qu'il ne répond qu'à celui qui a demandé l'exécution ou à tous?
    - Mode d'exécution?
      - Normalement les cellules sont exécutées indépendamment
      - Potentiellement, l'environnement actuel (variables existantes) ne correspondent plus aux cellules de codes exécutées
	- Car elles ont été modifiées depuis leur dernière exécution
      - L'exécution d'une cellule se limite à celle-ci
	- On ne re-exécute pas les cellules précédentes
      - À moins de changer cela, il faut donc transmettre son environnement au bot lors d'une demande d'exécution
	- Paraît coûteux
- Iodide
  - Pas réellement un document structuré
    - S'agit juste d'un document texte avec des balises pour délimiter les cellules
    - Facilement compatible avec les CRDTs existants
  - Exécute le programme directement dans le navigateur à l'aide de *WebAssembly*
    - Pas besoin de partager un bot entre collaborateurs
    - Pas besoin de partager un environnement entre le client et le bot
- Pourrait essayer de forker Iodide et d'utiliser /mute-core/ pour fournir des fonctionnalités d'édition collaborative
  - Pourrait se faire dans le cadre d'un stage
  - Peut ensuite continuer sur l'intégration de mécanisme d'awareness dans l'outil si ça passe bien
*** DONE PhD : Réflexion sur un système de serveurs de signaling distribués
- À l'heure actuelle, on a un point unique de défaillance qui est le serveur de signaling
- Ça serait intéressant de résoudre ce problème
- Actuellement dans sa version centralisée
  - Permet à un noeud de contacter un membre du réseau et de lui communiquer une offre
  - Permet au noeud contacté de répondre par l'intermédiaire du signaling server
- Pourrait proposer un système composé de plusieurs serveurs de signaling
  - Chaque signaling server maintient une vue des connexions de chaque autre noeud
  - Chaque signaling server maintient une liste de ses connexions
  - Transmet les /insert/ et /remove/ locaux aux autres noeuds
    - Comme on utiliserait des /WebSockets/ entre les serveurs, aurait la garantie de l'ordre des messages par serveur
    - Pas besoin d'un CRDT
  - Membership correspond à l'union de ces vues
    - Besoin de compute seulement lors d'une suppression
  - Lorsqu'on perd la connexion d'un signaling server, peut GC ses infos
    - On a de toute façon plus moyen de contacter ses noeuds depuis ce signaling
    - Nécessite par contre d'envoyer son état local à l'ouverture d'une connexion pour repeupler l'ensemble
- Permettrait de faire du load-balancing et d'augmenter la fault-tolerance du système
- Lorsqu'un nouveau noeud essaie de rejoindre le réseau, peut utiliser différentes stratégies pour forcer l'inter-connexion
  - Choisit un noeud ou plusieurs noeuds aléatoirement et fait transiter les messages vers le signaling responsable si besoin
** Semaine du <2019-03-18 Mon> au <2019-03-22 Fri>
- Meeting
  - Mostly working on the draft of the paper on the renaming mechanism
    - Wrote the background part
    - Now working on the first part of the contribution, which is the renaming mechanism in the context of a centralised system
    - Will share it with Gérald and Olivier once this part is done
    - Is having some difficulties to break the renaming mechanism in smaller pieces to present it an incremental manner
      - And not just bomb the reader with all the notions at once
    - Was hoping to finish it today, but is more likely to finish it Monday, Tuesday at most
  - Study a bit Iodide
    - Mozilla's scientific notebook tool
      - Concurrent to Jupyter
    - https://hacks.mozilla.org/2019/03/iodide-an-experimental-tool-for-scientific-communicatiodide-for-scientific-communication-exploration-on-the-web/
    - Could be one use case for our work
    - Its design seems adapted to our work
      - Should be not too difficult to integrate collaborative editing features in it compared to other tools
    - Usually in notebook, the document is composed of several cells of different type
      - Text
      - Code
      - Image
    - Adding collaborative editing features would mean to provide CRDTs for each kind of cell and also a CRDT to compose them
    - But in Iodide, the document is only a text document
      - Is using a markup langage to logically split the document into cells
      - We know how to replicate text
    - Since the code is directly executed into the browser, won't require a bot
      - with all the security issued it would raise
    - They are interested in adding collaborative editing features
      - It's in their roadmap
      - Maybe we should give it a try
** Semaine du <2019-04-01 Mon> au <2019-04-05 Fri>
- Meeting
  - Wrote a deliverable for OpenPaas
  - Wrote a intership proposal to add a log-based message broker to MUTE architecture
    - To act as a bridge when WebRTC connections failed
    - Currently, it's the BotStorage which fill this role
    - But the bot is unstable
    - The main benefit of this approach is that this middleware won't need to understand the messages, just to relay them
    - The goal is to learn more about this technology and the issues it may face
  - Worked on the paper on the renaming mechanism
    - Spent quite a lot of time struggling with the formalism to adopt
    - I am interested in the formalism used in *Pure Operation-Based Replicated Data Types* by Baquero et al.
    - But this formalism is not suited to complex data types as LogootSplit
    - Fallback to formalism used in Logoot and LogootSplit paper
*** DONE Stage : Écrire le sujet de stage pour l'intégration d'un /log-based message broker/ dans *MUTE*
- Rédiger le sujet de stage basé sur cette proposition: [[stage-log-based-message-broker]]
- Vise un.e étudiant.e de TN, 2A IL
- Disponible à l'adresse suivant : https://coedit.re/sujet-stage-kafka
** Semaine du <2019-04-16 Tue> au <2019-04-19 Fri>
- CRDT : Roshi
  - Un LWW-Element Set open-source développé et utilisé par SoundCloud
  - Discussion HN disponible ici : https://news.ycombinator.com/item?id=19679321
  - GitHub disponible ici : https://github.com/soundcloud/roshi
  - Article de blog dédié disponible ici : https://developers.soundcloud.com/blog/roshi-a-crdt-system-for-timestamped-events
  - Utilisé avant de répondre au problème suivant
    - Propose une fonctionnalité de timeline nommée /stream/
    - Affiche à l'utilisateur l'ensemble des évènements produits par ses contacts dans l'ordre chronologique
  - Avait mis en place initialement une première stratégie, celle du /fan out on write/
    - Consiste à écrire dans les /streams/ de chacun des followers de l'utilisateur lorsque ce dernier génère un évènement
  - Cette approche présente plusieurs problèmes
    - Plus l'utilisateur a de followers, plus d'écritures seront déclenchées suite à la génération d'un évènement
    - De même, l'espace utilisé pour stocker les évènements augmente aussi avec le nombre de followers
      - Puisque l'évènement est dupliqué dans l'inbox de chacun des followers
  - Propose une alternative qui est /fan on read/
    - Quand un utilisateur génère un évènement, il l'enregistre dans son outbox
    - Lorsqu'un utilisateur calcule son /stream/, il aggrège les outboxes de tous ceux qu'il follow
  - Cette approche pose néanmoins le problème suivant
    - Il est difficile d'aggréger le contenu de milliers de outboxes (lecture, tri, fusion) dans un délai minimal
  - À l'échelle de SoundCloud, le volume de données que représente les évènements avoisine les 100Go, ce qui peut tenir en RAM
    - Ce qui améliore les perfs
  - Le problème restant porte sur comment peupler les outboxes tout en permettant leur aggrégation efficacement
    - Doit mettre en place de la réplication et du load-balancing des outboxes pour des raisons de performances
  - Propose d'utiliser un LWW-Element Set pour stocker les évènements d'une outbox
    - Ressemble fortement au LWW-Element Set proposé par Shapiro
      - Maintient un ensemble des évènements ajoutés et un autre ensemble des évènements supprimés
      - Chaque élément possède un timestamp
      - Obtient l'ensemble des éléments contenus dans le set en retirant de l'ensemble des évènements ajoutés les évènements qui apparaissent dans l'ensemble des supprimés avec un timestamp supérieur
    - La spécificité de leur CRDT est de retirer dès que possible un évènement d'un ensemble ou de l'autre
      - Dès qu'un évènement est supprimé de façon effective, il est retiré de l'ensemble des ajouts
      - Dès qu'un évènement est ajouté de façon effective, il est retiré de l'ensemble des suppressions
    - Lorsqu'une écriture est réalisée, elle est broadcastée à l'ensemble des clusters
    - Dès qu'un nombre défini de clusters a validé l'écriture, l'écriture est indiquée comme réussie à l'utilisateur
    - Pour la lecture, plusieurs stratégies sont définies
      - La lecture peut juste retourner l'état d'un cluster
      - La lecture peut aussi récupérer l'état de plusieurs clusters
      - Dans ce cas, des divergences peuvent être constatées
      - Retourne l'union des sets dans ce cas de figure
  - Requiert néanmoins une hypothèse forte qui est les timestamps associés aux évènements représentent de manière correcte l'ordre global de ces derniers
    - Justifie cette hypothèse en expliquant que Roshi tourne sur leurs serveurs, qui utilisent un protocole de synchronisation d'horloge
    - Et que le peu de cas incohérents restants n'ont qu'un impact limité
      - Un "like" qui survit à un "unlike" car l'ordre des timestamps associés aux deux évènements est incorrect
- DISTRIBUTED SYSTEMS : *Towards a Solution to the Red Wedding Problem*
  - Papier publié par C.S Meiklejohn, H. Miller et Z. Lakhani au workshop HotEdge
  - Disponible ici : http://christophermeiklejohn.com/publications/hotedge-2018-preprint.pdf
  - Présente le problème du *Red Wedding*
    - Avant, pendant puis après la diffusion d'un épisode de GoT, le wiki de la série souffre de multiples pics d'affluence
      - Les téléspectateurs consultent les pages liées aux différents personnages de l'épisode, les évènements précédents
      - Mais mettent aussi à jour les pages des personnages concernés ou des évènements ayant eu lieu au cours de l'épisode
    - Le wiki, initialement prévu pour supporter un grand nombre de lectures, se retrouve à devoir supporter un grand nombre d'écritures
    - Afin d'optimiser les performances de l'application pour supporter à la fois un grand nombre de lectures et d'écritures, propose d'utiliser
      - l'edge computing pour diminuer les latences réseaux
      - l'architecture serverless pour augmenter l'elasticité du système
  - De cette manière, vise à
    - Offrir des écritures rapides grâce à une latence faible
    - Augmenter le débit global des écritures en les remontant en patch au Data Center pour limiter l'impact de ce dernier sur les perfs
** Semaine du <2019-04-23 Tue> au <2019-04-26 Fri>
- Meeting
  - A envoyé une 1ère version du papier sur le mécanisme de renommage à Olivier et Gérald contenant que la présentation de l'approche centralisé
    - Permis d'avoir quelques retours déjà sur le contenu
    - Va servir de base pour une discussion sur la suite
    - Doit fixer une réunion avec Gérald et Olivier pour cette discussion
  - Travailler sur le papier m'a fait penser à une nouvelle approche
    - Consiste à décomposer la donnée répliquée en 1 une valeur de base et un CRDT pour représenter les modifications depuis cette valeur
    - En train de passer les revues les différents CRDTs pour voir
  - Stage
** Semaine du <2019-04-29 Mon> au <2019-05-03 Fri>
- Lire le blogpost *Local-first*
  - Disponible ici : https://www.inkandswitch.com/local-first.html
  - Rédigé par Martin Kleppmann, Adam Wiggins, Peter van Hardenberg et Mark McGranaghan
  - Présente les limites des applications centralisées
    - Perte pour l'utilisateur de l'ownership de ses données au profit du provider
    - Si le service vient à disparaître, le travail effectué sur l'application disparaît généralement aussi
  - Présente les principes d'une nouvelle catégorie d'application : /Local-first software/
    - Met l'accent sur la possibilité de conserver la propriété de ses données, de collaborer avec d'autres utilisateurs et de pouvoir travailler en mode déconnecté
  - Définit 7 critères qu'idéalement doit remplir une application /Local-first/
    1. Fast
    2. Multi-device
    3. Offline
    4. Collaboration
    5. Longevity
    6. Privacy
    7. User control
  - Montre que les applications traditionnelles ne remplissent pas l'ensemble de ces critères à la fois
  - Indique que les CRDTs sont la technologie de base permettant de nouvelles applications capables de remplir l'ensemble de ces critères
  - A développé la libraire /Automerge/ pour fournir une implémentation de CRDTs
    - Disponible ici : https://github.com/automerge/automerge
  - À partir de /Automerge/ et de la couche réseau /Dat/, a développé une nouvelle libraire /Hypermerge/
    - Disponible ici : https://github.com/automerge/hypermerge
  - A développé 3 prototypes en se basant sur ces technologies
  - *Trellis*, une adaptation de *Trello*
    - Disponible ici : https://github.com/automerge/trellis#readme
    - Vidéo de présentation : https://www.youtube.com/watch?v=L9fdyDlhByM
  - *PixelPusher*, un logiciel de dessin en pixel art collaboratif
    - Disponible ici : https://github.com/automerge/pixelpusher#readme
    - Rapport de projet : https://medium.com/@pvh/pixelpusher-real-time-peer-to-peer-collaboration-with-react-7c7bc8ecbf74
  - *PushPin*, un "mixed media canvas"
    - Le projet le plus abouti
    - Utilisé par leur équipe et par des testeurs externes
    - Disponible ici : https://inkandswitch.github.io/pushpin/
    - Vidéo de présentation : https://www.youtube.com/watch?v=Dox3XAoTCyg
  - Visualizing document history is important
    #+BEGIN_QUOTE
    In a distributed collaborative system another user can deliver any number of changes to you at any moment.
    Unlike centralized systems, where servers mediate change, local-first applications need to find their own solutions to these problems.
    Without the right tools, it can be difficult to understand how a document came to look the way it does, what versions of the document exist, or where contributions came from.

    In the Trellis project we experimented with a “time travel” interface, allowing a user to move back in time to see earlier states of a merged document,
    and automatically highlighting recently changed elements as changes are received from other users.
    The ability to traverse a potentially complex merged document history in a linear fashion helps to provide context and could become a universal tool for understanding collaboration.
    #+END_QUOTE
    - Souligne l'importance d'une fonctionnalité d'historique
    - Après, cet historique a-t-il besoin d'être répliqué ?
    - Est-ce qu'on peut juste fournir la sérialisation locale des opérations à l'utilisateur ?
      - Et donc avoir des différences d'un utilisateur à l'autre
    - Ou faut-il au contraire avoir un CRDT pour l'historique aussi ?
  - CRDTs accumulate a large change history, which creates performance problem
    #+BEGIN_QUOTE
    Our team used PushPin for “real” documents such as sprint planning.
    Performance and memory/disk usage quickly became a problem because CRDTs store all history, including character-by-character text edits.
    These pile up, but can’t easily be truncated because it’s impossible to know when someone might reconnect to your shared document after six months away
    and need to merge changes from that point forward.

    We continue to optimize Automerge, but this is a major area of ongoing work.
    #+END_QUOTE
    - Semblerait qu'ils aient rencontré des problèmes de performances liés à l'utilisation prolongée des CRDTs, notamment de séquences
    -
  - Cloud servers still have their place for discovery, backup, and burst compute.
    #+BEGIN_QUOTE
    A real-time collaborative prototype like PushPin lets users share their documents with other users without an intermediating server.
    This is excellent for privacy and ownership, but can result in situations where a user shares a document, and then closes their laptop lid before the other user has connected.
    If the users are not online at the same time, they cannot connect to each other.

    Servers thus have a role to play in the local-first world — not as central authorities, but as “cloud peers” that support client applications without being on the critical path.
    For example, a cloud peer that stores a copy of the document, and forwards it to other peers when they come online, could solve the closed-laptop problem above.

    Hashbase is an example of a cloud peer and bridge for Dat and Beaker Browser.

    Similarly, cloud peers could be:

      - an archival/backup location (especially for phones or other devices with limited storage);
      - a bridge to traditional server APIs (such as weather forecasts or a stock tickers);
      - a provider of burst computing resources (like rendering a video using a powerful GPU).

    The key difference between traditional systems and local-first systems is not an absence of servers, but a change in their responsibilities:
    they are in a supporting role, not the source of truth.
    #+END_QUOTE
  - Donne quelques pistes de recherche et de travail autour des applications /Local-first/
    - Amélioration des performances des CRDTs
    - Intégration non-immédiate des modifications des autres collaborateurs
      - Actuellement, on propose des modèles où les modifications distantes sont directement intégrées dans ma copie à leur livraison
      - D'un point de vue applicatif, peut vouloir rejeter des modifications, effectuer des modifications privées, ou reformater l'historique de mes modifications
      - Effectue le parallèle avec les notions de "branches", "forks" et "rebasing" des systèmes de gestion de versions
    - Gestion et compatibilité des versions différentes d'une application /Local-first/
      - Des collaborateurs peuvent utiliser des versions différentes d'une même application
      - Ces versions peuvent avoir des fonctionnalités et des modèles de données différents
      - Comment écrire les applications pour permettre l'interopérabilité entre les différentes versions ?
    - Notion de /online/
      - Dans une application /Local-first/, on peut potentiellement collaborer avec d'autres pairs sans avoir accès à l'internet
      - Comment communiquer à l'utilisateur qu'il travaille au sein d'une partition ?
    - Historique du document
      - Dans un système distribué sans coordination, chaque noeud peut jouer les opérations dans un ordre différent des autres en fonction des livraisons des messages
      - Jusque là, nous utilisons un historique linéaire des versions pour représenter les changements
      - Mais quand il n'y a plus d'ordre véritable, comment pouvons-nous retravailler cette représentation de l'historique ?
	- De façon à ce que l'utilisateur puisse comprendre les étapes ayant mené à l'état actuel et qu'il puisse les communiquer à ses autres collaborateurs
    - Contrôles d'accès en distribué
      - Dans une application /Local-first/, rien ne peut empêcher un collaborateur de modifier son document
      - Comment faire pour donner le choix aux autres utilisateurs d'accepter ou de refuser ses modifications ?
      - Comment les notions de "share" et "permissions" évoluent dans ce nouveau paradigme ?
** Semaine du <2019-05-06 Mon> au <2019-05-10 Fri>
- MUTE : Exploiter les résultats de l'expérience
  - A fait tourner une expérience en fin de semaine dernière pour générer des traces d'utilisations
  - Paramètres de l'expérience
    - 5 bots
    - 100k opérations chacun
    - Une action d'insertion (85%) ou de suppression (15%) toutes les 300ms
    - Avant chaque opération, 5% de chance de se déplacer dans le document
  - L'expérience n'a pas pu aller jusqu'au bout suite à différents problèmes
    - Les opérations jouées par un noeud sont ajoutées au fur et à mesure dans un fichier
    - Pour chaque opération, les statistiques de la structure de données actuelles sont ajoutées au log
      - Ceci était initialement prévu pour permettre de mieux expliquer et exploiter les temps d'intégration des opérations
      - Mais comme on a changé de façon de procéder, que l'expérience sert dorénavant à générer des snapshots pour effectuer des mesures ensuite dessus
      - Ces stats se révèlent inutiles
      - Cependant elles nécessitent de parcourir la structure de données pour être calculées
    - Le calcul des stats et l'écriture fréquente dans la fichier alourdissaientt l'exécution de la simulation
    - De plus, une partition s'était produite
      - Un noeud, le *Bot2*, travaillait de façon isolée sur un document
	- D'après le vecteur d'état loggé, il aurait brièvement collaboré avec un autre noeud
	- Mais l'identifiant de ce dernier ne correspond à aucun des autres bots
      - Le *Bot2* était inconnu et deconnecté des autres bots
    - L'objectif n'aurait donc jamais pu être atteint, les bots auraient attendu éternellement
      - Chaque bot, après avoir effectué ses opérations, attend d'avoir observé toutes les opérations pour quitter
    - Cette partition peut avoir été provoquée par un redémarrage du *Master* survenu après avoir démarré sa collaboration avec *Bot2*, mais avant que les autres ne démarrent
      - Cependant, les logs montrent que le noeud avait lequel collaborait *Bot2* a effectué 154 opérations avant de s'arrêter
      - Ça reviendrait à 45s de collaboration avant que le crash ne se produise
      - 45s durant lesquelles aucun autre bot n'aurait démarré et ne se serait connecté au *Master*
    - Cette explication s'avère donc peu convaincante
  - Solutions
    - Modifier /toJSON()/ de *LogootSRopes* pour exclure /mapBaseToBlock/
      - Au passage, on a remarqué dans les snapshots qu'apparaissait le champ /mapBaseToBlock/
      - Ce champ, qui associe à chaque base de position le bloc auquel elle correspond, est volumineux
      - Mais n'est pas utilisé lors de la désérialisation de la séquence répliquée
      - Le retirer de la sérialisation
    - Bufferiser les opérations pour écrire dans le log que toutes les X opérations
    - Retirer le calcul des stats de la structure de l'entrée de chaque opération
    - Modifier la procédure de démarrage des bots pour qu'ils n'assument plus que *Master* est démarré
      - Essayer de se connecter à *Master*
      - Si la connexion échoue, ressayer au bout de quelques secondes, plusieurs fois si besoin
      - Si au bout de plusieurs tentatives, la connexion n'a toujours pas pu être établie, quitter avec un message d'erreur
      - Sinon, commencer à travailler sur le document
    - Ajouter des logs concernant la couche réseau pour mieux observer l'évolution de la collaboration
  - Mais j'ai tout de même pu récupérer 2 snapshots d'un site
    - 1 snapshot après avoir observé 10k opérations, environ 7k caractères
    - 1 snapshot après avoir observé 290k opérations, environ 200k caractères
  - Ces snapshots peuvent être exploitées pour obtenir quelques résultats rapides
    - Reste à définir quels résultats on souhaite obtenir
    - Un bref coup d'oeil aux expérimentations d'autres papiers (Spanner, Dynamo) fait apparaitre que les stats généralement utilisées sont
      - la moyenne du temps d'intégration d'une opération
      - l'écart type
      - des centiles, généralement le 99ème
    - Peut, à partir d'une snapshot donnée, générer et exécuter des opérations locales puis des distantes
    - Pour chaque opération
      - Calculer le temps d'intégration
      - Conserver l'ensemble de ces valeurs
    - Peut ensuite utiliser chaque ensemble de valeurs obtenu pour calculer
      - Le temps d'intégration moyen
      - L'écart type
      - Le temps médian
      - Le 99ème centile
    - Peut faire ces mesures pour les opérations d'insertions et de suppressions
    - Pose le problème de la génération d'opérations locales
      - Le principal problème concerne le nombre de caractères sur lesquelles l'opération porte
      - Ce nombre ne devrait pas modifier le comportement de l'insertion
      - Mais peut influer sur les performances de l'opération de suppression
	- Si une opération de suppression modifie la racine, son prédecesseur et son successeur, le temps d'intégration nécessaire sera plus important que si elle ne modifiait qu'un bloc
      - Sur combien d'éléments les opérations de suppression doivent-elles porter ?
      - Pour le moment, peut générer différents ensembles d'opérations de suppression portant sur 1, 5 et 10 éléments
  - Ces mesures aboutiraient à des fichiers de résultats de la forme suivante
    #+BEGIN_QUOTE
    {
      "global": {
        "local": { mean, median, ninetyNinePercentile, standardDeviation },
        "remote":  { mean, median, ninetyNinePercentile, standardDeviation }
      },
      "insert": {
        "local": { mean, median, ninetyNinePercentile, standardDeviation },
        "remote":  { mean, median, ninetyNinePercentile, standardDeviation }
      },
      "remove1": {
        "local": { mean, median, ninetyNinePercentile, standardDeviation },
        "remote":  { mean, median, ninetyNinePercentile, standardDeviation }
      },
      "remove5": {
        "local": { mean, median, ninetyNinePercentile, standardDeviation },
        "remote":  { mean, median, ninetyNinePercentile, standardDeviation }
      },
      "remove10": {
        "local": { mean, median, ninetyNinePercentile, standardDeviation },
        "remote":  { mean, median, ninetyNinePercentile, standardDeviation }
      },
      "stats": { ... }
    }
    #+END_QUOTE
  - En attendant d'avoir fini de modifier le code de /results-exploit/ en conséquence, j'ai fait tourner rapidement une expérience sur les snapshots précédentes pour avoir un ordre d'idée
  - L'expérience consiste à générer et appliquer 1000 opérations sur des clones de la snapshot donnée
    - Les opérations se font sur 1 seul caractère pour le moment
    - Lors de la génération d'une opération, il est équiprobable qu'il s'agisse d'une insertion ou d'une suppression
  - Les temps résultants sont exprimés en ns
  - On obtient les fichiers de résultats suivants
    - Pour la snapshot de 10k opérations
      #+BEGIN_QUOTE
      {
        "local": {
          "mean": 53731.568,
          "median": 48630,
          "ninetyNinePercentile": 277565,
          "standardDeviation": 70099.00976909287
        },
        "remote": {
          "mean": 91138.477,
          "median": 74061,
          "ninetyNinePercentile": 503819,
          "standardDeviation": 106249.73596990005
        }
      }
      #+END_QUOTE
    - Pour la snapshot de 290k opérations
      #+BEGIN_QUOTE
      {
        "local": {
          "mean": 97693.909,
          "median": 85672,
          "ninetyNinePercentile": 702773.5,
          "standardDeviation": 113293.02883307834
        },
        "remote": {
          "mean": 176295.627,
          "median": 142888.5,
          "ninetyNinePercentile": 819070.5,
          "standardDeviation": 145088.37949201127
        }
      }
      #+END_QUOTE
  - Plusieurs constats
    - L'ordre de grandeur du temps d'intégration d'une opération est en dizaines de µs
    - Ce temps n'a que très peu augmenté par rapport au nombre de caractères
      - Il n'a même pas doublé
  - *Ça risque d'être difficile d'axer l'argumentation du papier sur l'aspect temps d'intégration*
    - L'augmentation est trop faible et n'indique pas un problème qui pourrait impacter l'expérience utilisateur
    - Va falloir plutôt centrer la discussion sur l'aspect RAM et bandwidth
  - On a relancé une expérience afin de générer de nouvelles snapshots
  - On a constaté un phénomène intéressant
    - Les noeuds ne travaillent pas tous autant
      #+ATTR_HTML: :width 200px
      #+ATTR_ORG: :width 200
      [[file:~/Desktop/Screen Shot 2019-05-07 at 16.07.43.png]]
    - Ces différences sont aussi bien sur l'aspect consommation CPU que consommation RAM
    - Ce qui étonnant, c'est que les noeuds qui sont en avance qui consomment le moins
    - Une raison qui expliquerait ce phénomène serait que le /querySync/, à partir d'un certain stade, ne permettrait plus aux noeuds de rattraper leur retard
    - Mais au contraire contribuerait à augmenter ce retard
    - À confirmer
** Semaine du <2019-06-03 Mon> au <2019-06-07 Fri>
- *The Bloom Clock*
  - Disponible ici : https://arxiv.org/abs/1905.13064
  - Idée
    - Présente une nouvelle structure de donnée, la *Bloom Clock*
    - Cette structure de donnée permet de déterminer avec une certaine probabilité l'ordre partiel entre des évènements d'un système distribué
    - Il s'agit donc d'une alternative à la *Vector Clock*
    - Son principal avantage, comparé à la *Vector Clock*, est que sa taille ne croît pas linéairement avec le nombre de noeuds
    - Mais avec des paramètres que le développeur peut fixer, en fonction du niveau de confiance en la clock qu'il souhaite obtenir
  - Rappel *Bloom Filter*
    - Structure de donnée probabiliste, efficace en terme de consommation mémoire
    - Permet de vérifier rapidement si un élément est dans un ensemble
    - Afin de gagner en espace mémoire, cette structure de donnée introduit une notion de probabilité
    - Cette structure de donnée ne peut pas retourner de faux négatifs, mais peut par contre retourner des faux positifs
    - Concrètement, cela signifie que cette structure de donnée permet de certifier qu'un élément n'est pas dans l'ensemble
    - Mais ne permet de dire avec certitude si un élément appartient à l'ensemble
    - Ce comportement s'explique par le fonctionnement interne du *Bloom Filter*
    - Définit un tableau de /m/ bits, tous initialisés à 0
    - Définit /k/ fonctions de hash indépendantes, chacune associant n'importe quelle entrée à un index du tableau définit précédemment
    - Lorsqu'un élément est ajouté à l'ensemble, passe à 1 chaque index du tableau correspondant aux résultats du hash de l'élément par les /k/ fonctions
    - Lorsqu'on vérifie si un élément appartient à l'ensemble, vérifie les indexes correspondant aux résultats des /k/ fonctions de hash
      - Si la valeur à un de ces indexes vaut 0, cela indique que l'élément n'a pas été ajouté à l'ensemble
      - Si toutes les valeurs aux indexes valent 1, on ne peut pas tirer de conclusions
    - En effet, il est possible que tous les indexes donnés par les hashs d'un élément correspondent à des 1
    - Mais que ça résulte d'un effet de bord de l'ajout d'autres éléments
      - Par exemple, l'ajout des éléments /X/ et /Y/ passent les indexes /{1, 3, 5, 6, 8, 9}/ à 1
      - On ne peut plus savoir avec certitude si un élément correspondant à un sous-ensemble de ces indexes a été ajouté ou pas
  - Fonctionnement
    - Repose sur un *Counting Bloom Filter* de taille /m/ et avec /k/ fonctions de hash
      - Comme un *Bloom Filter*, mais au lieu d'utiliser un tableau de bits, on utilise un tableau d'entiers
    - À chaque fois qu'un event local est produit par un noeud, hash /k/ fois l'event avec les fonctions de hash et incrémente les entrées correspondants du *Bloom Filter*
      - e.g. [0, 1, 1, 0, 0, 1] -> [1, 2, 1, 0, 1, 1]
    - À la réception d'un event de la *Bloom Clock* associée, le noeud met à jour sa propre *Bloom Clock* ne prenant le maximum pairwise
      - e.g. [0, 2, 1, 0, 1, 2] + [1, 2, 2, 0, 0, 2] -> [1, 2, 2, 0, 1, 2]
  - Propriétés
    - Concurrence
      - Deux *Bloom Clocks* peuvent ne pas être comparables
	- e.g. [0, 2, 1, 0, 1, 2] | [1, 2, 2, 0, 0, 2]
      - Aucune clock ne domine l'autre
      - Ceci ne peut pas être le résultat d'un faux négatif
      - Cela indique forcément que les events estampillés sont concurrents
    - Causalité
      - Deux *Bloom Clocks* peuvent être ordonnées
	- e.g. [1, 1, 1, 1, 1, 1] < [1, 2, 2, 1, 1, 2]
      - Indique que le 1er event a eu lieu avant le second
      - Peut être le résultat d'un faux positif
	- Comme expliqué précédemment dans la partie *Bloom Filter*
      - Ces events peuvent donc être en fait concurrents
      - Créé donc une relation de causalité non-justifiée dans le cas d'un faux positif
      - Ce qui entraîne une coordination non nécessaire, mais qui est en soit inoffensive pour la correction de l'application
  - Factorisation d'entier
    - Propose, pour mininimiser la taille des entiers du tableau, de soustraire la valeur minimale commune du tableau et de l'additionner à un compteur séparé
    - Possible aussi avec les *Vector Clocks*, mais le problème est qu'il suffit qu'un seul noeud n'effectue plus de modifications pour que le mécanisme soit gelé
    - Alors que dans les *Bloom Clocks*, comme les entrées sont incrémentées de façon aléatoire, garantit que toutes les entrées augmenteront de façon similaire
  - Comparaison avec *Vector Clocks*
    - Taille de la clock ne dépend plus du nombre de noeuds, une métrique qu'on ne maitrise pas dans les systèmes distribués
    - Mais du taux de faux positifs qu'on autorise
      - Se traduit en la taille /m/ du tableau et le nombre /k/ de fonctions de hash
    - Permet de vérifier rapidement la concurrence entre deux events
    - Et ne force potentiellement qu'une causalité inexistante lors d'un faux positif
  - Reste à voir l'usage qu'on pourrait en faire
  - Ou si d'autres variantes du *Bloom Filter* pourraient être réutilisés dans nos applications
- Statistiques sur *Protobuf*
  - Histoire de se faire un ordre d'idée des performances de *Protobuf* par rapport au reste de l'application
  - A calculé quelques stats à partir des mesures qu'on a effectué dans le cadre des simulations
  - Dans le fichier résultat de /mute-bot-random/, on a pour chaque opération /t1/ et /t2/
    - /t1/ correspond au timestamp mesuré au plus proche de la couche réseau
      - Correspond à la fin de la sérialisation dans le cas d'une opération locale
      - Correspond au début de la désérialisation dans le cas d'une opération distante
    - /t2/ correspond au timestamp mesuré au plus proche de la couche *Sync*
      - Correspond au début de la sérialisation dans le cas d'une opération locale
      - Correspond à la fin de la désérialisation dans le cas d'une opération distante
  - Mis en place un petit script pour récupérer les temps correspondants à la (dé)sérialisation, 10k opérations de chaque type
    #+BEGIN_BLOCK
    const ops = JSON.parse(data)

    let cptLocal = 0
    let cptRemote = 0

    const localOps = ops.filter((op: any) => op.type === "local" && cptLocal++ < 10000)
    const remoteOps = ops.filter((op: any) => op.type === "remote" && cptRemote++ < 10000)

    const localTimes = localOps.map((op: any) => {
      return (op.time1[0] * 1000000000 + op.time1[1]) - (op.time2[0] * 1000000000 + op.time2[1])
    })

    const remoteTimes = remoteOps.map((op: any) => {
      return (op.time2[0] * 1000000000 + op.time2[1]) - (op.time1[0] * 1000000000 + op.time1[1])
    })

    console.log(processTimes(localTimes))
    console.log(processTimes(remoteTimes))
    #+END_BLOCK
  - Obtient les résultats suivants, en ns
    - Sérialisation
      #+BEGIN_BLOCK
      {
         mean: 1090985.2778,
         median: 972866,
         ninetyNinePercentile: 3333145,
         standardDeviation: 424887.0168152513
      }
      #+END_BLOCK
    - Désérialisation
      #+BEGIN_BLOCK
      {
        mean: 70877.8204,
        median: 58458,
        ninetyNinePercentile: 149044,
        standardDeviation: 108483.43933078331
      }
      #+END_BLOCK
  - En résumé
    - 70us pour désérialiser
    - 1ms pour sérialiser
    - Donc environ 150x plus coûteux de sérialiser que désérialiser
  - Ce qui est intéressant de noter est que j'ai calculé ces stats sur les premières opérations de la collaboration et les dernières
  - Et qu'on ne remarque pas d'impact significatif sur les performances
* Backlog
** TODO MUTE : Rédiger une issue sur le problème de divergence suivant
- Dans MUTE, quand une modification est effectuée, son traitement est le suivant
  - L'opération locale est appliquée sur le modèle
  - Une opération distante résultante est générée
  - L'opération distante est broadcastée aux autres noeuds
- Le problème provient de comment on gère la sauvegarde locale de l'état
- L'état local est sauvegardé à intervals réguliers
- Ceci permet d'éviter de sauvegarder l'état pour chaque opération
- Cependant, cela créé le cas possible de divergence suivant
  - Un noeud génère une opération, identifiée de manière unique à l'aide d'un Dot /<siteId, seq>/
  - Le noeud propage l'opération avec succès
  - Le noeud crash avant d'avoir sauvegardé son nouvel état
    - Et donc la nouvelle valeur de /seq/
  - Le noeud relance l'application
  - À ce stade, le noeud n'a donc plus de traces de la dernière opération locale qu'il a effectué
  - Le noeud effectue une nouvelle opération, qui va partager le même identifiant "unique" que l'opération précédente
  - Le noeud propage l'opération
  - À ce stade, on va assister à une partition
    - Une partie des noeuds qui ont reçu et intégré la 1ère opération vont ignorer la 2nde, le dot étant identique
    - Les autres noeuds vont recevoir et intégrer la 2nde
- Il s'agit d'un cas d'équivocation
  - Alors que le noeud est potentiellement honnête
- Pistes de solution
  - Sauvegarder l'état avant de propager les opérations
    - Peut faire ça pour chaque opération
      - Mais ça serait coûteux
    - Peut donc mettre plutôt en place une bufferisation des opérations puis un envoi du batch après sauvegarde réussie
    - Introduirait par contre de la latence dans le système
** TODO DISTRIBUTED SYSTEMS : Lire *Incremental Consistency Guarantees for Replicated Objects*
- Par Rachid Guerraoui, Matej Pavlovic, Dragos-Adrian Seredinschi
- Introduit la notion de *Correctables*, des objets répliqués dont les garanties de cohérence évoluent au cours de l'exécution
- https://www.usenix.org/system/files/conference/osdi16/osdi16-guerraoui.pdf
** TODO COLLABORATIVE EDITING : Lire *Specification and Complexity of Collaborative Text Editing*
- Disponible ici : https://dl.acm.org/citation.cfm?id=2933090 (H. Attiya, S. Burckhardt, A. Gotsman, A. Morrison, H. Yang , M. Zawirski)
- Spécifie le comportement d'une liste répliquée
- Précise que pour assurer ce comportement, les protocoles doivent ajouter un overhead au moins linéaire au nombre d'éléments
- Indique qu'un protocole existant s'approche de cette limite
- Vérifiez le système modèle du protocole en question et à quelle spécification de la liste répliquée il correspond
- À partir de là, voir si je me place dans le même contexte
** TODO DISTRIBUTED SYSTEMS : Regarder *Distributed Transactions are dead, long live distributed transactions!*
- Lien : https://www.reactivesummit.org/2018/schedule/distributed-transactions-are-dead-long-live-distributed-transactions
** TODO PhD : Faire valider les formations scientifiques suivies
** TODO MUTE : Exploser le test "renameId() retains order between ids"
- Ce test évalue de nombreux cas différents
- Lorsqu'il échoue, le message d'erreur n'indique pas exactement le cas qui échoue
- Exploser ce test en plusieurs, chacun testant un scénario précis, aidera à détecter les erreurs et les potentiels scénarios manquants
** TODO MUTE : Exploser le test "reverseRenameId() retains order between ids"
- Ce test évalue de nombreux cas différents
- Lorsqu'il échoue, le message d'erreur n'indique pas exactement le cas qui échoue
- Exploser ce test en plusieurs, chacun testant un scénario précis, aidera à détecter les erreurs et les potentiels scénarios manquants
** IN-PROGRESS PhD : Motiver le problème adressé
- Contacter les gens de Redis pour discuter des implémentations proposés pour les types *String* et *List*
- Contacter les gens de xRay pour discuter du CRDT implémenté
- Creuser du côté de Postgres
** IN-PROGRESS Méthode formelle : Se remettre à niveau en méthode formelle
- D'après la discussion de Victorien avec Stephan Merz, ce dernier lui a conseillé
  - De commencer par du model-checking (TLA+)
  - Puis de passer sur un assistant de preuves (Isabelle/Coq)
- Il faudrait donc je me forme sur ces sujets pour valider le mécanisme de renommage formellement
- Ressources disponibles
  - Page de Lamport sur TLA+
    - http://lamport.azurewebsites.net/tla/tla.html
  - Cours vidéo de Lamport sur TLA+
    - http://lamport.azurewebsites.net/video/videos.html
  - Un tuto que j'ai vu passé un jour sur HN
    - https://learntla.com/introduction/
** TODO DISTRIBUTED DATABASES : Lire *NewSQL database systems are failing to guarantee consistency, and I blame Spanner*
- Disponible ici : https://dbmsmusings.blogspot.com/2018/09/newsql-database-systems-are-failing-to.html
- Semblerait que des CTO des bases de données mentionnées dans l'article ont répondu dans les commentaires, ce qui a donné lieu à des conversations intéressantes à priori
** TODO MUTE : Identifier l'origine de la divergence de la réunion du <2018-06-01 Fri>
- Un bloc de texte s'est retrouvé inséré au mauvais endroit pour une copie des notes prises au cours de cette réunion
- En identifiant la structure obtenue, on s'est rendu compte que ce bloc n'aurait pas dû être placé à cet endroit étant donné son identifiant
- Enquêter pour identifier l'origine de ce mauvais positionnement
  - Est-ce lié à l'algorithme de comparaison des identifiants ?
  - Ou est-ce lié à l'algorithme d'insertion au sein de l'arbre ?
** TODO CRDT : Se renseigner sur replikativ
- http://replikativ.io/
** TODO CRDT : Se renseigner sur OrbitDB
- https://github.com/orbitdb/orbit-db
** TODO DISTRIBUTED SYSTEMS : Se renseigner sur Dat
- https://datproject.org/
- https://datproject.org/paper
** TODO CONSENSUS : Lire *The weakest failure detector for solving consensus*
- *Perspectives on the CAP Theorem* ([[perspective-cap-theorem]]) cite ce papier de la manière suivante
  #+BEGIN_QUOTE
  In many ways, a failure detector exactly encapsulates the synchrony requirements for consensus.
  They showed that a particular failure detector Ω is the weakest failure detector for solving consensus.
  The failure detector Ω essentially encapsulates a leader election service
  #+END_QUOTE
- Je ne comprends pas le lien entre /failure detector/ et /synchrony requirements for consensus/
- Ce papier m'aidera p-e à comprendre
** TODO CRDT : Lire *Replicated Data Types: Specification, Verification, Optimality*
** TODO CRDT : Lire *CRDT notes*
- Disponible ici: https://github.com/pfrazee/crdt_notes
- Regroupe des notes de lectures sur plusieurs articles concernant les CRDT
** TODO CRDT : Lire *A Conflict-Free Replicated JSON Datatype*
- Disponible ici: https://arxiv.org/pdf/1608.03960.pdf
** TODO MUTE : Faire une liste des scénarios à tester d'une version à l'autre
** TODO MUTE : Rendre permanent l'identifiant et les clocks d'un utilisateur pour un document
- Pour l'historique, il est important de conserver la liste des collaborateurs ayant collaboré sur un document
- Pour faciliter l'implémentation de cette liste, il faut modifier le code pour réutiliser le même identifiant pour un même utilisateur au fil des sessions
- À noter que les clocks aussi doivent être conservée pour prévenir tout bug
  - Clock de /mute-structs/ pour éviter de générer un bloc avec le même identifiant
  - Clock de /mute-core/ pour éviter de labeller des messages avec le même couple /<identifiant, clock>/
** TODO MUTE : Ajouter le /docId/ en tant que paramètre du constructeur de *DocService*
- Cet attribut est actuellement initialisé via le flux /initSource/
- Celui-ci émet l'identifiant du document actuel à son initialisation
- Cependant, l'initialisation du document correspond à l'instantiation du *DocService*
- Nous pouvons donc simplifier le code en ajoutant /docId/ en tant que paramètre du constructeur
** TODO MUTE : Étudier la suppression de *LogootSBlock*
- L'utilisation de *LogootSBlock* semble limitée
  - Conserve une référence vers un *IdentifierInterval*
  - Cet *IdentifierInterval* ne peut que grossir
  - Conserve le nombre d'éléments de cet *IdentifierInterval* qui existent encore
  - Permet de déterminer si les éléments de cet *IdentifierInterval* nous appartiennent
    - Pour voir si on peut append ou prepend
- C'est ensuite une ou plusieurs instances de *RopesNodes* qui référencent un *LogootSBlock*
- Les instances de *LogootSBlock* sont aussi référencées dans /mapBaseToBlock/
  - Cette map nous permet de récupérer un bloc via sa base
  - Elle est utilisée pour récupérer un bloc s'il existe, ou pour déterminer qu'il faut le créer
- Serait-il possible de supprimer *LogootSBlock* ?
  - L'objectif serait de simplifier la structure de données
  - Il faudrait en contrepartie enrichir *RopesNodes* ou *LogootSRopes*
  - Voir si ça permettrait effectivement de simplifier le code
** TODO MUTE : Fusionner blocs adjacents issus d'un split après la suppression
- Lors d'un split, un bloc est coupé en deux et un nouveau bloc est inséré au milieu
- Après la suppression du bloc inséré, on pourrait fusionner les deux parties du bloc initial pour le recréer
- Ceci permettrait de rendre commutatif ce scénario d'opérations : /insertion-split-deletion/
** TODO CRDT : Lire *Kaleido*
* Commandes utiles
- Récupérer la dernière ligne du log de chaque container Docker tournant sur la machine
  - /docker ps -q | xargs -i -t docker logs --tail=1 {}/
- Supprimer le champ /renamingMap/ d'une snapshot donnée au format JSON et enregistrer le résultat obtenu
  - /cat Snapshot.json | jq -jc 'del(.mapBaseToBlock)' > Snapshot-without-map.json/
- Supprimer les lignes de 100 à 100000 d'un fichier
  - /sed '100,100000d' file > file-truncated/
- Insérer une image à une position absolue en LaTeX
  #+BEGIN_BLOCK
  \begin{picture}(0,0)
    \put(5,-40){\hbox{\includegraphics[scale=0.8]{signature.png}}}
  \end{picture}
  #+END_BLOCK
* Glossaire
- <<<Eventual convergence>>> property
  #+BEGIN_QUOTE
  Copies of shared objects are identical at all sites if updates cease and all generated updates are propagated to all sites.
  #+END_QUOTE
- <<<Precedence>>> property
  #+BEGIN_QUOTE
  If one update /Oa/ causally precedes another update /Ob/, then,
  at each site, the execution of /Oa/ happens before the execution of /Ob/.
  #+END_QUOTE
- <<<Intention-preservation>>>
  #+BEGIN_QUOTE
  For any update /O/, the effect of executing /O/ at all sites is the same as the intention of /O/ when executed at the site that originated it,
  and the effect of executing /O/ does not change the effect of non concurrent operations.
  #+END_QUOTE
- Mathematics operations properties
  - <<<Associativity>>>
    #+BEGIN_QUOTE
    Associativity means that you can apply a function in any order:
      =f(a,f(b,c)) = f(f(a,b),c)=
    #+END_QUOTE
  - <<<Commutativity>>>
    #+BEGIN_QUOTE
    Commutativity means that a function's arguments are order-insensitive:
      =f(a,b) = f(b,a))=
    #+END_QUOTE
  - <<<Idempotence>>>
    #+BEGIN_QUOTE
    Idempotence means you can call a function on the same input any number of times and get the same result:
      =f(f(x))=f(x) (e.g., max(42, max(42, 42)) = 42))=
    #+END_QUOTE
- Systems properties
  - <<<Safety>>>
    #+BEGIN_QUOTE
    Informally, an algorithm is safe if nothing bad ever happens. -- Perspectives on the CAP Theorem
    #+END_QUOTE
  - <<<Liveness>>>
    #+BEGIN_QUOTE
    By contrast, an algorithm is live if eventually something good happens. -- Perspectives on the CAP Theorem
    #+END_QUOTE
  - <<<Unreliable>>>
    #+BEGIN_QUOTE
    There are many different ways in which a system can be unreliable. There may be partitions,
    crash failures, message loss, malicious attacks (or Byzantine failures), etc. -- Perspectives on the CAP Theorem
    #+END_QUOTE
- <<<Effectively-once>>>
  #+BEGIN_QUOTE
  Viktor Klang: "I'm coining the phrase 'effectively-once' for message processing with at-least once + idempotent operations", twitter.com, October 20, 2016
  #+END_QUOTE
- <<<GC>>>
  - Garbage Collection
